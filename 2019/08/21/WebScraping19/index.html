<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">



<!-- S 新增代码 -->
<script src="//cdn.bootcss.com/pace/1.0.2/pace.min.js"></script>
<link href="//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css" rel="stylesheet">
<style>
    .pace .pace-progress {
        background: #24292e; /*进度条颜色*/
        height: 3px;
    }
    .pace .pace-progress-inner {
         box-shadow: 0 0 10px #1E92FB, 0 0 5px     #1E92FB; /*阴影颜色*/
    }
    .pace .pace-activity {
        border-top-color: #1E92FB;    /*上边框颜色*/
        border-left-color: #1E92FB;    /*左边框颜色*/
    }
</style>
<!-- E 新增代码 -->





 <script>(function(i,s,o,g,r,a,m){i["DaoVoiceObject"]=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset="utf-8";m.parentNode.insertBefore(a,m)})(window,document,"script",('https:' == document.location.protocol ? 'https:' : 'http:') + "//widget.daovoice.io/widget/37874f6b.js","daovoice")
 
daovoice('init', {
      app_id: "5052a9e6"
    });
  daovoice('update');
  
  </script>



  <meta name="baidu-site-verification" content="Ji5duS2dR8">
















<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">



  <meta name="google-site-verification" content="true">













  <meta name="baidu-site-verification" content="true">










  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.3.0" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/logoo32.png?v=6.3.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/logoo16.png?v=6.3.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.3.0" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.3.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  







  <meta name="description" content="偶然间看到一本书《Python3网络爬虫开发实战》，作者还特别地写了一系列的博客，本篇论文，几乎参照他的博客写的，作为学习笔记。">
<meta name="keywords" content="Web-Scraping,python">
<meta property="og:type" content="article">
<meta property="og:title" content="WebScraping-19">
<meta property="og:url" content="https://purespring.top/2019/08/21/WebScraping19/index.html">
<meta property="og:site_name" content="清 泉">
<meta property="og:description" content="偶然间看到一本书《Python3网络爬虫开发实战》，作者还特别地写了一系列的博客，本篇论文，几乎参照他的博客写的，作为学习笔记。">
<meta property="og:locale" content="zh-CN">
<meta property="og:image" content="https://purespring.top/2019/08/21/WebScraping19/1.jpg">
<meta property="og:image" content="https://purespring.top/2019/08/21/WebScraping19/2.png">
<meta property="og:image" content="https://purespring.top/2019/08/21/WebScraping19/3.png">
<meta property="og:image" content="https://purespring.top/2019/08/21/WebScraping19/4.png">
<meta property="og:updated_time" content="2019-08-30T14:59:02.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WebScraping-19">
<meta name="twitter:description" content="偶然间看到一本书《Python3网络爬虫开发实战》，作者还特别地写了一系列的博客，本篇论文，几乎参照他的博客写的，作为学习笔记。">
<meta name="twitter:image" content="https://purespring.top/2019/08/21/WebScraping19/1.jpg">



  <link rel="alternate" href="/atom.xml" title="清 泉" type="application/atom+xml">




  <link rel="canonical" href="https://purespring.top/2019/08/21/WebScraping19/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>WebScraping-19 | 清 泉</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>


<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>


<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">


  
  <link rel="stylesheet" href="/dist/APlayer.min.css">
<div id="aplayer"></div>
<script type="text/javascript" src="/dist/APlayer.min.js"></script>
<script type="text/javascript" src="/dist/music.js"></script>





  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>




<a href="https://github.com/ninanxiaoguai" class="github-corner" aria-label="View source on Github"><svg width="80" height="80" viewbox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>







    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">清 泉</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">
    <a href="/categories/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://purespring.top/2019/08/21/WebScraping19/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="spring">
      <meta itemprop="description" content="梦想天空分外蓝~">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="清 泉">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">WebScraping-19
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-08-21 14:24:43" itemprop="dateCreated datePublished" datetime="2019-08-21T14:24:43+08:00">2019-08-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-08-30 22:59:02" itemprop="dateModified" datetime="2019-08-30T22:59:02+08:00">2019-08-30</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/Web-Scraping/" itemprop="url" rel="index"><span itemprop="name">Web-Scraping</span></a></span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/08/21/WebScraping19/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/08/21/WebScraping19/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/08/21/WebScraping19/" class="leancloud_visitors" data-flag-title="WebScraping-19">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数：</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">本文字数：</span>
                
                <span title="本文字数">40k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">37 分钟</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>偶然间看到一本书《Python3网络爬虫开发实战》，作者还特别地写了一系列的<a href="https://cuiqingcai.com/5500.html" target="_blank" rel="noopener">博客</a>，本篇论文，几乎参照他的博客写的，作为学习笔记。</p>
<a id="more"></a>
<h2 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h2><p>python3的urllib库<a href="https://docs.python.org/3/library/urllib.html" target="_blank" rel="noopener">官方文档</a>，分为四个模块：</p>
<ul>
<li>request：它是最基本的HTTP 请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入URL 以及额外的参数，就可以模拟实现这个过程了。</li>
<li>error：异常处理模块，如果出现请求错误， 我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。</li>
<li>parse：一个工具模块，提供了许多URL 处理方法，比如拆分、解析、合并等。</li>
<li>robotparser：主要是用来识别网站的robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少。</li>
</ul>
<h3 id="request"><a href="#request" class="headerlink" title="request"></a>request</h3><p><code>urllib.request</code>模块提供了最基本的构造HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程， 同时它还带有处理授权验证(authenticaton)、重定向(redirection) 、浏览器Cookies 以及其他内容。</p>
<h4 id="urlopen"><a href="#urlopen" class="headerlink" title="urlopen"></a>urlopen</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">response = urllib.request.urlopen(<span class="string">"https://www.python.org"</span>)</span><br><span class="line">print(type(response))</span><br></pre></td></tr></table></figure>
<p>输出的为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;http.client.HTTPResponse&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以发现，它是一个<code>HTTPResposne</code>类型的对象。它主要包含<code>read()</code>、<code>readinto()</code>、<code>getheader(name)</code>、<code>getheaders()</code>、<code>fileno()</code>等<strong>方法</strong>，以及<code>msg</code>、<code>version</code>、<code>status</code>、<code>reason</code>、<code>debuglevel</code>、<code>closed</code>等<strong>属性</strong>。</p>
<p>实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"> </span><br><span class="line">response = urllib.request.urlopen(<span class="string">'https://www.python.org'</span>)</span><br><span class="line">print(response.status)</span><br><span class="line">print(response.getheaders())</span><br><span class="line">print(response.getheader(<span class="string">'Server'</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="number">200</span></span><br><span class="line">[(<span class="string">'Server'</span>, <span class="string">'nginx'</span>), (<span class="string">'Content-Type'</span>, <span class="string">'text/html; charset=utf-8'</span>), (<span class="string">'X-Frame-Options'</span>, <span class="string">'SAMEORIGIN'</span>), (<span class="string">'X-Clacks-Overhead'</span>, <span class="string">'GNU Terry Pratchett'</span>), (<span class="string">'Content-Length'</span>, <span class="string">'47397'</span>), (<span class="string">'Accept-Ranges'</span>, <span class="string">'bytes'</span>), (<span class="string">'Date'</span>, <span class="string">'Mon, 01 Aug 2016 09:57:31 GMT'</span>), (<span class="string">'Via'</span>, <span class="string">'1.1 varnish'</span>), (<span class="string">'Age'</span>, <span class="string">'2473'</span>), (<span class="string">'Connection'</span>, <span class="string">'close'</span>), (<span class="string">'X-Served-By'</span>, <span class="string">'cache-lcy1125-LCY'</span>), (<span class="string">'X-Cache'</span>, <span class="string">'HIT'</span>), (<span class="string">'X-Cache-Hits'</span>, <span class="string">'23'</span>), (<span class="string">'Vary'</span>, <span class="string">'Cookie'</span>), (<span class="string">'Strict-Transport-Security'</span>, <span class="string">'max-age=63072000; includeSubDomains'</span>)]</span><br><span class="line">nginx</span><br></pre></td></tr></table></figure>
<p>可见，前两个输出分别输出了响应的状态码和响应的头信息，最后一个输出通过调用<code>getheader()</code>方法并传递一个参数<code>Server</code>获取了响应头中的<code>Server</code>值，结果是<code>nginx</code>，意思是服务器是用Nginx搭建的。</p>
<p>利用最基本的<code>urlopen()</code>方法，可以完成最基本的简单网页的GET请求抓取。</p>
<p>如果想给链接传递一些参数，该怎么实现呢？首先看一下<code>urlopen()</code>函数的API：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.request.urlopen(url, data=<span class="literal">None</span>, [timeout, ]*, cafile=<span class="literal">None</span>, capath=<span class="literal">None</span>, cadefault=**<span class="literal">False</span>**, context=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>
<p>可以发现，除了第一个参数可以传递URL之外，我们还可以传递其他内容，比如<code>data</code>（附加数据）、<code>timeout</code>（超时时间）等。</p>
<p>下面我们详细说明下这几个参数的用法。</p>
<h5 id="data参数"><a href="#data参数" class="headerlink" title="data参数"></a><code>data</code>参数</h5><p><code>data</code>参数是可选的。如果要添加该参数，并且如果它是字节流编码格式的内容，即<code>bytes</code>类型，则需要通过<code>bytes()</code>方法转化。另外，如果传递了这个参数，则它的请求方式就不再是GET方式，而是POST方式。</p>
<p>下面用实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">data = bytes(urllib.parse.urlencode(&#123;<span class="string">'word'</span>: <span class="string">'hello'</span>&#125;), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://httpbin.org/post'</span>, data=data)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>
<p>这里我们传递了一个参数<code>word</code>，值是<code>hello</code>。它需要被转码成<code>bytes</code>（字节流）类型。其中转字节流采用了<code>bytes()</code>方法，该方法的第一个参数需要是<code>str</code>（字符串）类型，需要用<code>urllib.parse</code>模块里的<code>urlencode()</code>方法来将参数字典转化为字符串；第二个参数指定编码格式，这里指定为<code>utf8</code>。</p>
<p>这里请求的站点是<code>httpbin.org</code>，它可以提供HTTP请求测试。本次我们请求的URL为<code>http://httpbin.org/post</code>，这个链接可以用来测试POST请求，它可以输出请求的一些信息，其中包含我们传递的<code>data</code>参数。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     &quot;args&quot;: &#123;&#125;,</span><br><span class="line">     &quot;data&quot;: &quot;&quot;,</span><br><span class="line">     &quot;files&quot;: &#123;&#125;,</span><br><span class="line">     &quot;form&quot;: &#123;</span><br><span class="line">         &quot;word&quot;: &quot;hello&quot;</span><br><span class="line">     &#125;,</span><br><span class="line">     &quot;headers&quot;: &#123;</span><br><span class="line">         &quot;Accept-Encoding&quot;: &quot;identity&quot;,</span><br><span class="line">         &quot;Content-Length&quot;: &quot;10&quot;,</span><br><span class="line">         &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;,</span><br><span class="line">         &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">         &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot;</span><br><span class="line">     &#125;,</span><br><span class="line">     &quot;json&quot;: null,</span><br><span class="line">     &quot;origin&quot;: &quot;123.124.23.253&quot;,</span><br><span class="line">     &quot;url&quot;: &quot;http://httpbin.org/post&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们传递的参数出现在了<code>form</code>字段中，这表明是模拟了表单提交的方式，以POST方式传输数据。</p>
<h5 id="timeout参数"><a href="#timeout参数" class="headerlink" title="timeout参数"></a><code>timeout</code>参数</h5><p><code>timeout</code>参数用于设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。它支持HTTP、HTTPS、FTP请求。</p>
<p>下面用实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"> </span><br><span class="line">response = urllib.request.urlopen(<span class="string">'http://httpbin.org/get'</span>, timeout=<span class="number">1</span>)</span><br><span class="line">print(response.read())</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">---------------------------------------------------------------------------</span><br><span class="line">timeout                                   Traceback (most recent call last)</span><br><span class="line">&lt;ipython-input-16-59340fde1030&gt; in &lt;module&gt;</span><br><span class="line">----&gt; 1 response = urllib.request.urlopen(&apos;http://httpbin.org/get&apos;, timeout=1)</span><br><span class="line"></span><br><span class="line">D:\Anaconda3\lib\urllib\request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)</span><br><span class="line">...</span><br><span class="line">timeout: timed out</span><br></pre></td></tr></table></figure>
<p>这里我们设置超时时间是1秒。程序1秒过后，服务器依然没有响应，于是抛出了<code>URLError</code>异常。该异常属于<code>urllib.error</code>模块，错误原因是超时。</p>
<p>因此，可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用<code>try except</code>语句来实现，相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"> </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'http://httpbin.org/get'</span>, timeout=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre></td></tr></table></figure>
<p>这里我们请求了<code>http://httpbin.org/get</code>测试链接，设置超时时间是0.1秒，然后捕获了<code>URLError</code>异常，接着判断异常是<code>socket.timeout</code>类型（意思就是超时异常），从而得出它确实是因为超时而报错，打印输出了<code>TIME OUT</code>。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TIME OUT</span><br></pre></td></tr></table></figure>
<p>按照常理来说，0.1秒内基本不可能得到服务器响应，因此输出了<code>TIME OUT</code>的提示。</p>
<p>通过设置<code>timeout</code>这个参数来实现超时处理，有时还是很有用的。</p>
<h5 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h5><p>初了<code>data</code>参数和<code>timeout</code>参数外，还有<code>context</code>参数，它必须是<code>ssl.SSLContext</code>类型，用来指定SSL设置。</p>
<p>此外，<code>cafile</code>和<code>capath</code>这两个参数分别指定CA证书和它的路径，这个在请求HTTPS链接时会有用。</p>
<p><code>cadefault</code>参数现在已经弃用了，其默认值为<code>False</code>。</p>
<p>前面讲解了<code>urlopen()</code>方法的用法，通过这个最基本的方法，我们可以完成简单的请求和网页抓取。若需更加详细的信息，可以参见<a href="https://docs.python.org/3/library/urllib.request.html" target="_blank" rel="noopener">官方文档</a>。</p>
<h4 id="Request"><a href="#Request" class="headerlink" title="Request"></a>Request</h4><p>我们知道利用<code>urlopen()</code>方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需要加入Headers等信息，就可以利用更强大的<code>Request</code>类来构建。</p>
<p>首先，我们用实例来感受一下<code>Request</code>的用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"> </span><br><span class="line">request = urllib.request.Request(<span class="string">'https://python.org'</span>)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>可以发现，我们依然是用<code>urlopen()</code>方法来发送这个请求，只不过这次该方法的参数不再是URL，而是一个<code>Request</code>类型的对象。通过构造这个<strong>数据结构</strong>，一方面我们可以将请求独立成一个对象，另一方面可更加丰富和灵活地配置参数。</p>
<p>下面我们看一下<code>Request</code>可以通过怎样的参数来构造，它的构造方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">urllib</span>.<span class="title">request</span>.<span class="title">Request</span><span class="params">(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>第一个参数<code>url</code>用于请求URL，这是必传参数，其他都是可选参数。</li>
<li>第二个参数<code>data</code>如果要传，必须传<code>bytes</code>（字节流）类型的。如果它是字典，可以先用<code>urllib.parse</code>模块里的<code>urlencode()</code>编码。</li>
<li>第三个参数<code>headers</code>是一个字典，它就是请求头，我们可以在构造请求时通过<code>headers</code>参数直接构造，也可以通过调用请求实例的<code>add_header()</code>方法添加。</li>
</ul>
<p>添加请求头最常用的用法就是通过修改<code>User-Agent</code>来伪装浏览器，默认的<code>User-Agent</code>是Python-urllib，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11</span><br></pre></td></tr></table></figure>
<ul>
<li>第四个参数<code>origin_req_host</code>指的是请求方的host名称或者IP地址。</li>
<li>第五个参数<code>unverifiable</code>表示这个请求是否是无法验证的，默认是<code>False</code>，意思就是说用户没有足够权限来选择接收这个请求的结果。例如，我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时unverifiable<code>的值就是</code>True`。</li>
<li>第六个参数<code>method</code>是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等。</li>
</ul>
<p>下面我们传入多个参数构建请求来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, parse</span><br><span class="line"> </span><br><span class="line">url = <span class="string">'http://httpbin.org/post'</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>,</span><br><span class="line">    <span class="string">'Host'</span>: <span class="string">'httpbin.org'</span></span><br><span class="line">&#125;</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'Germey'</span></span><br><span class="line">&#125;</span><br><span class="line">data = bytes(parse.urlencode(dict), encoding=<span class="string">'utf8'</span>)</span><br><span class="line">req = request.Request(url=url, data=data, headers=headers, method=<span class="string">'POST'</span>)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>这里我们通过4个参数构造了一个请求，其中<code>url</code>即请求URL，<code>headers</code>中指定了<code>User-Agent</code>和<code>Host</code>，参数<code>data</code>用<code>urlencode()</code>和<code>bytes()</code>方法转成字节流。另外，指定了请求方式为POST。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;,</span><br><span class="line">  &quot;data&quot;: &quot;&quot;,</span><br><span class="line">  &quot;files&quot;: &#123;&#125;,</span><br><span class="line">  &quot;form&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;Germey&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;identity&quot;,</span><br><span class="line">    &quot;Content-Length&quot;: &quot;11&quot;,</span><br><span class="line">    &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;,</span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;,</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;json&quot;: null,</span><br><span class="line">  &quot;origin&quot;: &quot;117.179.104.211, 117.179.104.211&quot;,</span><br><span class="line">  &quot;url&quot;: &quot;https://httpbin.org/post&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>观察结果可以发现，我们成功设置了<code>data</code>、<code>headers</code>和<code>method</code>。</p>
<p>另外，<code>headers</code>也可以用<code>add_header()</code>方法来添加：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">req = request.Request(url=url, data=data, method=<span class="string">'POST'</span>)</span><br><span class="line">req.add_header(<span class="string">'User-Agent'</span>, <span class="string">'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)'</span>)</span><br></pre></td></tr></table></figure>
<p>如此一来，我们就可以更加方便地构造请求，实现请求的发送啦。</p>
<h4 id="高级用法"><a href="#高级用法" class="headerlink" title="高级用法"></a>高级用法</h4><p>在上面的过程中，我们虽然可以构造请求，但是对于一些更高级的操作（比如Cookies处理、代理设置等），我们该怎么办呢？</p>
<p>接下来，就需要更强大的工具Handler登场了。简而言之，我们可以把它理解为各种处理器，有专门处理登录验证的，有处理Cookies的，有处理代理设置的。利用它们，我们几乎可以做到HTTP请求中所有的事情。</p>
<p>首先，介绍一下<code>urllib.request</code>模块里的<code>BaseHandler</code>类，它是所有其他<code>Handler</code>的父类，它提供了最基本的方法，例如<code>default_open()</code>、<code>protocol_request()</code>等。</p>
<p>接下来，就有各种<code>Handler</code>子类继承这个<code>BaseHandler</code>类，举例如下。</p>
<ul>
<li><strong>HTTPDefaultErrorHandler</strong>：用于处理HTTP响应错误，错误都会抛出<code>HTTPError</code>类型的异常。</li>
<li><strong>HTTPRedirectHandler</strong>：用于处理重定向。</li>
<li><strong>HTTPCookieProcessor</strong>：用于处理Cookies。</li>
<li><strong>ProxyHandler</strong>：用于设置代理，默认代理为空。</li>
<li><strong>HTTPPasswordMgr</strong>：用于管理密码，它维护了用户名和密码的表。</li>
<li><strong>HTTPBasicAuthHandler</strong>：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。</li>
</ul>
<p>另外，还有其他的<code>Handler</code>类，这里就不一一列举了，详情可以参考<a href="https://docs.python.org/3/library/urllib.request.html#urllib.request.BaseHandler" target="_blank" rel="noopener">官方文档</a>。</p>
<p>关于怎么使用它们，现在先不用着急，后面会有实例演示。</p>
<p>另一个比较重要的类就是<code>OpenerDirector</code>，我们可以称为<code>Opener</code>。我们之前用过<code>urlopen()</code>这个方法，实际上它就是urllib为我们提供的一个<code>Opener</code>。</p>
<p>那么，为什么要引入<code>Opener</code>呢？因为需要实现更高级的功能。之前使用的<code>Request</code>和<code>urlopen()</code>相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以需要深入一层进行配置，使用更底层的实例来完成操作，所以这里就用到了<code>Opener</code>。</p>
<p><code>Opener</code>可以使用<code>open()</code>方法，返回的类型和<code>urlopen()</code>如出一辙。那么，它和<code>Handler</code>有什么关系呢？简而言之，就是利用<code>Handler</code>来构建<code>Opener</code>。</p>
<p>下面用几个实例来看看它们的用法。</p>
<p>有些网站在打开时就会弹出提示框，直接提示你输入用户名和密码，验证成功后才能查看页面，如图3-2所示。</p>
<p><img src="/2019/08/21/WebScraping19/1.jpg" alt=""></p>
<p>那么，如果要请求这样的页面，该怎么办呢？借助<code>HTTPBasicAuthHandler</code>就可以完成，相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_opener</span><br><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"> </span><br><span class="line">username = <span class="string">'username'</span></span><br><span class="line">password = <span class="string">'password'</span></span><br><span class="line">url = <span class="string">'http://localhost:5000/'</span></span><br><span class="line"> </span><br><span class="line">p = HTTPPasswordMgrWithDefaultRealm()</span><br><span class="line">p.add_password(<span class="literal">None</span>, url, username, password)</span><br><span class="line">auth_handler = HTTPBasicAuthHandler(p)</span><br><span class="line">opener = build_opener(auth_handler)</span><br><span class="line"> </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = opener.open(url)</span><br><span class="line">    html = result.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">    print(html)</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>这里首先实例化<code>HTTPBasicAuthHandler</code>对象，其参数是<code>HTTPPasswordMgrWithDefaultRealm</code>对象，它利用<code>add_password()</code>添加进去用户名和密码，这样就建立了一个处理验证的<code>Handler</code>。</p>
<p>接下来，利用这个<code>Handler</code>并使用<code>build_opener()</code>方法构建一个<code>Opener</code>，这个<code>Opener</code>在发送请求时就相当于已经验证成功了。</p>
<p>接下来，利用<code>Opener</code>的<code>open()</code>方法打开链接，就可以完成验证了。这里获取到的结果就是验证后的页面源码内容。</p>
<h5 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.error <span class="keyword">import</span> URLError</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> ProxyHandler, build_opener</span><br><span class="line"> </span><br><span class="line">proxy_handler = ProxyHandler(&#123;</span><br><span class="line">    <span class="string">'http'</span>: <span class="string">'http://127.0.0.1:9743'</span>,</span><br><span class="line">    <span class="string">'https'</span>: <span class="string">'https://127.0.0.1:9743'</span></span><br><span class="line">&#125;)</span><br><span class="line">opener = build_opener(proxy_handler)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = opener.open(<span class="string">'https://www.baidu.com'</span>)</span><br><span class="line">    print(response.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line"><span class="keyword">except</span> URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>这里我们在本地搭建了一个代理，它运行在9743端口上。</p>
<p>这里使用了<code>ProxyHandler</code>，其参数是一个字典，键名是协议类型（比如HTTP或者HTTPS等），键值是代理链接，可以添加多个代理。</p>
<p>然后，利用这个Handler及<code>build_opener()</code>方法构造一个<code>Opener</code>，之后发送请求即可。</p>
<h4 id="Cookies"><a href="#Cookies" class="headerlink" title="Cookies"></a>Cookies</h4><p>Cookies的处理就需要相关的<code>Handler</code>了。</p>
<p>我们先用实例来看看怎样将网站的Cookies获取下来，相关代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> http.cookiejar, urllib.request</span><br><span class="line"> </span><br><span class="line">cookie = http.cookiejar.CookieJar()</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> cookie:</span><br><span class="line">    print(item.name+<span class="string">"="</span>+item.value)</span><br></pre></td></tr></table></figure>
<p>先，我们必须声明一个<code>CookieJar</code>对象。接下来，就需要利用<code>HTTPCookieProcessor</code>来构建一个<code>Handler</code>，最后利用<code>build_opener()</code>方法构建出<code>Opener</code>，执行<code>open()</code>函数即可。</p>
<p>运行结果如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">BAIDUID=<span class="number">32</span>ED45BB4CAA948553F9E9C25E67DF57:FG=<span class="number">1</span></span><br><span class="line">BIDUPSID=<span class="number">32</span>ED45BB4CAA948553F9E9C25E67DF57</span><br><span class="line">H_PS_PSSID=<span class="number">1420</span>_21122_18560_29523_29519_29098_29568_29220_29461</span><br><span class="line">PSTM=<span class="number">1566277884</span></span><br><span class="line">delPer=<span class="number">0</span></span><br><span class="line">BDSVRTM=<span class="number">0</span></span><br><span class="line">BD_HOME=<span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>可以看到，这里输出了每条Cookie的名称和值。</p>
<p>不过既然能输出，那可不可以输出成文件格式呢？我们知道Cookies实际上也是以文本形式保存的。</p>
<p>答案当然是肯定的，这里通过下面的实例来看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'cookies.txt'</span></span><br><span class="line">cookie = http.cookiejar.MozillaCookieJar(filename)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">cookie.save(ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这时<code>CookieJar</code>就需要换成<code>MozillaCookieJar</code>，它在生成文件时会用到，是<code>CookieJar</code>的子类，可以用来处理Cookies和文件相关的事件，比如读取和保存Cookies，可以将Cookies保存成Mozilla型浏览器的Cookies格式。</p>
<p>运行之后，可以发现生成了一个cookies.txt文件，其内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Netscape HTTP Cookie File</span><br><span class="line"># http://curl.haxx.se/rfc/cookie_spec.html</span><br><span class="line"># This is a generated file!  Do not edit.</span><br><span class="line"></span><br><span class="line">.baidu.com	TRUE	/	FALSE	3713761816	BAIDUID	A1EB1A2981419DCB5E77E799ACECE43C:FG=1</span><br><span class="line">.baidu.com	TRUE	/	FALSE	3713761816	BIDUPSID	A1EB1A2981419DCB5E77E799ACECE43C</span><br><span class="line">.baidu.com	TRUE	/	FALSE		H_PS_PSSID	1465_21100_29073_29523_29520_29099_29568_29221_29458_29588</span><br><span class="line">.baidu.com	TRUE	/	FALSE	3713761816	PSTM	1566278169</span><br><span class="line">.baidu.com	TRUE	/	FALSE		delPer	0</span><br><span class="line">www.baidu.com	FALSE	/	FALSE		BDSVRTM	0</span><br><span class="line">www.baidu.com	FALSE	/	FALSE		BD_HOME	0</span><br></pre></td></tr></table></figure>
<p>另外，<code>LWPCookieJar</code>同样可以读取和保存Cookies，但是保存的格式和<code>MozillaCookieJar</code>不一样，它会保存成libwww-perl(LWP)格式的Cookies文件。</p>
<p>要保存成LWP格式的Cookies文件，可以在声明时就改为：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cookie = http.cookiejar.LWPCookieJar(filename)</span><br></pre></td></tr></table></figure>
<p>由此看来，生成的格式还是有比较大差异的。</p>
<p>那么，生成了Cookies文件后，怎样从文件中读取并利用呢？</p>
<p>下面我们以<code>LWPCookieJar</code>格式为例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cookie = http.cookiejar.LWPCookieJar()</span><br><span class="line">cookie.load(<span class="string">'cookies.txt'</span>, ignore_discard=<span class="literal">True</span>, ignore_expires=<span class="literal">True</span>)</span><br><span class="line">handler = urllib.request.HTTPCookieProcessor(cookie)</span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line">response = opener.open(<span class="string">'http://www.baidu.com'</span>)</span><br><span class="line">print(response.read().decode(<span class="string">'utf-8'</span>))</span><br></pre></td></tr></table></figure>
<p>可以看到，这里调用<code>load()</code>方法来读取本地的Cookies文件，获取到了Cookies的内容。不过前提是我们首先生成了LWPCookieJar格式的Cookies，并保存成文件，然后读取Cookies之后使用同样的方法构建Handler和Opener即可完成操作。</p>
<p>运行结果正常的话，会输出百度网页的源代码。</p>
<p>通过上面的方法，我们可以实现绝大多数请求功能的设置了。</p>
<p>这便是urllib库中<code>request</code>模块的基本用法，如果想实现更多的功能，可以参考<a href="https://docs.python.org/3/library/urllib.request.html#basehandler-objects" target="_blank" rel="noopener">官方文档</a>。</p>
<h3 id="error"><a href="#error" class="headerlink" title="error"></a>error</h3><p>如果出现了异常，该怎么办呢？这时如果不处理这些异常，程序很可能因报错而终止运行，所以异常处理还是十分有必要的。</p>
<p>urllib的<code>error</code>模块定义了由<code>request</code>模块产生的异常。如果出现了问题，<code>request</code>模块便会抛出<code>error</code>模块中定义的异常。</p>
<h4 id="URLError"><a href="#URLError" class="headerlink" title="URLError"></a>URLError</h4><p>URLError类来自urllib库的error模块，它继承自OSError类，是error异常模块的基类，由request模块生的异常都可以通过捕获这个类来处理。</p>
<p>它具有一个属性reason，即返回错误的原因。</p>
<p>下面用一个实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br></pre></td></tr></table></figure>
<p>我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了URLError这个异常，运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Not Found</span><br></pre></td></tr></table></figure>
<p>程序没有直接报错，而是输出了如上内容，这样通过如上操作，我们就可以避免程序异常终止，同时异常得到了有效处理。</p>
<h4 id="HTTPError"><a href="#HTTPError" class="headerlink" title="HTTPError"></a>HTTPError</h4><p>它是<code>URLError</code>的子类，专门用来处理HTTP请求错误，比如认证请求失败等。它有如下3个属性。</p>
<ul>
<li><strong>code</strong>：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。</li>
<li><strong>reason</strong>：同父类一样，用于返回错误的原因。</li>
<li><strong>headers</strong>：返回请求头。</li>
</ul>
<p>下面我们用几个实例来看看：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request,error</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Not Found</span><br><span class="line">404</span><br><span class="line">Server: nginx/1.10.3 (Ubuntu)</span><br><span class="line">Date: Wed, 21 Aug 2019 06:42:29 GMT</span><br><span class="line">Content-Type: text/html; charset=UTF-8</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Connection: close</span><br><span class="line">Set-Cookie: PHPSESSID=jjamgq0r0ts5rur6ab5fjtp140; path=/</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Vary: Cookie</span><br><span class="line">Expires: Wed, 11 Jan 1984 05:00:00 GMT</span><br><span class="line">Cache-Control: no-cache, must-revalidate, max-age=0</span><br><span class="line">Link: &lt;https://cuiqingcai.com/wp-json/&gt;; rel=&quot;https://api.w.org/&quot;</span><br></pre></td></tr></table></figure>
<p>依然是同样的网址，这里捕获了<code>HTTPError</code>异常，输出了<code>reason</code>、<code>code</code>和<code>headers</code>属性。</p>
<p>因为<code>URLError</code>是<code>HTTPError</code>的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码更好的写法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request, error</span><br><span class="line"> </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = request.urlopen(<span class="string">'http://cuiqingcai.com/index.htm'</span>)</span><br><span class="line"><span class="keyword">except</span> error.HTTPError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason, e.code, e.headers, sep=<span class="string">'\n'</span>)</span><br><span class="line"><span class="keyword">except</span> error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(e.reason)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure>
<p>这样就可以做到先捕获<code>HTTPError</code>，获取它的错误状态码、原因、<code>headers</code>等信息。如果不是<code>HTTPError</code>异常，就会捕获<code>URLError</code>异常，输出错误原因。最后，用<code>else</code>来处理正常的逻辑。这是一个较好的异常处理写法。</p>
<p>有时候，<code>reason</code>属性返回的不一定是字符串，也可能是一个对象。再看下面的实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.error</span><br><span class="line"> </span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    response = urllib.request.urlopen(<span class="string">'https://www.baidu.com'</span>, timeout=<span class="number">0.01</span>)</span><br><span class="line"><span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">    print(type(e.reason))</span><br><span class="line">    <span class="keyword">if</span> isinstance(e.reason, socket.timeout):</span><br><span class="line">        print(<span class="string">'TIME OUT'</span>)</span><br></pre></td></tr></table></figure>
<p>这里我们直接设置超时时间来强制抛出<code>timeout</code>异常。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;socket.timeout&apos;&gt;</span><br><span class="line">TIME OUT</span><br></pre></td></tr></table></figure>
<p>可以发现，<code>reason</code>属性的结果是<code>socket.timeout</code>类。所以，这里我们可以用<code>isinstance()</code>方法来判断它的类型，作出更详细的异常判断。</p>
<p>本节中，我们讲述了<code>error</code>模块的相关用法，通过合理地捕获异常可以做出更准确的异常判断，使程序更加稳健。</p>
<h3 id="parse"><a href="#parse" class="headerlink" title="parse"></a>parse</h3><p>前面说过，urllib库里还提供了<code>parse</code>这个模块，它定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。它支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、 mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、 sip、sips、snews、svn、svn+ssh、telnet和wais。本节中，我们介绍一下该模块中常用的方法来看一下它的便捷之处。</p>
<h4 id="urlparse"><a href="#urlparse" class="headerlink" title="urlparse()"></a>urlparse()</h4><p>该方法可以实现URL的识别和分段，这里先用一个实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(type(result), result)</span><br></pre></td></tr></table></figure>
<p>这里我们利用<code>urlparse()</code>方法进行了一个URL的解析。首先，输出了解析结果的类型，然后将结果也输出出来。</p>
<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;<span class="class"><span class="keyword">class</span> '<span class="title">urllib</span>.<span class="title">parse</span>.<span class="title">ParseResult</span>'&gt; <span class="title">ParseResult</span><span class="params">(scheme=<span class="string">'http'</span>, netloc=<span class="string">'www.baidu.com'</span>, path=<span class="string">'/index.html'</span>, params=<span class="string">'user'</span>, query=<span class="string">'id=5'</span>, fragment=<span class="string">'comment'</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>可以看到，返回结果是一个<code>ParseResult</code>类型的对象，它包含6部分，分别是<code>scheme</code>、<code>netloc</code>、<code>path</code>、<code>params</code>、<code>query</code>和<code>fragment</code>。</p>
<p>观察一下该实例的URL：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.baidu.com/index.html;user?id=5#comment</span><br></pre></td></tr></table></figure>
<p>可以发现，<code>urlparse()</code>方法将其拆分成了6部分。大体观察可以发现，解析时有特定的分隔符。比如，://前面的就是<code>scheme</code>，代表协议；第一个/前面便是<code>netloc</code>，即域名；分号;前面是<code>params</code>，代表参数。</p>
<p>所以，可以得出一个标准的链接格式，具体如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scheme://netloc/path;parameters?query#fragment</span><br></pre></td></tr></table></figure>
<p>一个标准的URL都会符合这个规则，利用<code>urlparse()</code>方法可以将它拆分开来。</p>
<p>除了这种最基本的解析方式外，<code>urlparse()</code>方法还有其他配置吗？接下来，看一下它的API用法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.parse.urlparse(urlstring, scheme=<span class="string">''</span>, allow_fragments=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>可以看到，它有3个参数。</p>
<ul>
<li><strong>urlstring</strong>：这是必填项，即待解析的URL。</li>
<li><strong>scheme</strong>：它是默认的协议（比如<code>http</code>或<code>https</code>等）。假如这个链接没有带协议信息，会将这个作为默认的协议。我们用实例来看一下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ParseResult(scheme=&apos;https&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;)</span><br></pre></td></tr></table></figure>
<p>可以发现，我们提供的URL没有包含最前面的<code>scheme</code>信息，但是通过指定默认的<code>scheme</code>参数，返回的结果是<code>https</code>。</p>
<p>假设我们带上了<code>scheme</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>, scheme=<span class="string">'https'</span>)</span><br></pre></td></tr></table></figure>
<p>则结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;)</span><br></pre></td></tr></table></figure>
<p>可见，<code>scheme</code>参数只有在URL中不包含<code>scheme</code>信息时才生效。如果URL中有<code>scheme</code>信息，就会返回解析出的<code>scheme</code>。</p>
<ul>
<li><strong>allow_fragments</strong>：即是否忽略<code>fragment</code>。如果它被设置为<code>False</code>，<code>fragment</code>部分就会被忽略，它会被解析为<code>path</code>、<code>parameters</code>或者<code>query</code>的一部分，而<code>fragment</code>部分为空。下面我们用实例来看一下：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5#comment&apos;, fragment=&apos;&apos;)</span><br></pre></td></tr></table></figure>
<p>假设URL中不包含<code>params</code>和<code>query</code>，我们再通过实例看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html#comment&apos;, params=&apos;&apos;, query=&apos;&apos;, fragment=&apos;&apos;)</span><br></pre></td></tr></table></figure>
<p>可以发现，当URL中不包含<code>params</code>和<code>query</code>时，<code>fragment</code>便会被解析为<code>path</code>的一部分。</p>
<p>返回结果<code>ParseResult</code>实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名获取。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlparse</span><br><span class="line"> </span><br><span class="line">result = urlparse(<span class="string">'http://www.baidu.com/index.html#comment'</span>, allow_fragments=<span class="literal">False</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>], result.netloc, result[<span class="number">1</span>], sep=<span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure>
<p>这里我们分别用索引和属性名获取了<code>scheme</code>和<code>netloc</code>，其运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http</span><br><span class="line">http</span><br><span class="line">www.baidu.com</span><br><span class="line">www.baidu.com</span><br></pre></td></tr></table></figure>
<p>可以发现，二者的结果是一致的，两种方法都可以成功获取。</p>
<h4 id="urlsplit"><a href="#urlsplit" class="headerlink" title="urlsplit()"></a>urlsplit()</h4><p>这个方法和<code>urlparse()</code>方法非常相似，只不过它不再单独解析<code>params</code>这一部分，只返回5个结果。上面例子中的<code>params</code>会合并到<code>path</code>中。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"> </span><br><span class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SplitResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;)</span><br></pre></td></tr></table></figure>
<p>可以发现，返回结果是<code>SplitResult</code>，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlsplit</span><br><span class="line"> </span><br><span class="line">result = urlsplit(<span class="string">'http://www.baidu.com/index.html;user?id=5#comment'</span>)</span><br><span class="line">print(result.scheme, result[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http http</span><br></pre></td></tr></table></figure>
<h4 id="urlunsplit"><a href="#urlunsplit" class="headerlink" title="urlunsplit()"></a>urlunsplit()</h4><p>与<code>urlunparse()</code>类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为5。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlunsplit</span><br><span class="line"> </span><br><span class="line">data = [<span class="string">'http'</span>, <span class="string">'www.baidu.com'</span>, <span class="string">'index.html'</span>, <span class="string">'a=6'</span>, <span class="string">'comment'</span>]</span><br><span class="line">print(urlunsplit(data))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.baidu.com/index.html?a=6#comment</span><br></pre></td></tr></table></figure>
<h4 id="urljoin"><a href="#urljoin" class="headerlink" title="urljoin()"></a>urljoin()</h4><p>有了<code>urlunparse()</code>和<code>urlunsplit()</code>方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。</p>
<p>此外，生成链接还有另一个方法，那就是<code>urljoin()</code>方法。我们可以提供一个<code>base_url</code>（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析<code>base_url</code>的<code>scheme</code>、<code>netloc</code>和<code>path</code>这3个内容并对新链接缺失的部分进行补充，最后返回结果。</p>
<p>下面通过几个实例看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urljoin</span><br><span class="line"> </span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com/about.html'</span>, <span class="string">'https://cuiqingcai.com/FAQ.html?question=2'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com?wd=abc'</span>, <span class="string">'https://cuiqingcai.com/index.php'</span>))</span><br><span class="line">print(urljoin(<span class="string">'http://www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com'</span>, <span class="string">'?category=2#comment'</span>))</span><br><span class="line">print(urljoin(<span class="string">'www.baidu.com#comment'</span>, <span class="string">'?category=2'</span>))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">http://www.baidu.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html</span><br><span class="line">https://cuiqingcai.com/FAQ.html?question=2</span><br><span class="line">https://cuiqingcai.com/index.php</span><br><span class="line">http://www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?category=2#comment</span><br><span class="line">www.baidu.com?category=2</span><br></pre></td></tr></table></figure>
<p>可以发现，<code>base_url</code>提供了三项内容<code>scheme</code>、<code>netloc</code>和<code>path</code>。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而<code>base_url</code>中的<code>params</code>、<code>query</code>和<code>fragment</code>是不起作用的。</p>
<p>通过<code>urljoin()</code>方法，我们可以轻松实现链接的解析、拼合与生成。</p>
<h4 id="urlencode"><a href="#urlencode" class="headerlink" title="urlencode()"></a>urlencode()</h4><p>这里我们再介绍一个常用的方法——<code>urlencode()</code>，它在构造GET请求参数的时候非常有用，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> urlencode</span><br><span class="line"> </span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">base_url = <span class="string">'http://www.baidu.com?'</span></span><br><span class="line">url = base_url + urlencode(params)</span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure>
<p>这里首先声明了一个字典来将参数表示出来，然后调用<code>urlencode()</code>方法将其序列化为GET请求参数。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://www.baidu.com?name=germey&amp;age=22</span><br></pre></td></tr></table></figure>
<p>可以看到，参数就成功地由字典类型转化为GET请求参数了。</p>
<p>这个方法非常常用。有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为URL的参数时，只需要调用该方法即可。</p>
<h4 id="parse-qs"><a href="#parse-qs" class="headerlink" title="parse_qs()"></a>parse_qs()</h4><p>有了序列化，必然就有反序列化。如果我们有一串GET请求参数，利用<code>parse_qs()</code>方法，就可以将它转回字典，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qs</span><br><span class="line"> </span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qs(query))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1&#123;&apos;name&apos;: [&apos;germey&apos;], &apos;age&apos;: [&apos;22&apos;]&#125;</span><br></pre></td></tr></table></figure>
<h4 id="parse-qsl"><a href="#parse-qsl" class="headerlink" title="parse_qsl()"></a>parse_qsl()</h4><p>另外，还有一个<code>parse_qsl()</code>方法，它用于将参数转化为元组组成的列表，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> parse_qsl</span><br><span class="line"> </span><br><span class="line">query = <span class="string">'name=germey&amp;age=22'</span></span><br><span class="line">print(parse_qsl(query))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&apos;name&apos;, &apos;germey&apos;), (&apos;age&apos;, &apos;22&apos;)]</span><br></pre></td></tr></table></figure>
<p>可以看到，运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。</p>
<h4 id="quote"><a href="#quote" class="headerlink" title="quote()"></a>quote()</h4><p>该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"> </span><br><span class="line">keyword = <span class="string">'壁纸'</span></span><br><span class="line">url = <span class="string">'https://www.baidu.com/s?wd='</span> + quote(keyword)</span><br><span class="line">print(url)</span><br></pre></td></tr></table></figure>
<p>这里我们声明了一个中文的搜索文字，然后用<code>quote()</code>方法对其进行URL编码，最后得到的结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8</span><br></pre></td></tr></table></figure>
<p>可以看到，利用<code>unquote()</code>方法可以方便地实现解码。</p>
<p>本节中，我们介绍了<code>parse</code>模块的一些常用URL处理方法。有了这些方法，我们可以方便地实现URL的解析和构造，建议熟练掌握。</p>
<h3 id="Robots"><a href="#Robots" class="headerlink" title="Robots"></a>Robots</h3><p>Robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。</p>
<h4 id="robots-txt"><a href="#robots-txt" class="headerlink" title="robots.txt"></a>robots.txt</h4><p>当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在robots.txt文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。</p>
<p>下面我们看一个robots.txt的样例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Disallow: /</span><br><span class="line">Allow: /public/</span><br></pre></td></tr></table></figure>
<p>这实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。</p>
<p>上面的<code>User-agent</code>描述了搜索爬虫的名称，这里将其设置为*则代表该协议对任何爬取爬虫有效。比如，我们可以设置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">User-agent: Baiduspider</span><br></pre></td></tr></table></figure>
<p>这就代表我们设置的规则对百度爬虫是有效的。如果有多条<code>User-agent</code>记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。</p>
<p><code>Disallow</code>指定了不允许抓取的目录，比如上例子中设置为/则代表不允许抓取所有页面。</p>
<p><code>Allow</code>一般和<code>Disallow</code>一起使用，一般不会单独使用，用来排除某些限制。现在我们设置为<code>/public/</code>，则表示所有页面不允许抓取，但可以抓取public目录。</p>
<p>下面我们再来看几个例子。禁止所有爬虫访问任何目录的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User-agent: * </span><br><span class="line">Disallow: /</span><br></pre></td></tr></table></figure>
<p>允许所有爬虫访问任何目录的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Disallow:</span><br></pre></td></tr></table></figure>
<p>另外，直接把robots.txt文件留空也是可以的。</p>
<p>禁止所有爬虫访问网站某些目录的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">User-agent: *</span><br><span class="line">Disallow: /private/</span><br><span class="line">Disallow: /tmp/</span><br></pre></td></tr></table></figure>
<p>只允许某一个爬虫访问的代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">User-agent: WebCrawler</span><br><span class="line">Disallow:</span><br><span class="line">User-agent: *</span><br><span class="line">Disallow: /</span><br></pre></td></tr></table></figure>
<p>这些是robots.txt的一些常见写法。</p>
<h4 id="name-of-Spider"><a href="#name-of-Spider" class="headerlink" title="name of Spider"></a>name of Spider</h4><p>大家可能会疑惑，爬虫名是哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫作BaiduSpider。表3-1列出了一些常见的搜索爬虫的名称及对应的网站。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">爬虫名称</th>
<th style="text-align:center">名称</th>
<th style="text-align:center">网站</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">BaiduSpider</td>
<td style="text-align:center">百度</td>
<td style="text-align:center">www.baidu.com</td>
</tr>
<tr>
<td style="text-align:center">Googlebot</td>
<td style="text-align:center">谷歌</td>
<td style="text-align:center">www.google.com</td>
</tr>
<tr>
<td style="text-align:center">360Spider</td>
<td style="text-align:center">360搜索</td>
<td style="text-align:center">www.so.com</td>
</tr>
<tr>
<td style="text-align:center">YodaoBot</td>
<td style="text-align:center">有道</td>
<td style="text-align:center">www.youdao.com</td>
</tr>
<tr>
<td style="text-align:center">ia_archiver</td>
<td style="text-align:center">Alexa</td>
<td style="text-align:center">www.alexa.cn</td>
</tr>
<tr>
<td style="text-align:center">Scooter</td>
<td style="text-align:center">altavista</td>
<td style="text-align:center">www.altavista.com</td>
</tr>
</tbody>
</table>
</div>
<h4 id="robotparser"><a href="#robotparser" class="headerlink" title="robotparser"></a>robotparser</h4><p>了解Robots协议之后，我们就可以使用<code>robotparser</code>模块来解析robots.txt了。该模块提供了一个类<code>RobotFileParser</code>，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。</p>
<p>该类用起来非常简单，只需要在构造方法里传入robots.txt的链接即可。首先看一下它的声明：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">urllib.robotparser.RobotFileParser(url=<span class="string">''</span>)</span><br></pre></td></tr></table></figure>
<p>当然，也可以在声明时不传入，默认为空，最后再使用<code>set_url()</code>方法设置一下也可。</p>
<p>下面列出了这个类常用的几个方法。</p>
<ul>
<li><strong>set_url()</strong>：用来设置robots.txt文件的链接。如果在创建<code>RobotFileParser</code>对象时传入了链接，那么就不需要再使用这个方法设置了。</li>
<li><strong>read()</strong>：读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为<code>False</code>，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。</li>
<li><strong>parse()</strong>：用来解析robots.txt文件，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。</li>
<li><strong>can_fetch()</strong>：该方法传入两个参数，第一个是<code>User-agent</code>，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是<code>True</code>或<code>False</code>。</li>
<li><strong>mtime()</strong>：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。</li>
<li><strong>modified()</strong>：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。</li>
</ul>
<p>下面我们用实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"> </span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.set_url(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br><span class="line">rp.read()</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br></pre></td></tr></table></figure>
<p>这里以简书为例，首先创建<code>RobotFileParser</code>对象，然后通过<code>set_url()</code>方法设置了robots.txt的链接。当然，不用这个方法的话，可以在声明时直接用如下方法设置：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rp = RobotFileParser(<span class="string">'http://www.jianshu.com/robots.txt'</span>)</span><br></pre></td></tr></table></figure>
<p>接着利用<code>can_fetch()</code>方法判断了网页是否可以被抓取。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<p>这里同样可以使用<code>parse()</code>方法执行读取和分析，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> urllib.robotparser <span class="keyword">import</span> RobotFileParser</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"> </span><br><span class="line">rp = RobotFileParser()</span><br><span class="line">rp.parse(urlopen(<span class="string">'http://www.jianshu.com/robots.txt'</span>).read().decode(<span class="string">'utf-8'</span>).split(<span class="string">'\n'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">'http://www.jianshu.com/p/b67554025d7d'</span>))</span><br><span class="line">print(rp.can_fetch(<span class="string">'*'</span>, <span class="string">"http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections"</span>))</span><br></pre></td></tr></table></figure>
<p>运行结果一样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">True</span><br><span class="line">False</span><br></pre></td></tr></table></figure>
<h2 id="request-1"><a href="#request-1" class="headerlink" title="request"></a>request</h2><p>上一节中，我们了解了urllib的基本用法，但是其中确实有不方便的地方，比如处理网页验证和Cookies时，需要写<code>Opener</code>和<code>Handler</code>来处理。为了更加方便地实现这些操作，就有了更为强大的库requests，有了它，Cookies、登录验证、代理设置等操作都不是事儿。</p>
<p>接下来，让我们领略一下它的强大之处吧。</p>
<h3 id="实例引入"><a href="#实例引入" class="headerlink" title="实例引入"></a>实例引入</h3><p>urllib库中的<code>urlopen()</code>方法实际上是以GET方式请求网页，而requests中相应的方法就是<code>get()</code>方法，是不是感觉表达更明确一些？下面通过实例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">'https://www.baidu.com/'</span>)</span><br><span class="line">print(type(r))</span><br><span class="line">print(r.status_code)</span><br><span class="line">print(type(r.text))</span><br><span class="line">print(r.text)</span><br><span class="line">print(r.cookies)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;requests.models.Response&apos;&gt;</span><br><span class="line">200</span><br><span class="line">&lt;class &apos;str&apos;&gt;</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class=&quot;bg s_ipt_wr&quot;&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus&gt;&lt;/span&gt;&lt;span class=&quot;bg s_btn_wr&quot;&gt;&lt;input type=submit id=su value=ç¾åº¦ä¸ä¸ class=&quot;bg s_btn&quot; autofocus&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;æ°é»&lt;/a&gt; &lt;a href=https://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;å°å¾&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;è§é¢&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;è´´å§&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;amp;tpl=mn&amp;amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;ç»å½&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write(&apos;&lt;a href=&quot;http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=&apos;+ encodeURIComponent(window.location.href+ (window.location.search === &quot;&quot; ? &quot;?&quot; : &quot;&amp;&quot;)+ &quot;bdorz_come=1&quot;)+ &apos;&quot; name=&quot;tj_login&quot; class=&quot;lb&quot;&gt;ç»å½&lt;/a&gt;&apos;);</span><br><span class="line">                &lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style=&quot;display: block;&quot;&gt;æ´å¤äº§å&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;å³äºç¾åº¦&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;ä½¿ç¨ç¾åº¦åå¿è¯»&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;æè§åé¦&lt;/a&gt;&amp;nbsp;äº¬ICPè¯030173å·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;</span><br><span class="line"></span><br><span class="line">&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt;</span><br></pre></td></tr></table></figure>
<p>这里我们调用<code>get()</code>方法实现与<code>urlopen()</code>相同的操作，得到一个<code>Response</code>对象，然后分别输出了<code>Response</code>的类型、状态码、响应体的类型、内容以及Cookies。</p>
<p>通过运行结果可以发现，它的返回类型是<code>requests.models.Response</code>，响应体的类型是字符串<code>str</code>，Cookies的类型是<code>RequestsCookieJar</code>。</p>
<p>使用<code>get()</code>方法成功实现一个GET请求，这倒不算什么，更方便之处在于其他的请求类型依然可以用一句话来完成，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">r = requests.post(<span class="string">'http://httpbin.org/post'</span>)</span><br><span class="line">r = requests.put(<span class="string">'http://httpbin.org/put'</span>)</span><br><span class="line">r = requests.delete(<span class="string">'http://httpbin.org/delete'</span>)</span><br><span class="line">r = requests.head(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">r = requests.options(<span class="string">'http://httpbin.org/get'</span>)</span><br></pre></td></tr></table></figure>
<p>这里分别用<code>post()</code>、<code>put()</code>、<code>delete()</code>等方法实现了POST、PUT、DELETE等请求。是不是比urllib简单太多了？</p>
<p>其实这只是冰山一角，更多的还在后面。</p>
<h3 id="GET-请求"><a href="#GET-请求" class="headerlink" title="GET 请求"></a>GET 请求</h3><h4 id="基本实例"><a href="#基本实例" class="headerlink" title="基本实例"></a>基本实例</h4><p>首先，构建一个最简单的GET请求，请求的链接为<code>http://httpbin.org/get</code>，该网站会判断如果客户端发起的是GET请求的话，它返回相应的请求信息：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">'http://httpbin.org/get'</span>)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;&#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*/*&quot;, </span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;origin&quot;: &quot;117.179.245.150, 117.179.245.150&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;https://httpbin.org/get&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以发现，我们成功发起了GET请求，返回结果中包含请求头、URL、IP等信息。</p>
<p>那么，对于GET请求，如果要附加额外的信息，一般怎样添加呢？比如现在想添加两个参数，其中<code>name</code>是<code>germey</code>，<code>age</code>是22。要构造这个请求链接，是不是要直接写成：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = requests.get(<span class="string">'http://httpbin.org/get?name=germey&amp;age=22'</span>)</span><br></pre></td></tr></table></figure>
<p>这样也可以，但是是不是有点不人性化呢？一般情况下，这种信息数据会用字典来存储。那么，怎样来构造这个链接呢？</p>
<p>这同样很简单，利用<code>params</code>这个参数就好了，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">'name'</span>: <span class="string">'germey'</span>,</span><br><span class="line">    <span class="string">'age'</span>: <span class="number">22</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>, params=data)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;args&quot;: &#123;</span><br><span class="line">    &quot;age&quot;: &quot;22&quot;, </span><br><span class="line">    &quot;name&quot;: &quot;germey&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;headers&quot;: &#123;</span><br><span class="line">    &quot;Accept&quot;: &quot;*/*&quot;, </span><br><span class="line">    &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, </span><br><span class="line">    &quot;Host&quot;: &quot;httpbin.org&quot;, </span><br><span class="line">    &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;origin&quot;: &quot;117.179.245.150, 117.179.245.150&quot;, </span><br><span class="line">  &quot;url&quot;: &quot;https://httpbin.org/get?name=germey&amp;age=22&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>通过运行结果可以判断，请求的链接自动被构造成了：<code>http://httpbin.org/get?age=22&amp;name=germey</code>。</p>
<p>另外，网页的返回类型实际上是<code>str</code>类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个字典格式的话，可以直接调用<code>json()</code>方法。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">"http://httpbin.org/get"</span>)</span><br><span class="line">print(type(r.text))</span><br><span class="line">print(r.json())</span><br><span class="line">print(type(r.json()))</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;str&apos;&gt;</span><br><span class="line">&#123;&apos;args&apos;: &#123;&#125;, &apos;headers&apos;: &#123;&apos;Accept&apos;: &apos;*/*&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Host&apos;: &apos;httpbin.org&apos;, &apos;User-Agent&apos;: &apos;python-requests/2.21.0&apos;&#125;, &apos;origin&apos;: &apos;117.179.245.150, 117.179.245.150&apos;, &apos;url&apos;: &apos;https://httpbin.org/get&apos;&#125;</span><br><span class="line">&lt;class &apos;dict&apos;&gt;</span><br></pre></td></tr></table></figure>
<p>可以发现，调用<code>json()</code>方法，就可以将返回结果是JSON格式的字符串转化为字典。</p>
<p>但需要注意的书，如果返回结果不是JSON格式，便会出现解析错误，抛出<code>json.decoder.JSONDecodeError</code>异常。</p>
<h4 id="抓取网页"><a href="#抓取网页" class="headerlink" title="抓取网页"></a>抓取网页</h4><p>上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容了。下面以“知乎”$\rightarrow$“发现”页面为例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"> </span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'</span></span><br><span class="line">&#125;</span><br><span class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>, headers=headers)</span><br><span class="line">pattern = re.compile(<span class="string">'explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;'</span>, re.S)</span><br><span class="line">titles = re.findall(pattern, r.text)</span><br><span class="line">print(titles)</span><br></pre></td></tr></table></figure>
<p>这里我们加入了<code>headers</code>信息，其中包含了<code>User-Agent</code>字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取。</p>
<p>接下来我们用到了最基础的正则表达式来匹配出所有的问题内容。关于正则表达式的相关内容，我们会在3.3节中详细介绍，这里作为实例来配合讲解。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="抓取二进制数据"><a href="#抓取二进制数据" class="headerlink" title="抓取二进制数据"></a>抓取二进制数据</h4><p>在上面的例子中，我们抓取的是知乎的一个页面，实际上它返回的是一个HTML文档。如果想抓去图片、音频、视频等文件，应该怎么办呢？</p>
<p>图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制码。</p>
<p>下面以GitHub的站点图标为例来看一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</span><br><span class="line">print(r.text)</span><br><span class="line">print(r.content)</span><br></pre></td></tr></table></figure>
<p>这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标，</p>
<p><img src="/2019/08/21/WebScraping19/2.png" alt=""></p>
<p>这里打印了<code>Response</code>对象的两个属性，一个是<code>text</code>，另一个是<code>content</code>。</p>
<p>运行结果如图3-4所示，其中前两行是<code>r.text</code>的结果，最后一行是<code>r.content</code>的结果。</p>
<p><img src="/2019/08/21/WebScraping19/3.png" alt=""></p>
<p>可以注意到，前者出现了乱码，后者结果前带有一个<code>b</code>，这代表是<code>bytes</code>类型的数据。由于图片是二进制数据，所以前者在打印时转化为<code>str</code>类型，也就是图片直接转化为字符串，这理所当然会出现乱码。</p>
<p>接着，我们将刚才提取到的图片保存下来：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">"https://github.com/favicon.ico"</span>)</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'favicon.ico'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(r.content)</span><br></pre></td></tr></table></figure>
<p>这里用了<code>open()</code>方法，它的第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据。</p>
<p>运行结束之后，可以发现在文件夹中出现了名为favicon.ico的图标，如图3-5所示。</p>
<p><img src="/2019/08/21/WebScraping19/4.png" alt=""></p>
<p>同样地，音频和视频文件也可以用这种方法获取。</p>
<h4 id="添加headers"><a href="#添加headers" class="headerlink" title="添加headers"></a>添加headers</h4><p>与<code>urllib.request</code>一样，我们也可以通过<code>headers</code>参数来传递头信息。</p>
<p>比如，在上面“知乎”的例子中，如果不传递<code>headers</code>，就不能正常请求：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">"https://www.zhihu.com/explore"</span>)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span><span class="tag">&lt;<span class="name">title</span>&gt;</span>400 Bad Request<span class="tag">&lt;/<span class="name">title</span>&gt;</span><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span> <span class="attr">bgcolor</span>=<span class="string">"white"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">center</span>&gt;</span><span class="tag">&lt;<span class="name">h1</span>&gt;</span>400 Bad Request<span class="tag">&lt;/<span class="name">h1</span>&gt;</span><span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">hr</span>&gt;</span><span class="tag">&lt;<span class="name">center</span>&gt;</span>openresty<span class="tag">&lt;/<span class="name">center</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>但如果加上<code>headers</code>并加上<code>User-Agent</code>信息，那就没问题了：</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!doctype html&gt;</span><br><span class="line">&lt;html lang="zh" data-hairline="true" data-theme="light"&gt;&lt;head&gt;&lt;meta name="description" property="og:description" content="有问题，上知乎。知乎，可信赖的问答社区，以让每个人高效获得可信赖的解答为使命。知乎凭借认真、专业和友善的社区氛围，结构化、易获得的优质内容，基于问答的内容生产方式和独特的社区机制，吸引、聚集了各行各业中大量的亲历者、内行人、领域专家、领域爱好者，将高质量的内容透过人的节点来成规模地生产和分享。用户通过问答等交流方式建立信任和连接，打造和提升个人影响力，并发现、获得新机会。"src="https://static.zhihu.com/heifetz/main.app.7c8634e8d9de8fd5d961.js"&gt;&lt;/script&gt;&lt;script src="https://static.zhihu.com/heifetz/main.explore-routes.d628322decb4a68a77e7.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;</span><br></pre></td></tr></table></figure>
<p>当然，我们可以在<code>headers</code>这个参数中任意添加其他的字段信息。</p>
<h4 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h4><p>前面我们了解了最基本的GET请求，另外一种比较常见的请求方式是POST。使用<code>requests</code>实现POST请求同样非常简单，示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">data = &#123;<span class="string">'name'</span>: <span class="string">'germey'</span>, <span class="string">'age'</span>: <span class="string">'22'</span>&#125;</span><br><span class="line">r = requests.post(<span class="string">"http://httpbin.org/post"</span>, data=data)</span><br><span class="line">print(r.text)</span><br></pre></td></tr></table></figure>
<p>这里还是请求<code>http://httpbin.org/post</code>，该网站可以判断如果请求是POST方式，就把相关请求信息返回。</p>
<p>运行结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"args"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"data"</span>: <span class="string">""</span>, </span><br><span class="line">  <span class="string">"files"</span>: &#123;&#125;, </span><br><span class="line">  <span class="string">"form"</span>: &#123;</span><br><span class="line">    <span class="string">"age"</span>: <span class="string">"22"</span>, </span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"germey"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">"headers"</span>: &#123;</span><br><span class="line">    <span class="string">"Accept"</span>: <span class="string">"*/*"</span>, </span><br><span class="line">    <span class="string">"Accept-Encoding"</span>: <span class="string">"gzip, deflate"</span>, </span><br><span class="line">    <span class="string">"Content-Length"</span>: <span class="string">"18"</span>, </span><br><span class="line">    <span class="string">"Content-Type"</span>: <span class="string">"application/x-www-form-urlencoded"</span>, </span><br><span class="line">    <span class="string">"Host"</span>: <span class="string">"httpbin.org"</span>, </span><br><span class="line">    <span class="string">"User-Agent"</span>: <span class="string">"python-requests/2.21.0"</span></span><br><span class="line">  &#125;, </span><br><span class="line">  <span class="string">"json"</span>: null, </span><br><span class="line">  <span class="string">"origin"</span>: <span class="string">"117.179.245.150, 117.179.245.150"</span>, </span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"https://httpbin.org/post"</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以发现，我们成功获得了返回结果，其中<code>form</code>部分就是提交的数据，这就证明POST请求成功发送了。</p>
<h4 id="响应"><a href="#响应" class="headerlink" title="响应"></a>响应</h4><p>发送请求后，得到的自然就是响应。在上面的实例中，我们使用<code>text</code>和<code>content</code>获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies等。示例如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">'http://www.jianshu.com'</span>)</span><br><span class="line">print(type(r.status_code), r.status_code)</span><br><span class="line">print(type(r.headers), r.headers)</span><br><span class="line">print(type(r.cookies), r.cookies)</span><br><span class="line">print(type(r.url), r.url)</span><br><span class="line">print(type(r.history), r.history)</span><br></pre></td></tr></table></figure>
<p>这里分别打印输出<code>status_code</code>属性得到状态码，输出<code>headers</code>属性得到响应头，输出<code>cookies</code>属性得到Cookies，输出<code>url</code>属性得到URL，输出<code>history</code>属性得到请求历史。</p>
<p>运行结果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;int&apos;&gt; 403</span><br><span class="line">&lt;class &apos;requests.structures.CaseInsensitiveDict&apos;&gt; &#123;&apos;Server&apos;: &apos;Tengine&apos;, &apos;Content-Type&apos;: &apos;text/html&apos;, &apos;Transfer-Encoding&apos;: &apos;chunked&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;Date&apos;: &apos;Wed, 21 Aug 2019 10:57:51 GMT&apos;, &apos;Vary&apos;: &apos;Accept-Encoding&apos;, &apos;Strict-Transport-Security&apos;: &apos;max-age=31536000; includeSubDomains; preload&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;, &apos;x-alicdn-da-ups-status&apos;: &apos;endOs,0,403&apos;, &apos;Via&apos;: &apos;cache25.l2nu16-1[3,0], cache4.cn1252[27,0]&apos;, &apos;Timing-Allow-Origin&apos;: &apos;*&apos;, &apos;EagleId&apos;: &apos;6f28b09815663850714812872e&apos;&#125;</span><br><span class="line">&lt;class &apos;requests.cookies.RequestsCookieJar&apos;&gt; &lt;RequestsCookieJar[]&gt;</span><br><span class="line">&lt;class &apos;str&apos;&gt; https://www.jianshu.com/</span><br><span class="line">&lt;class &apos;list&apos;&gt; [&lt;Response [301]&gt;]</span><br></pre></td></tr></table></figure>
<p>因为<code>session_id</code>过长，在此简写。可以看到，<code>headers</code>和<code>cookies</code>这两个属性得到的结果分别是<code>CaseInsensitiveDict</code>和<code>RequestsCookieJar</code>类型。</p>
<p>状态码常用来判断请求是否成功，而requests还提供了一个内置的状态码查询对象<code>requests.codes</code>，示例如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"> </span><br><span class="line">r = requests.get(<span class="string">'http://www.jianshu.com'</span>)</span><br><span class="line">exit() <span class="keyword">if</span> <span class="keyword">not</span> r.status_code == requests.codes.ok <span class="keyword">else</span> print(<span class="string">'Request Successfully'</span>)</span><br></pre></td></tr></table></figure>
<p>这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用<code>requests.codes.ok</code>得到的是成功的状态码200。</p>
<p>那么，肯定不能只有<code>ok</code>这个条件码。下面列出了返回码和相应的查询条件：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"># 信息性状态码</span><br><span class="line">100: (&apos;continue&apos;,),</span><br><span class="line">101: (&apos;switching_protocols&apos;,),</span><br><span class="line">102: (&apos;processing&apos;,),</span><br><span class="line">103: (&apos;checkpoint&apos;,),</span><br><span class="line">122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;),</span><br><span class="line"> </span><br><span class="line"># 成功状态码</span><br><span class="line">200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\o/&apos;, &apos;✓&apos;),</span><br><span class="line">201: (&apos;created&apos;,),</span><br><span class="line">202: (&apos;accepted&apos;,),</span><br><span class="line">203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),</span><br><span class="line">204: (&apos;no_content&apos;,),</span><br><span class="line">205: (&apos;reset_content&apos;, &apos;reset&apos;),</span><br><span class="line">206: (&apos;partial_content&apos;, &apos;partial&apos;),</span><br><span class="line">207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),</span><br><span class="line">208: (&apos;already_reported&apos;,),</span><br><span class="line">226: (&apos;im_used&apos;,),</span><br><span class="line"> </span><br><span class="line"># 重定向状态码</span><br><span class="line">300: (&apos;multiple_choices&apos;,),</span><br><span class="line">301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\o-&apos;),</span><br><span class="line">302: (&apos;found&apos;,),</span><br><span class="line">303: (&apos;see_other&apos;, &apos;other&apos;),</span><br><span class="line">304: (&apos;not_modified&apos;,),</span><br><span class="line">305: (&apos;use_proxy&apos;,),</span><br><span class="line">306: (&apos;switch_proxy&apos;,),</span><br><span class="line">307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),</span><br><span class="line">308: (&apos;permanent_redirect&apos;,</span><br><span class="line">      &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0</span><br><span class="line"> </span><br><span class="line"># 客户端错误状态码</span><br><span class="line">400: (&apos;bad_request&apos;, &apos;bad&apos;),</span><br><span class="line">401: (&apos;unauthorized&apos;,),</span><br><span class="line">402: (&apos;payment_required&apos;, &apos;payment&apos;),</span><br><span class="line">403: (&apos;forbidden&apos;,),</span><br><span class="line">404: (&apos;not_found&apos;, &apos;-o-&apos;),</span><br><span class="line">405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),</span><br><span class="line">406: (&apos;not_acceptable&apos;,),</span><br><span class="line">407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),</span><br><span class="line">408: (&apos;request_timeout&apos;, &apos;timeout&apos;),</span><br><span class="line">409: (&apos;conflict&apos;,),</span><br><span class="line">410: (&apos;gone&apos;,),</span><br><span class="line">411: (&apos;length_required&apos;,),</span><br><span class="line">412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),</span><br><span class="line">413: (&apos;request_entity_too_large&apos;,),</span><br><span class="line">414: (&apos;request_uri_too_large&apos;,),</span><br><span class="line">415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),</span><br><span class="line">416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),</span><br><span class="line">417: (&apos;expectation_failed&apos;,),</span><br><span class="line">418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),</span><br><span class="line">421: (&apos;misdirected_request&apos;,),</span><br><span class="line">422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),</span><br><span class="line">423: (&apos;locked&apos;,),</span><br><span class="line">424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),</span><br><span class="line">425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),</span><br><span class="line">426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),</span><br><span class="line">428: (&apos;precondition_required&apos;, &apos;precondition&apos;),</span><br><span class="line">429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),</span><br><span class="line">431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),</span><br><span class="line">444: (&apos;no_response&apos;, &apos;none&apos;),</span><br><span class="line">449: (&apos;retry_with&apos;, &apos;retry&apos;),</span><br><span class="line">450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),</span><br><span class="line">451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),</span><br><span class="line">499: (&apos;client_closed_request&apos;,),</span><br><span class="line"> </span><br><span class="line"># 服务端错误状态码</span><br><span class="line">500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\&apos;, &apos;✗&apos;),</span><br><span class="line">501: (&apos;not_implemented&apos;,),</span><br><span class="line">502: (&apos;bad_gateway&apos;,),</span><br><span class="line">503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),</span><br><span class="line">504: (&apos;gateway_timeout&apos;,),</span><br><span class="line">505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),</span><br><span class="line">506: (&apos;variant_also_negotiates&apos;,),</span><br><span class="line">507: (&apos;insufficient_storage&apos;,),</span><br><span class="line">509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),</span><br><span class="line">510: (&apos;not_extended&apos;,),</span><br><span class="line">511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;)</span><br></pre></td></tr></table></figure>
<p>比如，如果想判断结果是不是404状态，可以用<code>requests.codes.not_found</code>来比对。</p>

      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Web-Scraping/" rel="tag"># Web-Scraping</a>
          
            <a href="/tags/python/" rel="tag"># python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/08/17/WebScraping18/" rel="next" title="WebScraping-18">
                <i class="fa fa-chevron-left"></i> WebScraping-18
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/01/09/IGDplus/" rel="prev" title="IGDplus">
                IGDplus <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          
  <div class="comments" id="comments"></div>
    
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src="//cdn.jsdelivr.net/npm/valine@1.1.6/dist/Valine.min.js"></script>
    <script>
        new Valine({
            av: AV,
            el: '.comments',
            notify: true, // 邮件提醒 v1.1.4新增，下一步中有具体的邮箱设置
            verify: true,
            app_id: '',
            app_key: '',
            placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!'
        });
    </script>
    

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="spring">
            
              <p class="site-author-name" itemprop="name">spring</p>
              <p class="site-description motion-element" itemprop="description">梦想天空分外蓝~</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">50</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">12</span>
                    <span class="site-state-item-name">分类</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">27</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              



            </nav>
          

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/ninanxiaoguai" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:ninanxiaoguai@163.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#urllib"><span class="nav-number">1.</span> <span class="nav-text">urllib</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#request"><span class="nav-number">1.1.</span> <span class="nav-text">request</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#urlopen"><span class="nav-number">1.1.1.</span> <span class="nav-text">urlopen</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#data参数"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">data参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#timeout参数"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">timeout参数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#其他"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">其他</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Request"><span class="nav-number">1.1.2.</span> <span class="nav-text">Request</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#高级用法"><span class="nav-number">1.1.3.</span> <span class="nav-text">高级用法</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#代理"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">代理</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cookies"><span class="nav-number">1.1.4.</span> <span class="nav-text">Cookies</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#error"><span class="nav-number">1.2.</span> <span class="nav-text">error</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#URLError"><span class="nav-number">1.2.1.</span> <span class="nav-text">URLError</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HTTPError"><span class="nav-number">1.2.2.</span> <span class="nav-text">HTTPError</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#parse"><span class="nav-number">1.3.</span> <span class="nav-text">parse</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#urlparse"><span class="nav-number">1.3.1.</span> <span class="nav-text">urlparse()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#urlsplit"><span class="nav-number">1.3.2.</span> <span class="nav-text">urlsplit()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#urlunsplit"><span class="nav-number">1.3.3.</span> <span class="nav-text">urlunsplit()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#urljoin"><span class="nav-number">1.3.4.</span> <span class="nav-text">urljoin()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#urlencode"><span class="nav-number">1.3.5.</span> <span class="nav-text">urlencode()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parse-qs"><span class="nav-number">1.3.6.</span> <span class="nav-text">parse_qs()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#parse-qsl"><span class="nav-number">1.3.7.</span> <span class="nav-text">parse_qsl()</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#quote"><span class="nav-number">1.3.8.</span> <span class="nav-text">quote()</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Robots"><span class="nav-number">1.4.</span> <span class="nav-text">Robots</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#robots-txt"><span class="nav-number">1.4.1.</span> <span class="nav-text">robots.txt</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#name-of-Spider"><span class="nav-number">1.4.2.</span> <span class="nav-text">name of Spider</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#robotparser"><span class="nav-number">1.4.3.</span> <span class="nav-text">robotparser</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#request-1"><span class="nav-number">2.</span> <span class="nav-text">request</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实例引入"><span class="nav-number">2.1.</span> <span class="nav-text">实例引入</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#GET-请求"><span class="nav-number">2.2.</span> <span class="nav-text">GET 请求</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#基本实例"><span class="nav-number">2.2.1.</span> <span class="nav-text">基本实例</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#抓取网页"><span class="nav-number">2.2.2.</span> <span class="nav-text">抓取网页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#抓取二进制数据"><span class="nav-number">2.2.3.</span> <span class="nav-text">抓取二进制数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#添加headers"><span class="nav-number">2.2.4.</span> <span class="nav-text">添加headers</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#POST请求"><span class="nav-number">2.2.5.</span> <span class="nav-text">POST请求</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#响应"><span class="nav-number">2.2.6.</span> <span class="nav-text">响应</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<div class="copyright">
  <!--
  &copy; 

<span itemprop="copyrightYear">2020</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">spring</span>
-->
  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">站点总字数：</span>
    
    <span title="站点总字数">441k</span>
  

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    
      <span class="post-meta-item-text">站点阅读时长 &asymp;</span>
    
    <span title="站点阅读时长">6:41</span>
  
</div>




<span>
  Hosted by <a href="https://pages.coding.me" style="font-weight: bold">Coding Pages</a>
 </span>




<!--

<div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动 v3.9.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a class="theme-link" target="_blank" href="https://theme-next.org">NexT.Muse</a> v6.3.0</div>


-->



        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


















  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_lines.min.js"></script>
  


  



  <script type="text/javascript" src="/js/src/utils.js?v=6.3.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.3.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=6.3.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=6.3.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.3.0"></script>



  






  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: 'NUATd6a1PUQYtIT7MpFYzq4X-gzGzoHsz',
        appKey: 'Iny9yO41c59ioJOJc84vGygI',
        placeholder: '如果填写邮箱，可以直接回复至您邮箱中~',
        avatar:'mm',
        meta:guest,
        pageSize:'10' || 10,
        visitor: true
    });
  </script>



  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  

  
    
      <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      },
      TeX: {equationNumbers: { autoNumber: "AMS" }}
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  


  
  

  

  

  

  

  

</body>
</html>
