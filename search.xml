<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[R2-Hypervolume Contribution Approximation]]></title>
    <url>%2F2020%2F01%2F12%2FR2%2F</url>
    <content type="text"><![CDATA[继续学习Hisao老师的论文，这次同样是两篇论文：“A new R2 indicator for better hypervolume approximation”与“R2-based Hypervolume Contribution Approximation”，首先提出R2的新计算方式，然后通过新的R2-indicator来估计“Hypervolume Contribution Approximation”，以达到近似的效果，感觉这个与之前的IGD+的目的很相似，不过可以用来indicator-based MOEAs，也是我下一个的要进行的研究话题。 Preliminary需要介绍一些基本的几何知识。 $\lambda$ 为一个方向向量，$r^*$为一个点。$a$ 也为一个点。 下图为二维图中(m=2)： 公式 $g^{mtch}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)=\min _{j \in\{1, \ldots, m\}}\left\{\frac{\left|r_{j}^{*}-a_{j}\right|}{\lambda_{j}}\right\}$ 对应的几何含义，蓝色为等高线，因此如果 $a$ 为两个射线上的点，那么 $g^{t e}\left(\mathbf{a} | \mathbf{w}, \mathbf{r}^{*}\right)$ 的值是相同的。 相反，如果$g^{2 t c h}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)=\max _{j \in\{1, \ldots, m\}}\left\{\frac{\left|r_{j}^{*}-a_{j}\right|}{\lambda_{j}}\right\}$ ，公式是这样的，那么等高线如下： 因此，也就是说，不在 $\lambda$ 上的 $a$，可以在不改变结果的前提下转换至在 $\lambda$ 上的某个点。如果我们有 $A={a_1, a_2, a_3}$， 和一组权重向量(红色线)，那么我们可以知道，如果选用 $g^{mtch}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)=\min _{j \in\{1, \ldots, m\}}\left\{\frac{\left|r_{j}^{*}-a_{j}\right|}{\lambda_{j}}\right\}$ ，对于$\lambda_1$来说，$a_1$ 的位置等同于 $b_1$，其他点类似，具体例子如图： 那如果下图这样呢？正如公式$g^{2 t c h}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)=\max _{j \in\{1, \ldots, m\}}\left\{\frac{\left|r_{j}^{*}-a_{j}\right|}{\lambda_{j}}\right\}$ 所说，因为有了绝对值，因此 $a$ 与 $aa$ 等价，其他类似。 R2原来是这样的： R_{2}^{t e}\left(A, W, \mathbf{r}^{*}\right)=\frac{1}{|W|} \sum_{\mathbf{w} \in W} \min _{\mathbf{a} \in A}\left\{g^{t e}\left(\mathbf{a} | \mathbf{w}, \mathbf{r}^{*}\right)\right\}其中： g^{t e}\left(\mathbf{a} | \mathbf{w}, \mathbf{r}^{*}\right)=\max _{j \in\{1, \ldots, m\}}\left\{w_{j}\left|r_{j}^{*}-a_{j}\right|\right\}之后有人提出更有几何意义的： R_{2}^{2 t c h}\left(A, \Lambda, \mathbf{r}^{*}\right)=\frac{1}{|\Lambda|} \sum_{\lambda \in \Lambda} \min _{\mathbf{a} \in A}\left\{g^{2 t c h}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)\right\}​其中： g^{2 t c h}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)=\max _{j \in\{1, \ldots, m\}}\left\{\frac{\left|r_{j}^{*}-a_{j}\right|}{\lambda_{j}}\right\}下图为 $g^{2 t c h}\left(\mathbf{a} | \lambda, \mathbf{r}^{*}\right)$的一个实例， $\lambda$ 穿过 $r^*$ 与 $A$ 集合所形成的hypervolume区域相交与 $p$ 点。由上一节，我们可以得到这样结论：经过 $r^*$ 的很多个向量与 $A$ 的边界焦点，的所有线段和(粗略的估计，呈线性关系，并非严格意义上的相等)的平均值即为hypervolume(红色的面积)。 再之后，上图求的也不方便，于是有了下面的修改版，注意，$r^*$ 变成了 $r$ R_{2}^{m t c h}(A, \Lambda, \mathbf{r})=\frac{1}{|\Lambda|} \sum_{\lambda \in \Lambda} \max _{\mathbf{a} \in A}\left\{g^{m t c h}(\mathbf{a} | \lambda, \mathbf{r})\right\}其中： g^{m t c h}(\mathbf{a} | \lambda, \mathbf{r})=\min _{j \in\{1, \ldots, m\}}\left\{\frac{\left|r_{j}-a_{j}\right|}{\lambda_{j}}\right\}其几何含义： 以上皆为前人的工作，此论文主要做了一个修改，变为： R_{2}^{n e w}(A, \Lambda, \mathbf{r})=\frac{1}{|\Lambda|} \sum_{\lambda \in \Lambda} \max _{\mathbf{a} \in A}\left\{g^{m t c h}(\mathbf{a} | \lambda, \mathbf{r})\right\}^{m}其中：$g^{m t c h}$没有变化，仅仅是加了一个指数幂！但结果变得异常好，同时也有了较为严谨一点点的理论证明。简直为画龙点睛。 Hypervolume Contribution ApproximationBackground众所周知，HV的计算公式如下： H V(A, \mathbf{r})=\mathcal{L}\left(\bigcup_{\mathbf{a} \in A}\{\mathbf{b} | \mathbf{a} \succ \mathbf{b} \succ \mathbf{r}\}\right)而对于一个点 $a$ 的hypervolume contribution 即为： C_{H V}(\mathbf{s}, A, \mathbf{r})=H V(A, \mathbf{r})-H V(A \backslash\{\mathbf{s}\}, \mathbf{r})比较形象的图为： 阴影部分一次为$a_1, a_2, a_3$的贡献值。 如果通过R2来计算的话，步骤如下： 传统方法的缺点如下： 为了得到一个解的超体积贡献近似，我们需要计算两个解集的R2值。超容量的贡献不能直接近似。 超体积的R2近似通常存在误差。因此，超体积贡献近似的误差可以被两个R2值的误差放大。因此，传统方法的逼近精度较低。 为了提高逼近精度，需要大量的向量来计算R2指标。因此，传统方法的计算量可以很大，从而达到很高的近似精度。 New method上面的方法可以看作所有的方向向量的起点都是 $r$，这样越到后面便会越稀疏，对结果会造成影响，因此可以以 $s$ 点为焦点开始放射方向向量。如图，分两种情况： 针对这两个情况，列出不同的式子来解决这个问题。 First situation 第一种情况，如果要计算的 $s$ 的贡献不在边界，那么我们想要得到的情况应该如右图所示，等高线是这样的形式。 L(\mathbf{s}, A, \lambda)=\min _{\mathbf{a} \in A \backslash\{\mathbf{s}\}}\left\{g^{* 2 \operatorname{th}}(\mathbf{a} | \lambda, \mathbf{s})\right\}其中，最大化问题： g^{* 2 \operatorname{tch}}(\mathbf{a} | \lambda, \mathbf{s})=\max _{j \in\{1, \ldots, m\}}\left\{\frac{s_{j}-a_{j}}{\lambda_{j}}\right\}最小化问题： g^{* 2 \operatorname{tch}}(\mathbf{a} | \lambda, \mathbf{s})=\max _{j \in\{1, \ldots, m\}}\left\{\frac{a_{j}-s_{j}}{\lambda_{j}}\right\}分析：在最大化问题中，与之前的差别在于没有了绝对值，这样便可以不考虑 $s_j &lt; a_j$ 的那些维度，因此可以吧这些维度的坐标当作0。把原公式转化为：$a_1, p, a_1’, a’’_1$ 的 $g^{* 2 \operatorname{tch}}(\mathbf{a} | \lambda, \mathbf{s})$ 值是一样的(蓝色线)，无论 $a_1$ 的纵坐标如何增加。 但是如果不去掉绝对值，那么当 $a_1$ 的纵坐标大到一定程度时($b$ 高于$a_1’’$ )，便会错误。紫色的便为添加绝对值后我们不想得到的等高线。 Second situation 第二种情况，又细分了两种(b)(c)。 $sp_1=sr &lt; sa_2&lt;sa_3$ $sp_2=sa_2&lt;sa_3&lt;sr$ 我们知道： L(\mathbf{s}, \mathbf{r}, \lambda)=g^{\mathrm{mtch}}(\mathbf{r} | \lambda, \mathbf{s})其中： g^{\operatorname{mtch}}(\mathbf{r} | \lambda, \mathbf{s})=\min _{j \in\{1, \ldots, m\}}\left\{\frac{\left|s_{j}-r_{j}\right|}{\lambda_{j}}\right\}类似的分析，我们可以知道： L(\mathbf{s}, A, \mathbf{r}, \lambda)=\min \{L(\mathbf{s}, A, \lambda), L(\mathbf{s}, \mathbf{r}, \lambda)\}为什么考虑 $r$ 的时候，变成了 $g^{mtch}$。 首先，我们要保证经过我们设计的操作后，等高线还是这样的(下图红色，即$sr$长度不变)，我们可以平移 $\lambda$ 使它经过 $r$，与我们之前处理的情景像，这样就要使 $s$ 向下平移至 $s’$。这样也可以说明$ss’$是一条等高线，因此，根据preliminary中，我们要选择 $g^{mtch}$。 因此，综上： \begin{array}{l} {R_{2}^{\mathrm{HVC}}(\mathbf{s}, A, \Lambda, \mathbf{r}, \alpha)=\frac{1}{|\Lambda|} \sum_{\lambda \in \Lambda} L(\mathbf{s}, A, \mathbf{r}, \lambda)^{\alpha}} \\ {=\frac{1}{|\Lambda|} \sum_{\lambda \in \Lambda} \min \left\{\min _{\mathbf{a} \in A \backslash\{\mathbf{s}\}}\left\{g^{* 2 \operatorname{tch}}(\mathbf{a} | \lambda, \mathbf{s})\right\}, g^{\mathrm{mtch}}(\mathbf{r} | \lambda, \mathbf{s})\right\}^{\alpha}} \end{array}]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>indicators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IGDplus]]></title>
    <url>%2F2020%2F01%2F09%2FIGDplus%2F</url>
    <content type="text"><![CDATA[最近还在学习多目标优化，在于其他算法比较时，就会用到indicator，发现在PlatEMO平台上，HV做了归一化，而IGD并没有做归一化处理，这样可能就会出现一些不对等的信息，出现这个问题的原因之一是在计算IGD时的采样点的方法不是很好。另外最近看到了IGD的升级版-IGD+，此版本是这学期的算法课老师Hisao Ishibuchi提出，也抱着慕名的心情拜读了一下，一共分为两个论文，其一是“Modified Distance Calculation in Generational Distance and Inverted Generational Distance” 与 “Comparison of Hypervolume, IGD and IGD+ from the Viewpoint of Optimal Distributions of Solutions” 很容易看出，第一个为提出IGD+算法，然后进一步与IGD与HV进行对比。 IGD 的不足这里有一个概念叫做：Pareto Compliant indicator，两个集合A与B，如果，总会有 A支配B。类似的还有Weak Pareto Compliant indicator，，总会有A弱支配B。目前HV是我们知道的唯一一个满足Pareto Compliant indicator的。IGD上面两个都不满足。 下图就是很好的例子，可以看出，A集是优于B集合的，但IGD会得出相反的结果。 于是提出了IGD+的概念 分析：在计算IGD中，会遇到两种： 左图中，，因此求出的IGD结果的结果是正确的，但相对于右图而言， 且 ，这样的IGD结果便会出现误导。 IGD+如图： 对于左面，没有修改；对于右面，只考虑了之间的差距。这是因为在上 优于 。数学表达式为： 其中，m为目标数， 新的IGD版本，满足Weak Pareto Compliant indicator，具体证明原论文有给出。 代码实现也很简单： 12345678910function Score = IGDplus(PopObj,PF)NP = size(PopObj,1);res = ones(size(PF,1),1);for i = 1:size(PF,1) tem = PopObj - repmat(PF(i,:),NP,1); tem(tem &lt;0) = 0; res(i) = min(sqrt(sum(tem.*tem,2)));endScore = mean(res);end IGD, HV, IGD+ 对比IGD 与 IGD+ 根据收敛性来说，绿色的点比紫色的点更好，IGD不可以正确表达，而IGD+可以正确表达。 二维分布经过其他实验研究表面： 当帕累托前沿为非线性时，通过HV计算的解的最优分布取决于帕累托前沿的斜率。 当帕累托前沿为线性时，HV指标通过均匀分布的解得到最大。 开始实验对比： 对于HV需要一个参考点，对于二维来说，分别设置：(1, 1), (1.1, 1.1), (2, 2) 与 (10, 10)。PF分别设置线性，凸，凹。将三个指标带入SMS-MOEA算法中。 线性： 在线性条件下： HV指标的点是均匀分布的，并且有关研究表明，当 时(是参考点的个数，r是选取的参考点)，两个极端点是被包括在其中的。 当Pareto前沿为线性时，通过均匀分布的解最小化IGD指标。 当Pareto前沿为线性时，IGD+指标似乎也被均匀分布的解最小化 因此，在线性中，三者很相似。 convex 与 concave 图5与6均是非线性。 HV：最大化解集的最优分布取决于Pareto前沿的斜率。解集的紧凑度最高的地方是斜率为45°的区域。从45°斜坡开始，密度随坡度的增大或减小而减小。这样，在帕累托前缘的中心就得到了更多的解。可以观察上图5、6的(a)-(d) 在convex中，在帕累托前沿的两个极端点附近的解决方案不能有较大的高压贡献。因此，即使在r = 2.0的图6(c)中，两个极值点附近也没有解。 在concave中，两个极值点的HV贡献较大。如图5(b) - (d)所示。当r = 1.0作为计算HV的参考点，这两个极值点与帕累托前沿的形状无关 。这是因为当r = 1.0时，他们的HV贡献总是0。 IGD：我们可以看到，独立于Pareto前沿形状的IGD极小化得到了相似的分布。IGD最小化是指从Pareto前沿的每个参考点到最近解的平均距离最小化。这个问题可以看作是给定参考点的聚类问题。在图4、图5和图6(e)中，给定的1,001个参考点被分成11个簇。也就是说，每个图中的Pareto front被划分为11条线或曲线。每个线段(长度约为帕累托前1/11)的非线性较弱。也就是说，即使帕累托前沿是非线性的，每个线段也类似于一条直线。因此，在图4、图5和图6(e)中得到了相似的IGD最小化分布。 图5和图6中一个有趣的观察结果是，(f)对于IGD+最小化的结果与(b)对于HV最大化的结果(r = 1.1)相比结果更相似。这可以通过图2(b)所示的IGD+计算机制来解释。从图2 (b)，我们可以看到，IGD +是大约的计算评估的区别真正的帕累托解集和主导地区的前面。这种差异的最小化是解集的高压的最大化。也就是说，IGD +最大化与HV最小化密切相关。因此，(f)中的IGD+极小化和(b)中的HV极大化得到了相似的解分布，如图5和图6所示。如图5(f)和图6(f)所示，在坡度接近0或90的区域，所得到的解非常接近支配区域。因此，在这些地区不需要许多解决方案来最小化IGD+。需要在具有45度斜度的帕累托前缘中心附近找到更多的解。两个极值点对于IGD+最小化的重要性取决于Pareto前沿的形状(即，凹或凸)。IGD+最优分布的这些特征与HV最优分布的特征相同。 三维分布实验分析的测试问题为： 结果如下： 实验结果如图7、8、9、10、11、12所示。 HV：在图7的线性三角帕累托前缘，计算HV参考点位置的影响较小。在图9中，concave三角帕累托前沿对参考点位置的影响也很小。经常使用的测试问题，如DTLZ1-4[8]和WFG4-9[11]，具有线形或凹形的三角形Pareto前沿。因此，EMO社区没有强调计算HV的参考点位置的重要性。然而，当帕累托前沿为倒三角形时(图8、10和12)，参考点的位置对HV最大化的解的最优分布有很大的影响。这意味着基于HV的比较结果强烈地依赖于参考点的位置。 在图7、8、9、10、11、12中，只有当图7中的Pareto front为线性三角形时，六种指标设置(即， HV与参考点四种设置，与IGD和IGD+)。在图8中，线性倒三角帕累托前缘，在r = 1.1, IGD, IGD+时，HV也得到了相似的结果。这些观察结果表明，当帕累托锋为线性时，三个指标的比较结果相似(当倒三角形帕累托前沿时，HV计算的参考点正确指定)。 如前一节所述，图7、8、9、10、11和12(e)中得到的IGD极小化解的分布是相似的(即，整个帕累托前边一致)。相反，得到的HV极大化和IGD+分布的解的分布与Pareto锋的形状有很大的关系。当r = 1.1(即所得到的HV最大值和IGD+最小值的分布是相似的。也就是说，图7、8、9、10、11、12中(b)和(f)的结果是相似的。 总结区别如下： 当帕累托前沿为三角形时(如DTLZ1-4, WFG4-9)，参考点的位置对获得的HV最大化的最优解分布几乎没有影响或影响很小。 当帕累托前沿为倒三角形时(如Inverted DTLZ、Minus-DTLZ、Minus- WFG)，参考点的位置对得到的HV最大化的近似最优解分布具有主导作用。 当Pareto前沿为线性三角形时(如DTLZ1)，三个指标的解分布相似。 平均分布的解决方案总是得到的IGD最小化独立于形状的帕累托前沿。 当参考点不太小也不太大时，得到了与Pareto前沿形状无关的HV最大化和IGD+最小化解的相似分布。 这些观察结果表明，IGD+可作为HV的代用品。(作为整体性能指标)当HV由于计算量大而不易使用时。这也表明，IGD+和HV并不总是评价解集均匀性的好指标。虽然IGD由于其不符合Pareto imcomplain的特性，并不总是一个好的整体性能指标，但在我们的计算实验中，IGD是一个很好的评估解决方案一致性的指标。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>indicators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-19]]></title>
    <url>%2F2019%2F08%2F21%2FWebScraping19%2F</url>
    <content type="text"><![CDATA[偶然间看到一本书《Python3网络爬虫开发实战》，作者还特别地写了一系列的博客，本篇论文，几乎参照他的博客写的，作为学习笔记。 urllibpython3的urllib库官方文档，分为四个模块： request：它是最基本的HTTP 请求模块，可以用来模拟发送请求。就像在浏览器里输入网址然后回车一样，只需要给库方法传入URL 以及额外的参数，就可以模拟实现这个过程了。 error：异常处理模块，如果出现请求错误， 我们可以捕获这些异常，然后进行重试或其他操作以保证程序不会意外终止。 parse：一个工具模块，提供了许多URL 处理方法，比如拆分、解析、合并等。 robotparser：主要是用来识别网站的robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬，它其实用得比较少。 requesturllib.request模块提供了最基本的构造HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程， 同时它还带有处理授权验证(authenticaton)、重定向(redirection) 、浏览器Cookies 以及其他内容。 urlopen123import urllib.requestresponse = urllib.request.urlopen("https://www.python.org")print(type(response)) 输出的为： 1&lt;class &apos;http.client.HTTPResponse&apos;&gt; 可以发现，它是一个HTTPResposne类型的对象。它主要包含read()、readinto()、getheader(name)、getheaders()、fileno()等方法，以及msg、version、status、reason、debuglevel、closed等属性。 实例： 1234567891011import urllib.request response = urllib.request.urlopen('https://www.python.org')print(response.status)print(response.getheaders())print(response.getheader('Server'))# 输出:200[('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'SAMEORIGIN'), ('X-Clacks-Overhead', 'GNU Terry Pratchett'), ('Content-Length', '47397'), ('Accept-Ranges', 'bytes'), ('Date', 'Mon, 01 Aug 2016 09:57:31 GMT'), ('Via', '1.1 varnish'), ('Age', '2473'), ('Connection', 'close'), ('X-Served-By', 'cache-lcy1125-LCY'), ('X-Cache', 'HIT'), ('X-Cache-Hits', '23'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]nginx 可见，前两个输出分别输出了响应的状态码和响应的头信息，最后一个输出通过调用getheader()方法并传递一个参数Server获取了响应头中的Server值，结果是nginx，意思是服务器是用Nginx搭建的。 利用最基本的urlopen()方法，可以完成最基本的简单网页的GET请求抓取。 如果想给链接传递一些参数，该怎么实现呢？首先看一下urlopen()函数的API： 1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=**False**, context=None) 可以发现，除了第一个参数可以传递URL之外，我们还可以传递其他内容，比如data（附加数据）、timeout（超时时间）等。 下面我们详细说明下这几个参数的用法。 data参数data参数是可选的。如果要添加该参数，并且如果它是字节流编码格式的内容，即bytes类型，则需要通过bytes()方法转化。另外，如果传递了这个参数，则它的请求方式就不再是GET方式，而是POST方式。 下面用实例来看一下： 123456import urllib.parseimport urllib.requestdata = bytes(urllib.parse.urlencode(&#123;'word': 'hello'&#125;), encoding='utf8')response = urllib.request.urlopen('http://httpbin.org/post', data=data)print(response.read()) 这里我们传递了一个参数word，值是hello。它需要被转码成bytes（字节流）类型。其中转字节流采用了bytes()方法，该方法的第一个参数需要是str（字符串）类型，需要用urllib.parse模块里的urlencode()方法来将参数字典转化为字符串；第二个参数指定编码格式，这里指定为utf8。 这里请求的站点是httpbin.org，它可以提供HTTP请求测试。本次我们请求的URL为http://httpbin.org/post，这个链接可以用来测试POST请求，它可以输出请求的一些信息，其中包含我们传递的data参数。 运行结果如下： 123456789101112131415161718&#123; &quot;args&quot;: &#123;&#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123; &quot;word&quot;: &quot;hello&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Content-Length&quot;: &quot;10&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;Python-urllib/3.5&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;123.124.23.253&quot;, &quot;url&quot;: &quot;http://httpbin.org/post&quot;&#125; 我们传递的参数出现在了form字段中，这表明是模拟了表单提交的方式，以POST方式传输数据。 timeout参数timeout参数用于设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间，还没有得到响应，就会抛出异常。如果不指定该参数，就会使用全局默认时间。它支持HTTP、HTTPS、FTP请求。 下面用实例来看一下： 1234import urllib.request response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)print(response.read()) 运行结果如下： 12345678---------------------------------------------------------------------------timeout Traceback (most recent call last)&lt;ipython-input-16-59340fde1030&gt; in &lt;module&gt;----&gt; 1 response = urllib.request.urlopen(&apos;http://httpbin.org/get&apos;, timeout=1)D:\Anaconda3\lib\urllib\request.py in urlopen(url, data, timeout, cafile, capath, cadefault, context)...timeout: timed out 这里我们设置超时时间是1秒。程序1秒过后，服务器依然没有响应，于是抛出了URLError异常。该异常属于urllib.error模块，错误原因是超时。 因此，可以通过设置这个超时时间来控制一个网页如果长时间未响应，就跳过它的抓取。这可以利用try except语句来实现，相关代码如下： 123456789import socketimport urllib.requestimport urllib.error try: response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)except urllib.error.URLError as e: if isinstance(e.reason, socket.timeout): print('TIME OUT') 这里我们请求了http://httpbin.org/get测试链接，设置超时时间是0.1秒，然后捕获了URLError异常，接着判断异常是socket.timeout类型（意思就是超时异常），从而得出它确实是因为超时而报错，打印输出了TIME OUT。 运行结果如下： 1TIME OUT 按照常理来说，0.1秒内基本不可能得到服务器响应，因此输出了TIME OUT的提示。 通过设置timeout这个参数来实现超时处理，有时还是很有用的。 其他初了data参数和timeout参数外，还有context参数，它必须是ssl.SSLContext类型，用来指定SSL设置。 此外，cafile和capath这两个参数分别指定CA证书和它的路径，这个在请求HTTPS链接时会有用。 cadefault参数现在已经弃用了，其默认值为False。 前面讲解了urlopen()方法的用法，通过这个最基本的方法，我们可以完成简单的请求和网页抓取。若需更加详细的信息，可以参见官方文档。 Request我们知道利用urlopen()方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求。如果请求中需要加入Headers等信息，就可以利用更强大的Request类来构建。 首先，我们用实例来感受一下Request的用法： 12345import urllib.request request = urllib.request.Request('https://python.org')response = urllib.request.urlopen(request)print(response.read().decode('utf-8')) 可以发现，我们依然是用urlopen()方法来发送这个请求，只不过这次该方法的参数不再是URL，而是一个Request类型的对象。通过构造这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可更加丰富和灵活地配置参数。 下面我们看一下Request可以通过怎样的参数来构造，它的构造方法如下： 1class urllib.request.Request(url, data=None, headers=&#123;&#125;, origin_req_host=None, unverifiable=False, method=None) 第一个参数url用于请求URL，这是必传参数，其他都是可选参数。 第二个参数data如果要传，必须传bytes（字节流）类型的。如果它是字典，可以先用urllib.parse模块里的urlencode()编码。 第三个参数headers是一个字典，它就是请求头，我们可以在构造请求时通过headers参数直接构造，也可以通过调用请求实例的add_header()方法添加。 添加请求头最常用的用法就是通过修改User-Agent来伪装浏览器，默认的User-Agent是Python-urllib，我们可以通过修改它来伪装浏览器。比如要伪装火狐浏览器，你可以把它设置为： 1Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11 第四个参数origin_req_host指的是请求方的host名称或者IP地址。 第五个参数unverifiable表示这个请求是否是无法验证的，默认是False，意思就是说用户没有足够权限来选择接收这个请求的结果。例如，我们请求一个HTML文档中的图片，但是我们没有自动抓取图像的权限，这时unverifiable的值就是True`。 第六个参数method是一个字符串，用来指示请求使用的方法，比如GET、POST和PUT等。 下面我们传入多个参数构建请求来看一下： 1234567891011121314from urllib import request, parse url = 'http://httpbin.org/post'headers = &#123; 'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)', 'Host': 'httpbin.org'&#125;dict = &#123; 'name': 'Germey'&#125;data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)print(response.read().decode('utf-8')) 这里我们通过4个参数构造了一个请求，其中url即请求URL，headers中指定了User-Agent和Host，参数data用urlencode()和bytes()方法转成字节流。另外，指定了请求方式为POST。 运行结果如下： 123456789101112131415161718&#123; &quot;args&quot;: &#123;&#125;, &quot;data&quot;: &quot;&quot;, &quot;files&quot;: &#123;&#125;, &quot;form&quot;: &#123; &quot;name&quot;: &quot;Germey&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept-Encoding&quot;: &quot;identity&quot;, &quot;Content-Length&quot;: &quot;11&quot;, &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)&quot; &#125;, &quot;json&quot;: null, &quot;origin&quot;: &quot;117.179.104.211, 117.179.104.211&quot;, &quot;url&quot;: &quot;https://httpbin.org/post&quot;&#125; 观察结果可以发现，我们成功设置了data、headers和method。 另外，headers也可以用add_header()方法来添加： 12req = request.Request(url=url, data=data, method='POST')req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)') 如此一来，我们就可以更加方便地构造请求，实现请求的发送啦。 高级用法在上面的过程中，我们虽然可以构造请求，但是对于一些更高级的操作（比如Cookies处理、代理设置等），我们该怎么办呢？ 接下来，就需要更强大的工具Handler登场了。简而言之，我们可以把它理解为各种处理器，有专门处理登录验证的，有处理Cookies的，有处理代理设置的。利用它们，我们几乎可以做到HTTP请求中所有的事情。 首先，介绍一下urllib.request模块里的BaseHandler类，它是所有其他Handler的父类，它提供了最基本的方法，例如default_open()、protocol_request()等。 接下来，就有各种Handler子类继承这个BaseHandler类，举例如下。 HTTPDefaultErrorHandler：用于处理HTTP响应错误，错误都会抛出HTTPError类型的异常。 HTTPRedirectHandler：用于处理重定向。 HTTPCookieProcessor：用于处理Cookies。 ProxyHandler：用于设置代理，默认代理为空。 HTTPPasswordMgr：用于管理密码，它维护了用户名和密码的表。 HTTPBasicAuthHandler：用于管理认证，如果一个链接打开时需要认证，那么可以用它来解决认证问题。 另外，还有其他的Handler类，这里就不一一列举了，详情可以参考官方文档。 关于怎么使用它们，现在先不用着急，后面会有实例演示。 另一个比较重要的类就是OpenerDirector，我们可以称为Opener。我们之前用过urlopen()这个方法，实际上它就是urllib为我们提供的一个Opener。 那么，为什么要引入Opener呢？因为需要实现更高级的功能。之前使用的Request和urlopen()相当于类库为你封装好了极其常用的请求方法，利用它们可以完成基本的请求，但是现在不一样了，我们需要实现更高级的功能，所以需要深入一层进行配置，使用更底层的实例来完成操作，所以这里就用到了Opener。 Opener可以使用open()方法，返回的类型和urlopen()如出一辙。那么，它和Handler有什么关系呢？简而言之，就是利用Handler来构建Opener。 下面用几个实例来看看它们的用法。 有些网站在打开时就会弹出提示框，直接提示你输入用户名和密码，验证成功后才能查看页面，如图3-2所示。 那么，如果要请求这样的页面，该怎么办呢？借助HTTPBasicAuthHandler就可以完成，相关代码如下： 123456789101112131415161718from urllib.request import HTTPPasswordMgrWithDefaultRealm, HTTPBasicAuthHandler, build_openerfrom urllib.error import URLError username = 'username'password = 'password'url = 'http://localhost:5000/' p = HTTPPasswordMgrWithDefaultRealm()p.add_password(None, url, username, password)auth_handler = HTTPBasicAuthHandler(p)opener = build_opener(auth_handler) try: result = opener.open(url) html = result.read().decode('utf-8') print(html)except URLError as e: print(e.reason) 这里首先实例化HTTPBasicAuthHandler对象，其参数是HTTPPasswordMgrWithDefaultRealm对象，它利用add_password()添加进去用户名和密码，这样就建立了一个处理验证的Handler。 接下来，利用这个Handler并使用build_opener()方法构建一个Opener，这个Opener在发送请求时就相当于已经验证成功了。 接下来，利用Opener的open()方法打开链接，就可以完成验证了。这里获取到的结果就是验证后的页面源码内容。 代理12345678910111213from urllib.error import URLErrorfrom urllib.request import ProxyHandler, build_opener proxy_handler = ProxyHandler(&#123; 'http': 'http://127.0.0.1:9743', 'https': 'https://127.0.0.1:9743'&#125;)opener = build_opener(proxy_handler)try: response = opener.open('https://www.baidu.com') print(response.read().decode('utf-8'))except URLError as e: print(e.reason) 这里我们在本地搭建了一个代理，它运行在9743端口上。 这里使用了ProxyHandler，其参数是一个字典，键名是协议类型（比如HTTP或者HTTPS等），键值是代理链接，可以添加多个代理。 然后，利用这个Handler及build_opener()方法构造一个Opener，之后发送请求即可。 CookiesCookies的处理就需要相关的Handler了。 我们先用实例来看看怎样将网站的Cookies获取下来，相关代码如下： 12345678import http.cookiejar, urllib.request cookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie: print(item.name+"="+item.value) 先，我们必须声明一个CookieJar对象。接下来，就需要利用HTTPCookieProcessor来构建一个Handler，最后利用build_opener()方法构建出Opener，执行open()函数即可。 运行结果如下: 1234567BAIDUID=32ED45BB4CAA948553F9E9C25E67DF57:FG=1BIDUPSID=32ED45BB4CAA948553F9E9C25E67DF57H_PS_PSSID=1420_21122_18560_29523_29519_29098_29568_29220_29461PSTM=1566277884delPer=0BDSVRTM=0BD_HOME=0 可以看到，这里输出了每条Cookie的名称和值。 不过既然能输出，那可不可以输出成文件格式呢？我们知道Cookies实际上也是以文本形式保存的。 答案当然是肯定的，这里通过下面的实例来看看： 123456filename = 'cookies.txt'cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True) 这时CookieJar就需要换成MozillaCookieJar，它在生成文件时会用到，是CookieJar的子类，可以用来处理Cookies和文件相关的事件，比如读取和保存Cookies，可以将Cookies保存成Mozilla型浏览器的Cookies格式。 运行之后，可以发现生成了一个cookies.txt文件，其内容如下： 1234567891011# Netscape HTTP Cookie File# http://curl.haxx.se/rfc/cookie_spec.html# This is a generated file! Do not edit..baidu.com TRUE / FALSE 3713761816 BAIDUID A1EB1A2981419DCB5E77E799ACECE43C:FG=1.baidu.com TRUE / FALSE 3713761816 BIDUPSID A1EB1A2981419DCB5E77E799ACECE43C.baidu.com TRUE / FALSE H_PS_PSSID 1465_21100_29073_29523_29520_29099_29568_29221_29458_29588.baidu.com TRUE / FALSE 3713761816 PSTM 1566278169.baidu.com TRUE / FALSE delPer 0www.baidu.com FALSE / FALSE BDSVRTM 0www.baidu.com FALSE / FALSE BD_HOME 0 另外，LWPCookieJar同样可以读取和保存Cookies，但是保存的格式和MozillaCookieJar不一样，它会保存成libwww-perl(LWP)格式的Cookies文件。 要保存成LWP格式的Cookies文件，可以在声明时就改为： 1cookie = http.cookiejar.LWPCookieJar(filename) 由此看来，生成的格式还是有比较大差异的。 那么，生成了Cookies文件后，怎样从文件中读取并利用呢？ 下面我们以LWPCookieJar格式为例来看一下： 123456cookie = http.cookiejar.LWPCookieJar()cookie.load('cookies.txt', ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8')) 可以看到，这里调用load()方法来读取本地的Cookies文件，获取到了Cookies的内容。不过前提是我们首先生成了LWPCookieJar格式的Cookies，并保存成文件，然后读取Cookies之后使用同样的方法构建Handler和Opener即可完成操作。 运行结果正常的话，会输出百度网页的源代码。 通过上面的方法，我们可以实现绝大多数请求功能的设置了。 这便是urllib库中request模块的基本用法，如果想实现更多的功能，可以参考官方文档。 error如果出现了异常，该怎么办呢？这时如果不处理这些异常，程序很可能因报错而终止运行，所以异常处理还是十分有必要的。 urllib的error模块定义了由request模块产生的异常。如果出现了问题，request模块便会抛出error模块中定义的异常。 URLErrorURLError类来自urllib库的error模块，它继承自OSError类，是error异常模块的基类，由request模块生的异常都可以通过捕获这个类来处理。 它具有一个属性reason，即返回错误的原因。 下面用一个实例来看一下： 12345from urllib import request, errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.URLError as e: print(e.reason) 我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了URLError这个异常，运行结果如下： 1Not Found 程序没有直接报错，而是输出了如上内容，这样通过如上操作，我们就可以避免程序异常终止，同时异常得到了有效处理。 HTTPError它是URLError的子类，专门用来处理HTTP请求错误，比如认证请求失败等。它有如下3个属性。 code：返回HTTP状态码，比如404表示网页不存在，500表示服务器内部错误等。 reason：同父类一样，用于返回错误的原因。 headers：返回请求头。 下面我们用几个实例来看看： 12345from urllib import request,errortry: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.HTTPError as e: print(e.reason, e.code, e.headers, sep='\n') 运行结果如下： 12345678910111213Not Found404Server: nginx/1.10.3 (Ubuntu)Date: Wed, 21 Aug 2019 06:42:29 GMTContent-Type: text/html; charset=UTF-8Transfer-Encoding: chunkedConnection: closeSet-Cookie: PHPSESSID=jjamgq0r0ts5rur6ab5fjtp140; path=/Pragma: no-cacheVary: CookieExpires: Wed, 11 Jan 1984 05:00:00 GMTCache-Control: no-cache, must-revalidate, max-age=0Link: &lt;https://cuiqingcai.com/wp-json/&gt;; rel=&quot;https://api.w.org/&quot; 依然是同样的网址，这里捕获了HTTPError异常，输出了reason、code和headers属性。 因为URLError是HTTPError的父类，所以可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码更好的写法如下： 12345678910from urllib import request, error try: response = request.urlopen('http://cuiqingcai.com/index.htm')except error.HTTPError as e: print(e.reason, e.code, e.headers, sep='\n')except error.URLError as e: print(e.reason)else: print('Request Successfully') 这样就可以做到先捕获HTTPError，获取它的错误状态码、原因、headers等信息。如果不是HTTPError异常，就会捕获URLError异常，输出错误原因。最后，用else来处理正常的逻辑。这是一个较好的异常处理写法。 有时候，reason属性返回的不一定是字符串，也可能是一个对象。再看下面的实例： 12345678910import socketimport urllib.requestimport urllib.error try: response = urllib.request.urlopen('https://www.baidu.com', timeout=0.01)except urllib.error.URLError as e: print(type(e.reason)) if isinstance(e.reason, socket.timeout): print('TIME OUT') 这里我们直接设置超时时间来强制抛出timeout异常。 运行结果如下： 12&lt;class &apos;socket.timeout&apos;&gt;TIME OUT 可以发现，reason属性的结果是socket.timeout类。所以，这里我们可以用isinstance()方法来判断它的类型，作出更详细的异常判断。 本节中，我们讲述了error模块的相关用法，通过合理地捕获异常可以做出更准确的异常判断，使程序更加稳健。 parse前面说过，urllib库里还提供了parse这个模块，它定义了处理URL的标准接口，例如实现URL各部分的抽取、合并以及链接转换。它支持如下协议的URL处理：file、ftp、gopher、hdl、http、https、imap、mailto、 mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、 sip、sips、snews、svn、svn+ssh、telnet和wais。本节中，我们介绍一下该模块中常用的方法来看一下它的便捷之处。 urlparse()该方法可以实现URL的识别和分段，这里先用一个实例来看一下： 1234from urllib.parse import urlparse result = urlparse('http://www.baidu.com/index.html;user?id=5#comment')print(type(result), result) 这里我们利用urlparse()方法进行了一个URL的解析。首先，输出了解析结果的类型，然后将结果也输出出来。 运行结果如下： 1&lt;class 'urllib.parse.ParseResult'&gt; ParseResult(scheme='http', netloc='www.baidu.com', path='/index.html', params='user', query='id=5', fragment='comment') 可以看到，返回结果是一个ParseResult类型的对象，它包含6部分，分别是scheme、netloc、path、params、query和fragment。 观察一下该实例的URL： 1http://www.baidu.com/index.html;user?id=5#comment 可以发现，urlparse()方法将其拆分成了6部分。大体观察可以发现，解析时有特定的分隔符。比如，://前面的就是scheme，代表协议；第一个/前面便是netloc，即域名；分号;前面是params，代表参数。 所以，可以得出一个标准的链接格式，具体如下： 1scheme://netloc/path;parameters?query#fragment 一个标准的URL都会符合这个规则，利用urlparse()方法可以将它拆分开来。 除了这种最基本的解析方式外，urlparse()方法还有其他配置吗？接下来，看一下它的API用法： 1urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True) 可以看到，它有3个参数。 urlstring：这是必填项，即待解析的URL。 scheme：它是默认的协议（比如http或https等）。假如这个链接没有带协议信息，会将这个作为默认的协议。我们用实例来看一下： 1234from urllib.parse import urlparse result = urlparse('www.baidu.com/index.html;user?id=5#comment', scheme='https')print(result) 运行结果如下： 1ParseResult(scheme=&apos;https&apos;, netloc=&apos;&apos;, path=&apos;www.baidu.com/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 可以发现，我们提供的URL没有包含最前面的scheme信息，但是通过指定默认的scheme参数，返回的结果是https。 假设我们带上了scheme： 1result = urlparse('http://www.baidu.com/index.html;user?id=5#comment', scheme='https') 则结果如下： 1ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 可见，scheme参数只有在URL中不包含scheme信息时才生效。如果URL中有scheme信息，就会返回解析出的scheme。 allow_fragments：即是否忽略fragment。如果它被设置为False，fragment部分就会被忽略，它会被解析为path、parameters或者query的一部分，而fragment部分为空。下面我们用实例来看一下： 1234from urllib.parse import urlparse result = urlparse('http://www.baidu.com/index.html;user?id=5#comment', allow_fragments=False)print(result) 运行结果如下： 1ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html&apos;, params=&apos;user&apos;, query=&apos;id=5#comment&apos;, fragment=&apos;&apos;) 假设URL中不包含params和query，我们再通过实例看一下： 1234from urllib.parse import urlparse result = urlparse('http://www.baidu.com/index.html#comment', allow_fragments=False)print(result) 运行结果如下： 1ParseResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html#comment&apos;, params=&apos;&apos;, query=&apos;&apos;, fragment=&apos;&apos;) 可以发现，当URL中不包含params和query时，fragment便会被解析为path的一部分。 返回结果ParseResult实际上是一个元组，我们可以用索引顺序来获取，也可以用属性名获取。示例如下： 1234from urllib.parse import urlparse result = urlparse('http://www.baidu.com/index.html#comment', allow_fragments=False)print(result.scheme, result[0], result.netloc, result[1], sep='\n') 这里我们分别用索引和属性名获取了scheme和netloc，其运行结果如下： 1234httphttpwww.baidu.comwww.baidu.com 可以发现，二者的结果是一致的，两种方法都可以成功获取。 urlsplit()这个方法和urlparse()方法非常相似，只不过它不再单独解析params这一部分，只返回5个结果。上面例子中的params会合并到path中。示例如下： 1234from urllib.parse import urlsplit result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')print(result) 运行结果如下： 1SplitResult(scheme=&apos;http&apos;, netloc=&apos;www.baidu.com&apos;, path=&apos;/index.html;user&apos;, query=&apos;id=5&apos;, fragment=&apos;comment&apos;) 可以发现，返回结果是SplitResult，它其实也是一个元组类型，既可以用属性获取值，也可以用索引来获取。示例如下： 1234from urllib.parse import urlsplit result = urlsplit('http://www.baidu.com/index.html;user?id=5#comment')print(result.scheme, result[0]) 运行结果如下： 1http http urlunsplit()与urlunparse()类似，它也是将链接各个部分组合成完整链接的方法，传入的参数也是一个可迭代对象，例如列表、元组等，唯一的区别是长度必须为5。示例如下： 1234from urllib.parse import urlunsplit data = ['http', 'www.baidu.com', 'index.html', 'a=6', 'comment']print(urlunsplit(data)) 运行结果如下： 1http://www.baidu.com/index.html?a=6#comment urljoin()有了urlunparse()和urlunsplit()方法，我们可以完成链接的合并，不过前提必须要有特定长度的对象，链接的每一部分都要清晰分开。 此外，生成链接还有另一个方法，那就是urljoin()方法。我们可以提供一个base_url（基础链接）作为第一个参数，将新的链接作为第二个参数，该方法会分析base_url的scheme、netloc和path这3个内容并对新链接缺失的部分进行补充，最后返回结果。 下面通过几个实例看一下： 12345678910from urllib.parse import urljoin print(urljoin('http://www.baidu.com', 'FAQ.html'))print(urljoin('http://www.baidu.com', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://cuiqingcai.com/FAQ.html?question=2'))print(urljoin('http://www.baidu.com?wd=abc', 'https://cuiqingcai.com/index.php'))print(urljoin('http://www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com#comment', '?category=2')) 运行结果如下： 12345678http://www.baidu.com/FAQ.htmlhttps://cuiqingcai.com/FAQ.htmlhttps://cuiqingcai.com/FAQ.htmlhttps://cuiqingcai.com/FAQ.html?question=2https://cuiqingcai.com/index.phphttp://www.baidu.com?category=2#commentwww.baidu.com?category=2#commentwww.baidu.com?category=2 可以发现，base_url提供了三项内容scheme、netloc和path。如果这3项在新的链接里不存在，就予以补充；如果新的链接存在，就使用新的链接的部分。而base_url中的params、query和fragment是不起作用的。 通过urljoin()方法，我们可以轻松实现链接的解析、拼合与生成。 urlencode()这里我们再介绍一个常用的方法——urlencode()，它在构造GET请求参数的时候非常有用，示例如下： 123456789from urllib.parse import urlencode params = &#123; 'name': 'germey', 'age': 22&#125;base_url = 'http://www.baidu.com?'url = base_url + urlencode(params)print(url) 这里首先声明了一个字典来将参数表示出来，然后调用urlencode()方法将其序列化为GET请求参数。 运行结果如下： 1http://www.baidu.com?name=germey&amp;age=22 可以看到，参数就成功地由字典类型转化为GET请求参数了。 这个方法非常常用。有时为了更加方便地构造参数，我们会事先用字典来表示。要转化为URL的参数时，只需要调用该方法即可。 parse_qs()有了序列化，必然就有反序列化。如果我们有一串GET请求参数，利用parse_qs()方法，就可以将它转回字典，示例如下： 1234from urllib.parse import parse_qs query = 'name=germey&amp;age=22'print(parse_qs(query)) 运行结果如下： 11&#123;&apos;name&apos;: [&apos;germey&apos;], &apos;age&apos;: [&apos;22&apos;]&#125; parse_qsl()另外，还有一个parse_qsl()方法，它用于将参数转化为元组组成的列表，示例如下： 1234from urllib.parse import parse_qsl query = 'name=germey&amp;age=22'print(parse_qsl(query)) 运行结果如下： 1[(&apos;name&apos;, &apos;germey&apos;), (&apos;age&apos;, &apos;22&apos;)] 可以看到，运行结果是一个列表，而列表中的每一个元素都是一个元组，元组的第一个内容是参数名，第二个内容是参数值。 quote()该方法可以将内容转化为URL编码的格式。URL中带有中文参数时，有时可能会导致乱码的问题，此时用这个方法可以将中文字符转化为URL编码，示例如下： 12345from urllib.parse import quote keyword = '壁纸'url = 'https://www.baidu.com/s?wd=' + quote(keyword)print(url) 这里我们声明了一个中文的搜索文字，然后用quote()方法对其进行URL编码，最后得到的结果如下： 1https://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8 可以看到，利用unquote()方法可以方便地实现解码。 本节中，我们介绍了parse模块的一些常用URL处理方法。有了这些方法，我们可以方便地实现URL的解析和构造，建议熟练掌握。 RobotsRobots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。 robots.txt当搜索爬虫访问一个站点时，它首先会检查这个站点根目录下是否存在robots.txt文件，如果存在，搜索爬虫会根据其中定义的爬取范围来爬取。如果没有找到这个文件，搜索爬虫便会访问所有可直接访问的页面。 下面我们看一个robots.txt的样例： 123User-agent: *Disallow: /Allow: /public/ 这实现了对所有搜索爬虫只允许爬取public目录的功能，将上述内容保存成robots.txt文件，放在网站的根目录下，和网站的入口文件（比如index.php、index.html和index.jsp等）放在一起。 上面的User-agent描述了搜索爬虫的名称，这里将其设置为*则代表该协议对任何爬取爬虫有效。比如，我们可以设置： 1User-agent: Baiduspider 这就代表我们设置的规则对百度爬虫是有效的。如果有多条User-agent记录，则就会有多个爬虫会受到爬取限制，但至少需要指定一条。 Disallow指定了不允许抓取的目录，比如上例子中设置为/则代表不允许抓取所有页面。 Allow一般和Disallow一起使用，一般不会单独使用，用来排除某些限制。现在我们设置为/public/，则表示所有页面不允许抓取，但可以抓取public目录。 下面我们再来看几个例子。禁止所有爬虫访问任何目录的代码如下： 12User-agent: * Disallow: / 允许所有爬虫访问任何目录的代码如下： 12User-agent: *Disallow: 另外，直接把robots.txt文件留空也是可以的。 禁止所有爬虫访问网站某些目录的代码如下： 123User-agent: *Disallow: /private/Disallow: /tmp/ 只允许某一个爬虫访问的代码如下： 1234User-agent: WebCrawlerDisallow:User-agent: *Disallow: / 这些是robots.txt的一些常见写法。 name of Spider大家可能会疑惑，爬虫名是哪儿来的？为什么就叫这个名？其实它是有固定名字的了，比如百度的就叫作BaiduSpider。表3-1列出了一些常见的搜索爬虫的名称及对应的网站。 爬虫名称 名称 网站 BaiduSpider 百度 www.baidu.com Googlebot 谷歌 www.google.com 360Spider 360搜索 www.so.com YodaoBot 有道 www.youdao.com ia_archiver Alexa www.alexa.cn Scooter altavista www.altavista.com robotparser了解Robots协议之后，我们就可以使用robotparser模块来解析robots.txt了。该模块提供了一个类RobotFileParser，它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页。 该类用起来非常简单，只需要在构造方法里传入robots.txt的链接即可。首先看一下它的声明： 1urllib.robotparser.RobotFileParser(url='') 当然，也可以在声明时不传入，默认为空，最后再使用set_url()方法设置一下也可。 下面列出了这个类常用的几个方法。 set_url()：用来设置robots.txt文件的链接。如果在创建RobotFileParser对象时传入了链接，那么就不需要再使用这个方法设置了。 read()：读取robots.txt文件并进行分析。注意，这个方法执行一个读取和分析操作，如果不调用这个方法，接下来的判断都会为False，所以一定记得调用这个方法。这个方法不会返回任何内容，但是执行了读取操作。 parse()：用来解析robots.txt文件，传入的参数是robots.txt某些行的内容，它会按照robots.txt的语法规则来分析这些内容。 can_fetch()：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。 mtime()：返回的是上次抓取和分析robots.txt的时间，这对于长时间分析和抓取的搜索爬虫是很有必要的，你可能需要定期检查来抓取最新的robots.txt。 modified()：它同样对长时间分析和抓取的搜索爬虫很有帮助，将当前时间设置为上次抓取和分析robots.txt的时间。 下面我们用实例来看一下： 1234567from urllib.robotparser import RobotFileParser rp = RobotFileParser()rp.set_url('http://www.jianshu.com/robots.txt')rp.read()print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections")) 这里以简书为例，首先创建RobotFileParser对象，然后通过set_url()方法设置了robots.txt的链接。当然，不用这个方法的话，可以在声明时直接用如下方法设置： 1rp = RobotFileParser('http://www.jianshu.com/robots.txt') 接着利用can_fetch()方法判断了网页是否可以被抓取。 运行结果如下： 12TrueFalse 这里同样可以使用parse()方法执行读取和分析，示例如下： 1234567from urllib.robotparser import RobotFileParserfrom urllib.request import urlopen rp = RobotFileParser()rp.parse(urlopen('http://www.jianshu.com/robots.txt').read().decode('utf-8').split('\n'))print(rp.can_fetch('*', 'http://www.jianshu.com/p/b67554025d7d'))print(rp.can_fetch('*', "http://www.jianshu.com/search?q=python&amp;page=1&amp;type=collections")) 运行结果一样： 12TrueFalse request上一节中，我们了解了urllib的基本用法，但是其中确实有不方便的地方，比如处理网页验证和Cookies时，需要写Opener和Handler来处理。为了更加方便地实现这些操作，就有了更为强大的库requests，有了它，Cookies、登录验证、代理设置等操作都不是事儿。 接下来，让我们领略一下它的强大之处吧。 实例引入urllib库中的urlopen()方法实际上是以GET方式请求网页，而requests中相应的方法就是get()方法，是不是感觉表达更明确一些？下面通过实例来看一下： 12345678import requests r = requests.get('https://www.baidu.com/')print(type(r))print(r.status_code)print(type(r.text))print(r.text)print(r.cookies) 运行结果如下： 12345678&lt;class &apos;requests.models.Response&apos;&gt;200&lt;class &apos;str&apos;&gt;&lt;!DOCTYPE html&gt;&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta content=always name=referrer&gt;&lt;link rel=stylesheet type=text/css href=https://ss1.bdstatic.com/5eN1bjq8AAUYm2zgoY3K/r/www/cache/bdorz/baidu.min.css&gt;&lt;title&gt;ç¾åº¦ä¸ä¸ï¼ä½ å°±ç¥é&lt;/title&gt;&lt;/head&gt; &lt;body link=#0000cc&gt; &lt;div id=wrapper&gt; &lt;div id=head&gt; &lt;div class=head_wrapper&gt; &lt;div class=s_form&gt; &lt;div class=s_form_wrapper&gt; &lt;div id=lg&gt; &lt;img hidefocus=true src=//www.baidu.com/img/bd_logo1.png width=270 height=129&gt; &lt;/div&gt; &lt;form id=form name=f action=//www.baidu.com/s class=fm&gt; &lt;input type=hidden name=bdorz_come value=1&gt; &lt;input type=hidden name=ie value=utf-8&gt; &lt;input type=hidden name=f value=8&gt; &lt;input type=hidden name=rsv_bp value=1&gt; &lt;input type=hidden name=rsv_idx value=1&gt; &lt;input type=hidden name=tn value=baidu&gt;&lt;span class=&quot;bg s_ipt_wr&quot;&gt;&lt;input id=kw name=wd class=s_ipt value maxlength=255 autocomplete=off autofocus=autofocus&gt;&lt;/span&gt;&lt;span class=&quot;bg s_btn_wr&quot;&gt;&lt;input type=submit id=su value=ç¾åº¦ä¸ä¸ class=&quot;bg s_btn&quot; autofocus&gt;&lt;/span&gt; &lt;/form&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=u1&gt; &lt;a href=http://news.baidu.com name=tj_trnews class=mnav&gt;æ°é»&lt;/a&gt; &lt;a href=https://www.hao123.com name=tj_trhao123 class=mnav&gt;hao123&lt;/a&gt; &lt;a href=http://map.baidu.com name=tj_trmap class=mnav&gt;å°å¾&lt;/a&gt; &lt;a href=http://v.baidu.com name=tj_trvideo class=mnav&gt;è§é¢&lt;/a&gt; &lt;a href=http://tieba.baidu.com name=tj_trtieba class=mnav&gt;è´´å§&lt;/a&gt; &lt;noscript&gt; &lt;a href=http://www.baidu.com/bdorz/login.gif?login&amp;amp;tpl=mn&amp;amp;u=http%3A%2F%2Fwww.baidu.com%2f%3fbdorz_come%3d1 name=tj_login class=lb&gt;ç»å½&lt;/a&gt; &lt;/noscript&gt; &lt;script&gt;document.write(&apos;&lt;a href=&quot;http://www.baidu.com/bdorz/login.gif?login&amp;tpl=mn&amp;u=&apos;+ encodeURIComponent(window.location.href+ (window.location.search === &quot;&quot; ? &quot;?&quot; : &quot;&amp;&quot;)+ &quot;bdorz_come=1&quot;)+ &apos;&quot; name=&quot;tj_login&quot; class=&quot;lb&quot;&gt;ç»å½&lt;/a&gt;&apos;); &lt;/script&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style=&quot;display: block;&quot;&gt;æ´å¤äº§å&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a href=http://home.baidu.com&gt;å³äºç¾åº¦&lt;/a&gt; &lt;a href=http://ir.baidu.com&gt;About Baidu&lt;/a&gt; &lt;/p&gt; &lt;p id=cp&gt;&amp;copy;2017&amp;nbsp;Baidu&amp;nbsp;&lt;a href=http://www.baidu.com/duty/&gt;ä½¿ç¨ç¾åº¦åå¿è¯»&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;æè§åé¦&lt;/a&gt;&amp;nbsp;äº¬ICPè¯030173å·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;&lt;RequestsCookieJar[&lt;Cookie BDORZ=27315 for .baidu.com/&gt;]&gt; 这里我们调用get()方法实现与urlopen()相同的操作，得到一个Response对象，然后分别输出了Response的类型、状态码、响应体的类型、内容以及Cookies。 通过运行结果可以发现，它的返回类型是requests.models.Response，响应体的类型是字符串str，Cookies的类型是RequestsCookieJar。 使用get()方法成功实现一个GET请求，这倒不算什么，更方便之处在于其他的请求类型依然可以用一句话来完成，示例如下： 12345r = requests.post('http://httpbin.org/post')r = requests.put('http://httpbin.org/put')r = requests.delete('http://httpbin.org/delete')r = requests.head('http://httpbin.org/get')r = requests.options('http://httpbin.org/get') 这里分别用post()、put()、delete()等方法实现了POST、PUT、DELETE等请求。是不是比urllib简单太多了？ 其实这只是冰山一角，更多的还在后面。 GET 请求基本实例首先，构建一个最简单的GET请求，请求的链接为http://httpbin.org/get，该网站会判断如果客户端发起的是GET请求的话，它返回相应的请求信息： 1234import requests r = requests.get('http://httpbin.org/get')print(r.text) 运行结果如下： 1234567891011&#123; &quot;args&quot;: &#123;&#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot; &#125;, &quot;origin&quot;: &quot;117.179.245.150, 117.179.245.150&quot;, &quot;url&quot;: &quot;https://httpbin.org/get&quot;&#125; 可以发现，我们成功发起了GET请求，返回结果中包含请求头、URL、IP等信息。 那么，对于GET请求，如果要附加额外的信息，一般怎样添加呢？比如现在想添加两个参数，其中name是germey，age是22。要构造这个请求链接，是不是要直接写成： 1r = requests.get('http://httpbin.org/get?name=germey&amp;age=22') 这样也可以，但是是不是有点不人性化呢？一般情况下，这种信息数据会用字典来存储。那么，怎样来构造这个链接呢？ 这同样很简单，利用params这个参数就好了，示例如下： 12345678import requests data = &#123; 'name': 'germey', 'age': 22&#125;r = requests.get("http://httpbin.org/get", params=data)print(r.text) 运行结果如下： 1234567891011121314&#123; &quot;args&quot;: &#123; &quot;age&quot;: &quot;22&quot;, &quot;name&quot;: &quot;germey&quot; &#125;, &quot;headers&quot;: &#123; &quot;Accept&quot;: &quot;*/*&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;Host&quot;: &quot;httpbin.org&quot;, &quot;User-Agent&quot;: &quot;python-requests/2.21.0&quot; &#125;, &quot;origin&quot;: &quot;117.179.245.150, 117.179.245.150&quot;, &quot;url&quot;: &quot;https://httpbin.org/get?name=germey&amp;age=22&quot;&#125; 通过运行结果可以判断，请求的链接自动被构造成了：http://httpbin.org/get?age=22&amp;name=germey。 另外，网页的返回类型实际上是str类型，但是它很特殊，是JSON格式的。所以，如果想直接解析返回结果，得到一个字典格式的话，可以直接调用json()方法。示例如下： 123456import requests r = requests.get("http://httpbin.org/get")print(type(r.text))print(r.json())print(type(r.json())) 运行结果如下： 123&lt;class &apos;str&apos;&gt;&#123;&apos;args&apos;: &#123;&#125;, &apos;headers&apos;: &#123;&apos;Accept&apos;: &apos;*/*&apos;, &apos;Accept-Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Host&apos;: &apos;httpbin.org&apos;, &apos;User-Agent&apos;: &apos;python-requests/2.21.0&apos;&#125;, &apos;origin&apos;: &apos;117.179.245.150, 117.179.245.150&apos;, &apos;url&apos;: &apos;https://httpbin.org/get&apos;&#125;&lt;class &apos;dict&apos;&gt; 可以发现，调用json()方法，就可以将返回结果是JSON格式的字符串转化为字典。 但需要注意的书，如果返回结果不是JSON格式，便会出现解析错误，抛出json.decoder.JSONDecodeError异常。 抓取网页上面的请求链接返回的是JSON形式的字符串，那么如果请求普通的网页，则肯定能获得相应的内容了。下面以“知乎”$\rightarrow$“发现”页面为例来看一下： 12345678910import requestsimport re headers = &#123; 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36'&#125;r = requests.get("https://www.zhihu.com/explore", headers=headers)pattern = re.compile('explore-feed.*?question_link.*?&gt;(.*?)&lt;/a&gt;', re.S)titles = re.findall(pattern, r.text)print(titles) 这里我们加入了headers信息，其中包含了User-Agent字段信息，也就是浏览器标识信息。如果不加这个，知乎会禁止抓取。 接下来我们用到了最基础的正则表达式来匹配出所有的问题内容。关于正则表达式的相关内容，我们会在3.3节中详细介绍，这里作为实例来配合讲解。 运行结果如下： 12 抓取二进制数据在上面的例子中，我们抓取的是知乎的一个页面，实际上它返回的是一个HTML文档。如果想抓去图片、音频、视频等文件，应该怎么办呢？ 图片、音频、视频这些文件本质上都是由二进制码组成的，由于有特定的保存格式和对应的解析方式，我们才可以看到这些形形色色的多媒体。所以，想要抓取它们，就要拿到它们的二进制码。 下面以GitHub的站点图标为例来看一下： 12345import requests r = requests.get("https://github.com/favicon.ico")print(r.text)print(r.content) 这里抓取的内容是站点图标，也就是在浏览器每一个标签上显示的小图标， 这里打印了Response对象的两个属性，一个是text，另一个是content。 运行结果如图3-4所示，其中前两行是r.text的结果，最后一行是r.content的结果。 可以注意到，前者出现了乱码，后者结果前带有一个b，这代表是bytes类型的数据。由于图片是二进制数据，所以前者在打印时转化为str类型，也就是图片直接转化为字符串，这理所当然会出现乱码。 接着，我们将刚才提取到的图片保存下来： 12345import requests r = requests.get("https://github.com/favicon.ico")with open('favicon.ico', 'wb') as f: f.write(r.content) 这里用了open()方法，它的第一个参数是文件名称，第二个参数代表以二进制写的形式打开，可以向文件里写入二进制数据。 运行结束之后，可以发现在文件夹中出现了名为favicon.ico的图标，如图3-5所示。 同样地，音频和视频文件也可以用这种方法获取。 添加headers与urllib.request一样，我们也可以通过headers参数来传递头信息。 比如，在上面“知乎”的例子中，如果不传递headers，就不能正常请求： 1234import requests r = requests.get("https://www.zhihu.com/explore")print(r.text) 运行结果如下： 1234567&lt;html&gt;&lt;head&gt;&lt;title&gt;400 Bad Request&lt;/title&gt;&lt;/head&gt;&lt;body bgcolor="white"&gt;&lt;center&gt;&lt;h1&gt;400 Bad Request&lt;/h1&gt;&lt;/center&gt;&lt;hr&gt;&lt;center&gt;openresty&lt;/center&gt;&lt;/body&gt;&lt;/html&gt; 但如果加上headers并加上User-Agent信息，那就没问题了： 12&lt;!doctype html&gt;&lt;html lang="zh" data-hairline="true" data-theme="light"&gt;&lt;head&gt;&lt;meta name="description" property="og:description" content="有问题，上知乎。知乎，可信赖的问答社区，以让每个人高效获得可信赖的解答为使命。知乎凭借认真、专业和友善的社区氛围，结构化、易获得的优质内容，基于问答的内容生产方式和独特的社区机制，吸引、聚集了各行各业中大量的亲历者、内行人、领域专家、领域爱好者，将高质量的内容透过人的节点来成规模地生产和分享。用户通过问答等交流方式建立信任和连接，打造和提升个人影响力，并发现、获得新机会。"src="https://static.zhihu.com/heifetz/main.app.7c8634e8d9de8fd5d961.js"&gt;&lt;/script&gt;&lt;script src="https://static.zhihu.com/heifetz/main.explore-routes.d628322decb4a68a77e7.js"&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 当然，我们可以在headers这个参数中任意添加其他的字段信息。 POST请求前面我们了解了最基本的GET请求，另外一种比较常见的请求方式是POST。使用requests实现POST请求同样非常简单，示例如下： 12345import requests data = &#123;'name': 'germey', 'age': '22'&#125;r = requests.post("http://httpbin.org/post", data=data)print(r.text) 这里还是请求http://httpbin.org/post，该网站可以判断如果请求是POST方式，就把相关请求信息返回。 运行结果如下： 1234567891011121314151617181920&#123; "args": &#123;&#125;, "data": "", "files": &#123;&#125;, "form": &#123; "age": "22", "name": "germey" &#125;, "headers": &#123; "Accept": "*/*", "Accept-Encoding": "gzip, deflate", "Content-Length": "18", "Content-Type": "application/x-www-form-urlencoded", "Host": "httpbin.org", "User-Agent": "python-requests/2.21.0" &#125;, "json": null, "origin": "117.179.245.150, 117.179.245.150", "url": "https://httpbin.org/post"&#125; 可以发现，我们成功获得了返回结果，其中form部分就是提交的数据，这就证明POST请求成功发送了。 响应发送请求后，得到的自然就是响应。在上面的实例中，我们使用text和content获取了响应的内容。此外，还有很多属性和方法可以用来获取其他信息，比如状态码、响应头、Cookies等。示例如下： 12345678import requests r = requests.get('http://www.jianshu.com')print(type(r.status_code), r.status_code)print(type(r.headers), r.headers)print(type(r.cookies), r.cookies)print(type(r.url), r.url)print(type(r.history), r.history) 这里分别打印输出status_code属性得到状态码，输出headers属性得到响应头，输出cookies属性得到Cookies，输出url属性得到URL，输出history属性得到请求历史。 运行结果如下： 12345&lt;class &apos;int&apos;&gt; 403&lt;class &apos;requests.structures.CaseInsensitiveDict&apos;&gt; &#123;&apos;Server&apos;: &apos;Tengine&apos;, &apos;Content-Type&apos;: &apos;text/html&apos;, &apos;Transfer-Encoding&apos;: &apos;chunked&apos;, &apos;Connection&apos;: &apos;keep-alive&apos;, &apos;Date&apos;: &apos;Wed, 21 Aug 2019 10:57:51 GMT&apos;, &apos;Vary&apos;: &apos;Accept-Encoding&apos;, &apos;Strict-Transport-Security&apos;: &apos;max-age=31536000; includeSubDomains; preload&apos;, &apos;Content-Encoding&apos;: &apos;gzip&apos;, &apos;x-alicdn-da-ups-status&apos;: &apos;endOs,0,403&apos;, &apos;Via&apos;: &apos;cache25.l2nu16-1[3,0], cache4.cn1252[27,0]&apos;, &apos;Timing-Allow-Origin&apos;: &apos;*&apos;, &apos;EagleId&apos;: &apos;6f28b09815663850714812872e&apos;&#125;&lt;class &apos;requests.cookies.RequestsCookieJar&apos;&gt; &lt;RequestsCookieJar[]&gt;&lt;class &apos;str&apos;&gt; https://www.jianshu.com/&lt;class &apos;list&apos;&gt; [&lt;Response [301]&gt;] 因为session_id过长，在此简写。可以看到，headers和cookies这两个属性得到的结果分别是CaseInsensitiveDict和RequestsCookieJar类型。 状态码常用来判断请求是否成功，而requests还提供了一个内置的状态码查询对象requests.codes，示例如下 1234import requests r = requests.get('http://www.jianshu.com')exit() if not r.status_code == requests.codes.ok else print('Request Successfully') 这里通过比较返回码和内置的成功的返回码，来保证请求得到了正常响应，输出成功请求的消息，否则程序终止，这里我们用requests.codes.ok得到的是成功的状态码200。 那么，肯定不能只有ok这个条件码。下面列出了返回码和相应的查询条件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778# 信息性状态码100: (&apos;continue&apos;,),101: (&apos;switching_protocols&apos;,),102: (&apos;processing&apos;,),103: (&apos;checkpoint&apos;,),122: (&apos;uri_too_long&apos;, &apos;request_uri_too_long&apos;), # 成功状态码200: (&apos;ok&apos;, &apos;okay&apos;, &apos;all_ok&apos;, &apos;all_okay&apos;, &apos;all_good&apos;, &apos;\\o/&apos;, &apos;✓&apos;),201: (&apos;created&apos;,),202: (&apos;accepted&apos;,),203: (&apos;non_authoritative_info&apos;, &apos;non_authoritative_information&apos;),204: (&apos;no_content&apos;,),205: (&apos;reset_content&apos;, &apos;reset&apos;),206: (&apos;partial_content&apos;, &apos;partial&apos;),207: (&apos;multi_status&apos;, &apos;multiple_status&apos;, &apos;multi_stati&apos;, &apos;multiple_stati&apos;),208: (&apos;already_reported&apos;,),226: (&apos;im_used&apos;,), # 重定向状态码300: (&apos;multiple_choices&apos;,),301: (&apos;moved_permanently&apos;, &apos;moved&apos;, &apos;\\o-&apos;),302: (&apos;found&apos;,),303: (&apos;see_other&apos;, &apos;other&apos;),304: (&apos;not_modified&apos;,),305: (&apos;use_proxy&apos;,),306: (&apos;switch_proxy&apos;,),307: (&apos;temporary_redirect&apos;, &apos;temporary_moved&apos;, &apos;temporary&apos;),308: (&apos;permanent_redirect&apos;, &apos;resume_incomplete&apos;, &apos;resume&apos;,), # These 2 to be removed in 3.0 # 客户端错误状态码400: (&apos;bad_request&apos;, &apos;bad&apos;),401: (&apos;unauthorized&apos;,),402: (&apos;payment_required&apos;, &apos;payment&apos;),403: (&apos;forbidden&apos;,),404: (&apos;not_found&apos;, &apos;-o-&apos;),405: (&apos;method_not_allowed&apos;, &apos;not_allowed&apos;),406: (&apos;not_acceptable&apos;,),407: (&apos;proxy_authentication_required&apos;, &apos;proxy_auth&apos;, &apos;proxy_authentication&apos;),408: (&apos;request_timeout&apos;, &apos;timeout&apos;),409: (&apos;conflict&apos;,),410: (&apos;gone&apos;,),411: (&apos;length_required&apos;,),412: (&apos;precondition_failed&apos;, &apos;precondition&apos;),413: (&apos;request_entity_too_large&apos;,),414: (&apos;request_uri_too_large&apos;,),415: (&apos;unsupported_media_type&apos;, &apos;unsupported_media&apos;, &apos;media_type&apos;),416: (&apos;requested_range_not_satisfiable&apos;, &apos;requested_range&apos;, &apos;range_not_satisfiable&apos;),417: (&apos;expectation_failed&apos;,),418: (&apos;im_a_teapot&apos;, &apos;teapot&apos;, &apos;i_am_a_teapot&apos;),421: (&apos;misdirected_request&apos;,),422: (&apos;unprocessable_entity&apos;, &apos;unprocessable&apos;),423: (&apos;locked&apos;,),424: (&apos;failed_dependency&apos;, &apos;dependency&apos;),425: (&apos;unordered_collection&apos;, &apos;unordered&apos;),426: (&apos;upgrade_required&apos;, &apos;upgrade&apos;),428: (&apos;precondition_required&apos;, &apos;precondition&apos;),429: (&apos;too_many_requests&apos;, &apos;too_many&apos;),431: (&apos;header_fields_too_large&apos;, &apos;fields_too_large&apos;),444: (&apos;no_response&apos;, &apos;none&apos;),449: (&apos;retry_with&apos;, &apos;retry&apos;),450: (&apos;blocked_by_windows_parental_controls&apos;, &apos;parental_controls&apos;),451: (&apos;unavailable_for_legal_reasons&apos;, &apos;legal_reasons&apos;),499: (&apos;client_closed_request&apos;,), # 服务端错误状态码500: (&apos;internal_server_error&apos;, &apos;server_error&apos;, &apos;/o\\&apos;, &apos;✗&apos;),501: (&apos;not_implemented&apos;,),502: (&apos;bad_gateway&apos;,),503: (&apos;service_unavailable&apos;, &apos;unavailable&apos;),504: (&apos;gateway_timeout&apos;,),505: (&apos;http_version_not_supported&apos;, &apos;http_version&apos;),506: (&apos;variant_also_negotiates&apos;,),507: (&apos;insufficient_storage&apos;,),509: (&apos;bandwidth_limit_exceeded&apos;, &apos;bandwidth&apos;),510: (&apos;not_extended&apos;,),511: (&apos;network_authentication_required&apos;, &apos;network_auth&apos;, &apos;network_authentication&apos;) 比如，如果想判断结果是不是404状态，可以用requests.codes.not_found来比对。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-18]]></title>
    <url>%2F2019%2F08%2F17%2FWebScraping18%2F</url>
    <content type="text"><![CDATA[本章的目标是为您提供一个框架，使您能够理解和讨论web抓取法律的各个方面，例如知识产权、未经授权的计算机访问和服务器使用，但不应该替代实际的法律建议。 这本书基本上已经到尾声了，学了很多的函数啊，术语呀，但是还没有具体的实战操作，因此，我将继续找一些实战例子去操作。 2010年，软件工程师皮特·沃登(Pete Warden)创建了一个网络爬虫程序，从Facebook上收集数据。他收集了大约2亿Facebook用户的姓名、位置信息、朋友和兴趣爱好的数据。当然，Facebook注意到了这一点，并向他发送了警告信，他遵守了。当被问及为什么要遵守《停止和终止》时，他说:“大数据?便宜。律师?没那么便宜。在本章中，您将了解与web抓取相关的美国法律(以及一些国际法律)，并学习如何分析给定web抓取情况的合法性和伦理。在阅读本文之前，请先考虑一个显而易见的事实:我是一名软件工程师，而不是律师。不要把你在这里或书中的任何其他章节读到的任何东西解释为专业的法律建议，也不要据此行事。虽然我相信我能够很好地讨论web抓取的合法性和伦理，但是在进行任何法律上含糊不清的web抓取项目之前，您应该咨询律师(而不是软件工程师)。 Trademarks, Copyrights, Patents, Oh My!知识产权入门!知识产权有三种基本类型:商标(由TM或R符号表示)、版权(无所不在的C)和专利(有时用文字说明发明受专利保护或专利号，但通常什么也没有)。 专利只用于宣布发明的所有权。您不能为图像、文本或任何信息本身申请专利。尽管有些专利，如软件专利，没有我们所认为的发明那么有形，但请记住，获得专利的是东西(或技术)，而不是专利中包含的信息。除非您使用的是抓取蓝图，或者某人的web抓取方法专利，否则您不太可能因抓取web而无意中侵犯专利。 商标也不太可能成为一个问题，但仍然是必须考虑的问题。美国专利商标局表示: A trademark is a word, phrase, symbol, and/or design that identifies and distinguishes the source of the goods of one party from those of others. A service mark is a word, phrase, symbol, and/or design that identifies and distinguishes the source of a service rather than goods. The term trademark is often used to refer to both trademarks and service marks. 除了我们想到商标时想到的传统的单词/符号商标，其他描述性的属性也可以被注册商标。这包括，例如，一个容器的形状(想想可口可乐瓶)，甚至一种颜色(最引人注目的是欧文斯康宁的粉红豹玻璃纤维绝缘材料的粉红色)。与专利不同，商标的所有权在很大程度上取决于使用它的环境。例如，如果我希望发布一篇带有可口可乐标志图片的博客文章，我可以这样做(只要我没有暗示我的博客文章是由可口可乐赞助或发布的)。如果我想生产一种新的软饮料，在包装上显示同样的可口可乐标志，这显然是商标侵权。同样的，虽然我可以用粉红豹粉包装我的新软饮料，但我不能用同样的颜色来制作家庭绝缘产品。 Copyright Law商标和专利都有一个共同点，那就是它们必须经过正式注册才能被认可。与人们普遍认为的相反，在有版权的材料上并非如此。是什么使图像、文本、音乐等具有版权?它不是页面底部的All Rights Reserved警告，也不是关于已发布和未发布内容的任何特别之处。你创作的每一件作品一旦问世，就会自动受到版权法的约束。 伯尔尼保护文学和艺术作品公约，以伯尔尼命名瑞士伯尔尼是版权的国际标准，1886年首次被采用。该公约说，从本质上说，所有成员国必须承认其他成员国公民的作品的版权保护，就像他们是自己国家的公民一样。实际上，这意味着，作为一名美国公民，你可能在美国因侵犯他人(比如法国人)所写材料的版权而被追究责任(反之亦然)。 显然，版权是web爬虫关注的问题。如果我从别人的博客上抓取内容并发布到我自己的博客上，我很有可能会惹上官司。幸运的是，我有几个层次的保护，这可能使我的博客抓取项目的防御能力，这取决于它的功能。首先，版权保护只适用于创造性作品。它不包括统计数字或事实。幸运的是，web scraper所追求的大部分内容都是统计数据和事实。虽然web scraper从web上收集诗歌，并在您自己的网站上显示这些诗歌可能违反了版权法，但是web scraper收集关于诗歌发布频率的信息并没有违反版权法。诗歌的原始形式是一种创造性的作品。一个网站每月平均发表的诗歌字数是真实的数据，而不是创造性的作品。如果所发布的内容是价格、公司高管姓名或其他一些事实信息，那么逐字发布的内容(与原始数据聚合/计算的内容相反)可能并不违反版权法。 即使是有版权的内容，也可以在合理的范围内，在数字千禧年的背景下直接使用版权法案。DMCA概述了一些自动处理受版权保护材料的规则。DMCA很长，从电子书到电话，有很多具体的规则。然而，有两点可能与web抓取特别相关: 在安全港保护下，如果你从一个你认为只包含没有版权的材料的来源中抓取材料，但是用户已经提交了版权材料，只要你在收到通知时删除了受版权保护的材料，你就会受到保护。 您不能为了收集内容而规避安全措施(例如密码保护)。 此外，DMCA还承认，《美国法典》下的合理使用是适用的，如果版权材料的使用属于合理使用范围，则不得根据《安全港保护条例》发布撤下通知。 显然，版权是web爬虫关注的问题。如果我从别人的博客上抓取内容并发布到我自己的博客上，我很有可能会惹上官司。幸运的是，我有几个层次的保护，这可能使我的博客抓取项目的防御能力，这取决于它的功能。首先，版权保护只适用于创造性作品。它不包括统计数字或事实。幸运的是，web scraper所追求的大部分内容都是统计数据和事实。虽然web scraper从web上收集诗歌，并在您自己的网站上显示这些诗歌可能违反了版权法，但是web scraper收集关于诗歌发布频率的信息并没有违反版权法。诗歌的原始形式是一种创造性的作品。一个网站每月平均发表的诗歌字数是真实的数据，而不是创造性的作品。如果所发布的内容是价格、公司高管姓名或其他一些事实信息，那么逐字发布的内容(与原始数据聚合/计算的内容相反)可能并不违反版权法。或者提供简短的数据示例来说明您的观点，这也可以，但是您可能需要检查美国代码中的公平使用条款来确保这一点。 Trespass to Chattels动产侵权从根本上不同于我们所认为的侵权法，因为它并不适用于房地产或土地，而是适用于动产(如服务器)。当您对属性的访问受到某种方式的干扰而不允许您访问或使用它时，它将适用。在这个云计算时代，人们很容易不把web服务器看作是真实的、有形的资源。然而，服务器不仅由昂贵的组件组成，而且还需要存储、监视、冷却和提供大量电力。据估计，全球10%的用电量是由电脑消耗的。(如果你对自己的电子设备的调查不能让你信服，考虑一下谷歌的大型服务器场，所有这些服务器场都需要连接到大型发电站。)虽然服务器是昂贵的资源，但从法律的角度来看，它们很有趣，因为网站管理员通常希望人们使用他们的资源(即，浏览他们的网站);他们只是不希望他们消耗太多的资源。通过浏览器查看网站是可以的;针对它启动全面的DDOS显然不是。 爬虫侵犯动产，须符合三个准则: 缺乏同意 因为web服务器对所有人都是开放的，所以它们通常也“同意”web爬虫。然而，许多网站的服务条款协议明确禁止使用爬虫。此外，任何交付给您的停止通知都明显地撤销了此同意。 实际伤害 服务器是昂贵的。除了服务器成本，如果你的抓取器让一个网站宕机，或者限制它为其他用户服务的能力，这可能会增加你造成的“伤害”。 意向性 如果您正在编写代码，您就知道它的作用! 你必须符合所有这三个标准的动产非法侵入申请。然而，如果你违反了服务条款协议，但没有造成实际伤害，不要认为你可以免于法律诉讼。您很可能违反了版权法、DMCA、计算机欺诈和滥用法案(稍后将详细介绍)，或者适用于web scraper的其他无数法律之一。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-17]]></title>
    <url>%2F2019%2F08%2F17%2FWebScraping17%2F</url>
    <content type="text"><![CDATA[在上一章中，您了解了如何跨多个线程和进程运行web爬虫，在这些线程和进程之间的通信有些受限，或者必须仔细计划。本章将这一概念引入其逻辑结论，即爬行器不仅在单独的进程中运行，而且在完全独立的机器上运行。 这一章是这本书的最后一章，这在某种程度上是恰当的。到目前为止，您一直在您的家庭计算机范围内从命令行运行所有Python应用程序。当然，您可能安装MySQL是为了复制真实服务器的环境。但这是不一样的。俗话说:如果你爱一样东西，就给它自由。本章介绍了在不同机器上运行脚本的几种方法，甚至是在您自己的机器上运行不同的IP地址。尽管你可能想把这一步是你现在不需要,你可能会惊讶于自己是多么容易开始使用你现有的工具(如个人网站上支付托管账户),和你的生活变得多么容易得多当你停止尝试的时候从你的笔记本电脑运行Python爬虫。 Why Use Remote Servers虽然在启动一个面向广大用户的web应用程序时，使用远程服务器似乎是一个明显的步骤，但是我们为自己的目的构建的工具通常都是在本地运行的。决定使用远程平台的人通常基于两个主要动机:需要更强大的功能和灵活性，以及需要使用另一个IP地址。 Avoiding IP Address Blocking在构建web scraper时，经验法则是:几乎所有东西都可以伪造。您可以从您不拥有的地址发送电子邮件，可以从命令行自动化鼠标移动数据，甚至可以通过Internet Explorer 5.0发送web管理员的网站流量来吓到他们。 唯一不能伪造的是你的IP地址。任何人都可以寄给你回信地址:总统，1600宾夕法尼亚大道西北，华盛顿特区20500。然而，如果这封信的邮戳来自美国阿尔伯克基，你可以相当肯定你没有与美国的总统通信。 阻止抓取者访问网站的大部分努力都集中在检测人类和机器人之间的差异上。到目前为止，封锁IP地址有点像一个农民放弃喷洒农药，而只是焚烧农田。这是丢弃从麻烦的IP地址发送的数据包的最后一招，但很有效。然而，这种解决方案也存在一些问题: IP地址访问列表维护起来很麻烦。尽管大型网站通常都有自己的程序来自动化这些列表的一些常规管理(机器人阻止机器人!) 每个地址都增加了接收数据包的少量处理时间，因为服务器必须根据列表检查接收到的数据包，以决定是否批准它们。许多地址乘以许多包可以很快地加起来。为了节省处理时间和复杂性，管理员经常将这些IP地址分组成块，并制定规则，例如，如果有一些紧密聚集的违规者，那么这个范围内的所有256个地址都将被阻塞。这就引出了第三点。 IP地址阻塞也会导致阻塞好人。例如，我在奥林工程学院(Olin College of Engineering)读本科时，一名学生编写了一些软件，试图在http://digg.com上为流行内容操纵投票(这是在Reddit流行起来之前)。一个被屏蔽的IP地址导致整个宿舍无法访问该网站。这个学生只是把他的软件转移到另一个服务器上;与此同时，Digg失去了许多主要目标用户的页面访问。 尽管有其缺点，IP地址阻塞仍然是服务器管理员阻止可疑的web抓取器访问服务器的一种非常常见的方法。如果一个IP地址被阻塞，唯一真正的解决方案是从另一个IP地址刮取。这可以通过将scraper移动到新服务器或使用Tor之类的服务通过不同的服务器路由流量来实现。 Portability and Extensibility有些任务对于家庭电脑和互联网连接来说太大了。虽然您不希望在任何单个网站上增加大量负载，但是您可能需要跨多个站点收集数据，并且需要比当前设置所能提供的更多的带宽和存储空间。此外，通过卸载计算密集型处理，您可以为更重要的任务(魔兽世界，有人吗?)你不需要担心电源和网络连接的维护(在星巴克启动你的应用程序，打包你的笔记本电脑，然后离开，知道一切都还在安全运行)，你可以在任何有网络连接的地方访问你收集的数据。如果您的应用程序需要如此强大的计算能力，以至于单个Amazon超大计算实例都不能满足您的要求，那么您还可以考虑分布式计算。这允许多台机器并行工作，以实现您的目标。作为一个简单的例子，您可能让一台机器抓取一组站点，另一台机器抓取另一组站点，并让它们将收集到的数据存储在同一个数据库中。 如果您的应用程序需要如此强大的计算能力，以至于单个Amazon超大计算实例都不能满足您的要求，那么您还可以考虑分布式计算。这允许多台机器并行工作，以实现您的目标。作为一个简单的例子，您可能让一台机器抓取一组站点，另一台机器抓取另一组站点，并让它们将收集到的数据存储在同一个数据库中。当然，正如前面几章所指出的，许多人可以复制谷歌搜索所做的，但是很少有人能复制谷歌搜索所做的规模。分布式计算是计算机科学的一个大领域，超出了本书的范围。然而，学习如何在远程服务器上启动应用程序是必要的第一步，您可能会对当今计算机的功能感到惊讶。 Tor洋葱路由器网络(Onion Router network)，缩写Tor更为人所知)是一个由志愿者服务器组成的网络，它们被设置为通过不同服务器的许多层(因此洋葱指的是不同的服务器)路由和重路由流量，以掩盖其起源。数据在进入网络之前是加密的，因此如果任何特定的服务器被窃听，通信的性质就不能被揭示。此外,虽然任何特定的入站和出站通信服务器可以妥协,一个需要知道的细节入站和出站通信路径上的所有服务器通信以破译的真正开始和端点通信一个近乎不可能的壮举。 Tor通常被人权工作者和政治揭发者用来与记者交流，它的大部分资金来自美国政府。当然，它也经常用于非法活动，因此仍然是政府监视的一个固定目标(尽管到目前为止，监视的效果好坏参半)。 limitation虽然您在本书中使用Tor的原因是为了更改您的IP地址，而不是实现完全匿名本身，但是有必要花一些时间来解决Tor在匿名流量方面的一些优势和限制。虽然您可以假设在使用Tor时，根据web服务器，您来自的IP地址不是可以追溯到您的IP地址，但是您与该web服务器共享的任何信息都可能暴露您。例如，如果您登录到自己的Gmail帐户，然后进行与谷歌有关的搜索，那么这些搜索现在可以与您的身份绑定在一起。然而，在显而易见的之外，即使是登录Tor的行为也可能对您的匿名性造成危险。2013年12月，一名哈佛本科生为了逃避期末考试，用一个匿名电子邮件账户，通过Tor网络向学校发送了一封炸弹威胁邮件。当哈佛大学的IT团队查看他们的日志时，他们发现，在炸弹威胁发出期间，Tor网络的流量仅来自一台注册为已知学生的机器。尽管他们无法确定这些流量的最终目的地(只知道它是通过Tor发送的)，但时间匹配且当时只有一台机器登录的事实足以对这名学生提出起诉。登录Tor并不是一件自动隐身斗篷，也不能让你在互联网上随心所欲。虽然它是一个有用的工具，但一定要谨慎、明智地使用它，当然还有道德。 安装并运行Tor是使用Python with Tor的必要条件，您将在下一节中看到这一点。幸运的是，Tor服务非常容易安装并开始运行。只需进入Tor下载页面并下载、安装、打开和连接!请记住，在使用Tor时，您的internet速度可能会显得比较慢。耐心点，它可能会环游世界好几次!258 PySocksPySocks是一个非常简单的Python模块，它能够通过代理服务器路由流量，并与Tor协同工作。您可以从它的网站上下载它，或者使用任意数量的第三方模块管理器来安装它。 虽然这个模块的文档并不多，但是使用它非常简单。运行此代码时，Tor服务必须运行在端口9150(默认端口)上: 12345678# Must have the TOR service running on port 9150 while running thisimport socksimport socketfrom urllib.request import urlopensocks.set_default_proxy(socks.SOCKS5, "localhost", 9150)socket.socket = socks.socksocketprint(urlopen('http://icanhazip.com').read()) 网站http://icanhazip.com只显示连接到服务器的客户机的IP地址，这对于测试非常有用。运行此脚本时，它应该显示一个不是您自己的IP地址。 如果您想在Tor中使用Selenium和PhantomJS，您根本不需要PySocks——只要确保Tor当前正在运行，并添加可选的service_args参数，指定Selenium应该通过端口9150连接: 1234567891011from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument("--headless")chrome_options.add_argument("--proxy-server=socks5://127.0.0.1:9150")driver = webdriver.Chrome(executable_path='drivers/chromedriver', options=chrome_options)driver.get('http://icanhazip.com')print(driver.page_source)driver.close() 同样，这应该打印出一个不是您自己的IP地址，而是您正在运行的Tor客户机当前使用的IP地址。 Remote Hosting尽管在你取出信用卡后完全匿名性就消失了，但是远程托管你的网页抓取器可以极大地提高它们的速度。这不仅是因为您能够在比您可能拥有的大得多的机器上购买时间，而且还因为连接不再需要在Tor网络的各个层之间来回跳转才能到达目的地。 Running from a Website-Hosting Account如果您有一个个人或商业网站，您可能已经有办法从外部服务器运行web scraper。即使使用相对锁定的web服务器(您无法访问命令行)，也可以通过web接口触发脚本启动和停止。如果您的网站托管在Linux服务器上，那么服务器很可能已经运行Python。如果您是在Windows服务器上托管，那么您可能就不走运了;您需要特别检查是否安装了Python，或者服务器管理员是否愿意安装它。 大多数小型网站托管提供商都提供cPanel软件，用于提供基本的管理服务和有关网站及相关服务的信息。如果你有访问cPanel的权限，你可以通过进入Apache处理程序并添加一个新的处理程序(如果还没有的话)来确保Python被设置为在你的服务器上运行: 12Handler: cgi-scriptExtension(s): .py 这告诉您的服务器，所有Python脚本都应该作为cgi脚本执行。CGI是通用网关接口的缩写，是任何可以在服务器上运行并动态生成网站上显示的内容的程序。通过显式地将Python脚本定义为CGI脚本，您允许服务器执行它们，而不是仅仅在浏览器中显示它们或向用户发送下载。 编写您的Python脚本，将其上载到服务器，并将文件权限设置为755以允许执行它。要执行脚本，请导航到您通过浏览器上载脚本的位置(或者更好的方法是编写一个scraper来为您执行脚本)。如果你担心公众访问和执行脚本，你有两个选择: 将脚本存储在一个模糊或隐藏的URL中，并确保永远不要从任何其他可访问的URL链接到脚本，以避免搜索引擎对其进行索引。 使用密码保护脚本，或者要求在脚本执行之前向其发送密码或秘密令牌。 当然，从专门用于显示网站的服务中运行Python脚本有点不太好。例如，您可能会注意到您的web剪贴簿兼网站加载速度有点慢。实际上，直到整个爬虫完成，页面才会真正加载(包括您可能已经写入的所有print语句的输出)。这可能需要几分钟、几个小时，或者根本不可能完成，这取决于它是如何编写的。虽然它确实完成了任务，但是您可能需要更多的实时输出。为此，您需要的服务器不仅仅是为web而设计的。 Running from the Cloud在过去的计算时代，程序员为执行代码而在计算机上付费或预留时间。随着个人电脑的出现，你不需要在自己的电脑上编写和执行代码。现在，应用程序的雄心已经超过了微处理器的发展，以至于程序员再次转向按小时付费的计算实例。然而，这一次，用户不再为一台物理机器上的时间付费，而是为它同等的计算能力付费，这种计算能力通常分布在许多机器上。这个系统模糊的结构使得计算能力可以根据需求高峰的时间来定价。例如，当低成本比即时性更重要时，Amazon允许对现场实例进行投标。 计算实例也更加专门化，可以根据应用程序的需要进行选择，选项包括高内存、快速计算和大存储。虽然web抓取器通常不会使用太多内存，但是您可能希望考虑使用大存储或快速计算来替代更通用的抓取应用程序实例。如果您正在进行大量的自然语言处理、OCR工作或路径查找(例如Wikipedia的六度问题)，那么快速计算实例可能会工作得很好。如果您正在抓取大量数据、存储文件或进行大规模分析，那么您可能希望使用存储优化的实例。尽管就花费而言，天空是有限的，但在撰写本文时，实例的启动价格仅为1.3美分/小时(对于Amazon EC2微实例)，谷歌最便宜的实例为4.5美分/小时，最低仅需10分钟。由于规模经济，购买大型公司的小型计算实例与购买自己的物理专用机器是一样的，只是现在，您不需要雇佣IT人员来保持它的运行。 当然，设置和运行云计算实例的分步说明在一定程度上超出了本书的范围，但是您可能会发现不需要分步说明。由于亚马逊和谷歌(更不用说这个行业中无数的小公司了)都在争夺云计算的利润，他们让创建新实例变得非常简单，只需遵循一个简单的提示，想一个应用程序名称，并提供一个信用卡号码。在撰写本文时，Amazon和谷歌都提供了数百美元的免费计算时间，以进一步吸引新客户。 一旦您设置了一个实例，您就应该自豪地拥有一个IP地址、用户名和公钥/私钥，可以使用它们通过SSH连接到您的实例。从这里开始，一切都应该与使用物理上拥有的服务器相同——当然，您不再需要担心硬件维护或运行过多的高级监视工具。 Additional Resources许多年前，在云中运行主要是那些喜欢费力阅读文档并且已经有一些服务器管理经验的人的领域。然而，由于云计算提供商之间的日益普及和竞争，这些工具已经得到了极大的改进。不过，对于构建大型或更复杂的抓取器和爬行器，您可能需要更多关于创建用于收集和存储数据的平台的指导。Marc Cohen、Kathryn Hurley和Paul Newson (O Reilly)编写的谷歌计算引擎是一种使用Python和JavaScript的谷歌云计算的直接资源。它不仅涵盖了谷歌的用户界面，而且还包括命令行和脚本工具，您可以使用这些工具为您的应用程序提供更大的灵活性。如果您更喜欢使用Amazon, Mitch Garnaat的Python和AWS Cookbook (O Reilly)是一个简短但非常有用的指南，它将帮助您开始使用Amazon Web Services，并向您展示如何启动和运行一个可伸缩的应用程序。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-16]]></title>
    <url>%2F2019%2F08%2F16%2FWebScraping16%2F</url>
    <content type="text"><![CDATA[网络爬行速度很快。至少，它通常比雇佣一打实习生手工从互联网上复制数据快得多!当然，技术的进步和享乐跑步机要求在某一时刻，即使是这样也“不够快”。“这就是人们开始关注分布式计算的时候。 与大多数其他技术领域不同，web爬行通常不能仅仅通过“在问题上投入更多的周期”来改进。“运行一个进程非常快;运行两个进程的速度不一定是原来的两倍。运行三个进程可能会禁止您访问正在处理所有请求的远程服务器! 然而，在某些情况下，并行web爬行或运行并行线程/进程仍然是有益的: 从多个源(多个远程服务器)而不是单个源收集数据 对收集的数据执行长时间/复杂的操作(例如进行图像分析或OCR)，这些操作可以与获取数据并行完成 从为每个查询付费的大型web服务收集数据，或者在使用协议的范围内创建到服务的多个连接 Processes versus ThreadsPython同时支持多处理和多线程。多处理和多线程都实现了相同的最终目标:同时执行两个编程任务，而不是以更传统的线性方式运行程序。 在计算机科学中，在操作系统上运行的每个进程可以有多个线程。每个进程都有自己分配的内存，这意味着多个线程可以访问相同的内存，而多个进程不能而且必须显式地通信信息。使用多线程编程在具有共享内存的单独线程中执行任务通常被认为比多进程编程更容易。但这种便利是有代价的。 Python的全局解释器锁(或GIL)的作用是防止线程同时执行同一行代码。GIL确保所有进程共享的公共内存不会被破坏(例如，内存中的字节一半用一个值写，一半用另一个值写)。这种锁定使编写多线程程序成为可能，并且知道在同一行中您将得到什么，但是它也有可能产生瓶颈。 Multithreaded CrawlingPython 3.x使用_thread模块;不推荐使用thread程模块。 下面的例子说明了如何使用多个线程执行任务: 12345678910111213141516import _threadimport timedef print_time(threadName, delay, iterations): start = int(time.time()) for i in range(0,iterations): time.sleep(delay) seconds_elapsed = str(int(time.time()) - start) print ("&#123;&#125; &#123;&#125;".format(seconds_elapsed, threadName))try: _thread.start_new_thread(print_time, ('Fizz', 3, 33)) _thread.start_new_thread(print_time, ('Buzz', 5, 20)) _thread.start_new_thread(print_time, ('Counter', 1, 100))except: print ('Error: unable to start thread')while 1: pass 这是对经典FizzBuzz编程测试的一个参考，输出有点冗长: 1234567891011121314151 Counter2 Counter3 Fizz3 Counter4 Counter5 Buzz5 Counter6 Fizz6 Counter7 Counter...15 Buzz15 Fizz15 Counter... 脚本启动三个线程，一个线程每三秒打印“Fizz”，另一个线程每五秒打印“Buzz”，第三个线程每秒钟打印“Counter”。 启动线程后，主执行线程将执行while 1循环，该循环将保持程序(及其子线程)执行，直到用户按Ctrl-C停止执行。 其中，代码解释：time.time()读取当前时间，time.sleep(1)，延时1s再进行下一行代码，因此，会每隔1s中输出一个数字，依次为1 2 3 4 5，如果将第三行注释掉，因为处理过快，将输出0 0 0 0 0。 12345start = int(time.time())for i in range(0,5): time.sleep(1) now = str(int(time.time()) - start) print(now) 可以在线程中执行一个有用的任务，而不是打印嘶嘶声和嗡嗡声，例如爬行一个网站: 123456789101112131415161718192021222324252627282930313233343536from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport randomimport _threadimport timevisited = []def getLinks(thread_name, bsObj): print('Getting links in &#123;&#125;'.format(thread_name)) links = bsObj.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')) return [link for link in links if link not in visited]def scrape_article(thread_name, path): visited.append(path) html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(path)) time.sleep(5) bsObj = BeautifulSoup(html, 'html.parser') title = bsObj.find('h1').get_text() print('Scraping &#123;&#125; in thread &#123;&#125;'.format(title, thread_name)) links = getLinks(thread_name, bsObj) if len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs['href'] print(newArticle) scrape_article(thread_name, newArticle)try: _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon',)) _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python',))except: print ('Error: unable to start threads')while 1: pass 输出： 1234567891011121314Scraping Monty Python in thread Thread 2Getting links in Thread 2/wiki/International_Standard_Name_IdentifierScraping Kevin Bacon in thread Thread 1Getting links in Thread 1/wiki/Critics%27_Choice_Movie_Award_for_Best_ActorScraping International Standard Name Identifier in thread Thread 2Getting links in Thread 2/wiki/ISO_639-3Scraping Critics&apos; Choice Movie Award for Best Actor in thread Thread 1Getting links in Thread 1/wiki/Darkest_Hour_(film)Scraping ISO 639-3 in thread Thread 2Getting links in Thread 2 因为爬行Wikipedia的速度几乎是单线程爬行速度的两倍，所以包含time.sleep(5)这一行可以防止脚本在Wikipedia的服务器上增加太多的负载。实际上，当运行在请求数量不成问题的服务器上时，应该删除这一行。 如果您想稍微重写一下这个代码，以跟踪到目前为止线程已经共同看到的文章，这样就不会有文章被访问两次了，那该怎么办?您可以在多线程环境中使用列表，就像在单线程环境中使用列表一样: 12345678visited = []def getLinks(thread_name, bsObj): print('Getting links in &#123;&#125;'.format(thread_name)) links = bsObj.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')) return [link for link in links if link not in visited]def scrape_article(thread_name, path): visited.append(path) 注意，您正在将路径附加到已访问路径列表中，作为scrape_article采取的第一个操作。这减少了，但并没有完全消除，它将被抓取两次的机会。 如果您运气不好，两个线程可能仍然在同一时刻偶然遇到相同的路径，两个线程都将看到它不在已访问列表中，然后两个线程都将随后将其添加到列表中并同时删除。然而，实际上，由于执行速度和Wikipedia包含的页面数量，这种情况不太可能发生。 这是竞态条件的一个例子。竞态条件调试起来很棘手，即使对于经验丰富的程序员也是如此，因此针对这些潜在的情况评估代码、估计其可能性并预测其影响的严重性是非常重要的。在这种特殊的竞争条件下，爬虫在同一页面上重复运行两次，可能不值得到处写。 Race Conditions and Queues虽然您可以使用列表在线程之间进行通信，但是列表并不是专门为线程之间的通信而设计的，滥用列表很容易导致程序执行缓慢，甚至由于竞争条件而导致错误。列表非常适合添加或从列表中读取内容，但不适合在任意点删除项目，特别是从列表的开头删除项目。例如：myList.pop(0)，实际上需要Python重写整个列表，从而降低程序执行速度。 更危险的是，列表还可以方便地意外地在不线程安全的行中编写。例如： 1myList[len(myList)-1] 实际上，在多线程环境中可能不会得到列表中的最后一项，或者在另一个操作修改列表之前立即计算len(myList)-1的值时，它甚至可能引发异常。 有人可能会争辩说，前面的语句可以更“python化”地写成myList[-1]，当然，没有人曾经在虚弱的时候意外地编写了非python代码(尤其是Java开发人员回想起他们使用myList[myList.length-1],长度是1)!但是，即使您的代码无可挑剔，也请考虑其他形式的非线程安全行，包括列表: 12my_list[i] = my_list[i] + 1my_list.append(my_list[-1]) 这两种情况都可能导致竞态条件，从而导致意想不到的结果。因此，让我们放弃列表，使用非列表变量将消息传递给线程! 12345# Read the message in from the global listmy_message = global_message# Write a message backglobal_message = "I've retrieved the message"# do something with my_message 这似乎很好，直到您意识到您可能无意中覆盖了来自另一个线程的另一条消息，即在第一行和第二行之间的那一瞬间，“我收到了您的消息”。“所以现在您只需要为每个线程构造一系列精心设计的个人消息对象，并使用一些逻辑来确定谁得到了什么……或者您可以使用为此目的构建的队列模块。 队列是类似列表的对象，可以使用先进先出(FIFO)方法或后进先出(LIFO)方法进行操作。队列通过队列接收来自任何线程的消息。put(&#39;My message&#39;)可以将消息传输到调用queue.get()的任何线程。队列的设计目的不是存储静态数据，而是以线程安全的方式传输它。从队列中检索后，它应该只存在于检索它的线程中。因此，它们通常用于委托任务或发送临时通知。 这在web爬行中非常有用。例如，假设您希望将scraper收集的数据持久化到数据库中，并且希望每个线程都能够快速地持久化它的数据。为所有线程提供一个共享连接可能会导致问题(单个连接不能并行处理请求)，但是为每个抓取线程提供自己的数据库连接没有任何意义。随着scraper的增长(您可能最终要在100个不同的线程中从100个不同的网站收集数据)，这可能会转化为大量的数据库连接，这些连接大部分是空闲的，在页面加载之后，它们只会偶尔执行一次写操作。 相反，您可以使用数量更少的数据库线程，每个线程都有自己的连接，可以从队列中取出项目并存储它们。这提供了一组更易于管理的数据库连接。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport randomimport _threadfrom queue import Queueimport timeimport pymysqldef storage(queue): conn = pymysql.connect(host='127.0.0.1', unix_socket='/tmp/mysql.sock', user='root', passwd='', db='mysql', charset='utf8') cur = conn.cursor() cur.execute('USE wiki_threads') while 1: if not queue.empty(): article = queue.get() cur.execute('SELECT * FROM pages WHERE path = %s', (article["path"])) if cur.rowcount == 0: print("Storing article &#123;&#125;".format(article["title"])) cur.execute('INSERT INTO pages (title, path) VALUES (%s, %s)', (article["title"], article["path"])) conn.commit() else: print("Article already exists: &#123;&#125;".format(article['title']))visited = []def getLinks(thread_name, bsObj): print('Getting links in &#123;&#125;'.format(thread_name)) links = bsObj.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')) return [link for link in links if link not in visited]def scrape_article(thread_name, path, queue): visited.append(path) html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(path)) time.sleep(5) bsObj = BeautifulSoup(html, 'html.parser') title = bsObj.find('h1').get_text() print('Added &#123;&#125; for storage in thread &#123;&#125;'.format(title, thread_name)) queue.put(&#123;"title":title, "path":path&#125;) links = getLinks(thread_name, bsObj) if len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs['href'] scrape_article(thread_name, newArticle, queue)queue = Queue()try: _thread.start_new_thread(scrape_article, ('Thread 1', '/wiki/Kevin_Bacon', queue,)) _thread.start_new_thread(scrape_article, ('Thread 2', '/wiki/Monty_Python', queue,)) _thread.start_new_thread(storage, (queue,))except: print ('Error: unable to start threads')while 1: pass 这个脚本创建了三个线程:两个线程从Wikipedia中随机抓取页面，第三个线程将收集到的数据存储在MySQL数据库中。有关MySQL和数据存储的更多信息，请参见第6章。 The threading ModulePython _thread模块是一个相当底层的模块，它允许您对线程进行微管理，但是没有提供很多高级函数来简化工作。threading模块是一个高级接口，它允许您干净地使用线程，同时仍然公开底层_thread的所有特性。 例如，您可以使用enumerate等静态函数来获得通过线程模块初始化的所有活动线程的列表，而不需要自己跟踪它们。activeCount函数同样提供线程总数。_thread中的许多函数都被赋予了更方便或更容易记住的名称，比如currentThread而不是get_ident来获取当前线程的名称。 12345678910111213import threadingimport timedef print_time(threadName, delay, iterations): start = int(time.time()) for i in range(0,iterations): time.sleep(delay) seconds_elapsed = str(int(time.time()) - start) print ('&#123;&#125; &#123;&#125;'.format(seconds_elapsed, threadName))t = threading.Thread(target=print_time, args=('Fizz', 3, 33)).start()t = threading.Thread(target=print_time, args=('Buzz', 5, 20)).start()t = threading.Thread(target=print_time, args=('Counter', 1, 100)).start() 它生成与前面简单_thread示例相同的“FizzBuzz”输出。 线程模块的优点之一是易于创建其他线程不可用的本地线程数据。如果您有多个线程，每个线程都抓取不同的网站，并且每个线程都跟踪自己的本地访问页面列表，那么这可能是一个很好的特性。 这个本地数据可以通过调用thread .local()在thread函数的任何位置创建: 123456import threadingdef crawler(url): data = threading.local() data.visited = [] # Crawl sitethreading.Thread(target=crawler, args=('http://brookings.edu')).start() 这解决了线程中共享对象之间发生竞争条件的问题。当一个对象不需要被共享时，它不应该被共享，而应该保存在本地线程内存中。为了在线程之间安全地共享对象，仍然可以使用上一节中的队列。 通常情况下，爬行器的设计运行时间很长。isAlive方法可以确保，如果线程崩溃，它会重新启动: 1234567threading.Thread(target=crawler)t.start()while True: time.sleep(1) if not t.isAlive(): t = threading.Thread(target=crawler) t.start() 可以通过扩展线程添加其他监视方法。threading.Thread对象: 123456789101112131415161718192021222324252627282930313233import threadingimport timeclass Crawler(threading.Thread): def __init__(self): threading.Thread.__init__(self) self.done = False def isDone(self): print("exe isDone") return self.done def run(self): print("exe run") time.sleep(5) self.done = True raise Exception('Something bad happened!')tim = int(time.time())t = Crawler()t.start()while True: print("loop------") now = str(int(time.time())-tim) print(now) time.sleep(1) if t.isDone(): print('Done') break if not t.isAlive(): t = Crawler() t.start() 这是我加入调试输出的，因此输出如下： 123456789101112131415161718192021222324exe runloop------0exe isDoneloop------1exe isDoneloop------2exe isDoneloop------3exe isDoneloop------4exe isDoneDoneException in thread Thread-1:Traceback (most recent call last): File &quot;D:\Anaconda3\lib\threading.py&quot;, line 917, in _bootstrap_inner self.run() File &quot;threading_crawler.py&quot;, line 17, in run raise Exception(&apos;Something bad happened!&apos;)Exception: Something bad happened! 这个新的爬虫类包含一个isDone方法，可以用来检查爬虫程序是否完成了爬行。如果需要完成一些额外的日志记录方法，使线程无法关闭，但是爬行工作的大部分已经完成，那么这可能是有用的。通常，isDone可以替换为某种状态或进度度量——例如，记录了多少页面，或者当前页面。 Crawler.run所引发的任何异常都会导致类被重新启动，直到isDone为真且程序退出为止。 扩展threading.Thread在您的爬虫类可以提高他们的健壮性和灵活性，以及您的能力，以监测任何属性的许多爬虫一次。 Multiprocess CrawlingPythonProcessing处理模块创建了可以从主进程启动和连接的新进程对象。下面的代码使用了线程进程一节中的FizzBuzz示例来演示。 1234567891011121314151617181920212223242526272829from multiprocessing import Processimport timefrom multiprocessing import freeze_supportdef print_time(threadName, delay, iterations): start = int(time.time()) for i in range(0,iterations): time.sleep(delay) seconds_elapsed = str(int(time.time()) - start) print (threadName if threadName else seconds_elapsed)processes = []processes.append(Process(target=print_time, args=(None, 1, 100)))processes.append(Process(target=print_time, args=("Fizz", 3, 33)))processes.append(Process(target=print_time, args=("Buzz", 5, 20)))if __name__ == '__main__': freeze_support() for p in processes: p.start() for p in processes: p.join() print("Program complete") 请记住，每个进程都被操作系统。如果您通过OS的活动监视器或任务管理器查看流程，您应该会看到这一点，如图16-1所示 Multiprocess Crawling多线程Wikipedia爬行示例可以修改为使用单独的进程，而不是单独的线程: 123456789101112131415161718192021222324252627282930313233343536from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport randomfrom multiprocessing import Process, Queueimport osimport timeimport Threaddef getLinks(bsObj, queue): print('Getting links in &#123;&#125;'.format(os.getpid())) links = bsObj.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')) return [link for link in links if link not in queue.get()]def scrape_article(path, queue): queue.get().append() print("Process &#123;&#125; list is now: &#123;&#125;".format(os.getpid(), visited)) html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(path)) time.sleep(5) bsObj = BeautifulSoup(html, 'html.parser') title = bsObj.find('h1').get_text() print('Scraping &#123;&#125; in process &#123;&#125;'.format(title, os.getpid())) links = getLinks(bsObj) if len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs['href'] print(newArticle) scrape_article(newArticle)processes = []queue = Queue()processes.append(Process(target=scrape_article, args=('/wiki/Kevin_Bacon', queue,)))processes.append(Process(target=scrape_article, args=('/wiki/Monty_Python', queue,)))for p in processes: p.start() 同样，通过包含time.sleep(5)，您可以人为地减慢scraper的进程，这样就可以在不给Wikipedia的服务器增加不合理的高负载的情况下使用它。在这里，您将用os.getpid()替换用户定义的thread_name(作为参数传递)，它不需要作为参数传递，并且可以在任何时候访问。 12345678910Scraping Kevin Bacon in process 84275Getting links in 84275/wiki/PhiladelphiaScraping Monty Python in process 84276Getting links in 84276/wiki/BBCScraping BBC in process 84276Getting links in 84276/wiki/Television_Centre,_Newcastle_upon_TyneScraping Philadelphia in process 84275 从理论上讲，在单独的进程中爬行比在单独的线程中爬行稍微快一些，主要有两个原因: 进程不受GIL的锁定限制，可以执行相同的代码行并同时修改相同的对象(实际上是相同对象的单独实例化)。 进程可以运行在多个CPU内核上，如果每个进程或线程都是处理器密集型的，那么这可能会提供速度优势。 然而，这些优势伴随着一个主要的缺点。在前面的程序中，所有找到的url都存储在一个全局访问列表中。当您使用多个线程时，此列表在所有线程之间共享;在没有罕见竞争条件的情况下，一个线程不能访问已经被另一个线程访问过的页面。但是，每个进程现在都有自己独立的已访问列表版本，并且可以自由访问其他进程已经访问过的页面。 Communicating Between Processes进程在它们自己的独立内存中运行，如果希望它们共享信息，这可能会导致问题。 修改前一个例子，打印当前输出的访问列表，你可以看到这个原则的行动: 123def scrape_article(path): visited.append(path) print("Process &#123;&#125; list is now: &#123;&#125;".format(os.getpid(), visited)) 这将导致如下输出: 12345678910Process 84552 list is now: [&apos;/wiki/Kevin_Bacon&apos;]Process 84553 list is now: [&apos;/wiki/Monty_Python&apos;]Scraping Kevin Bacon in process 84552Getting links in 84552/wiki/Desert_StormProcess 84552 list is now: [&apos;/wiki/Kevin_Bacon&apos;, &apos;/wiki/Desert_Storm&apos;]Scraping Monty Python in process 84553Getting links in 84553/wiki/David_JasonProcess 84553 list is now: [&apos;/wiki/Monty_Python&apos;, &apos;/wiki/David_Jason&apos;] 但是有一种方法可以通过两种类型的Python对象(队列和管道)在同一台机器上的进程之间共享信息。队列类似于前面看到的线程队列。信息可以由一个进程放入其中，然后由另一个进程删除。删除此信息后，它将从队列中删除。因为队列被设计为“临时数据传输”，它们不太适合保存静态引用，如“已访问的网页列表”。 但是，如果这个静态的web页面列表被某种类型的抓取委托器所替代呢?刮刀可以弹出一个任务队列的形式从一个路径刮(例如,/ wiki / Monty_Python)作为回报,添加一个列表,发现url回到一个单独的队列处理刮的全权代表,因此只有新url添加到第一个任务队列。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport randomfrom multiprocessing import Process, Queueimport osimport timedef task_delegator(taskQueue, foundUrlsQueue): #Initialize with a task for each process visited = ['/wiki/Kevin_Bacon', '/wiki/Monty_Python'] taskQueue.put('/wiki/Kevin_Bacon') taskQueue.put('/wiki/Monty_Python') while 1: #Check to see if there are new links in the foundUrlsQueue for processing if not foundUrlsQueue.empty(): links = [link for link in foundUrlsQueue.get() if link not in visited] for link in links: #Add new link to the taskQueue taskQueue.put(link) #Add new link to the visited list visited.append(link)def get_links(bsObj): links = bsObj.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')) return [link.attrs['href'] for link in links]def scrape_article(taskQueue, foundUrlsQueue): while 1: while taskQueue.empty(): #Sleep 100 ms while waiting for the task queue #This should be rare time.sleep(.1) path = taskQueue.get() html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(path)) time.sleep(5) bsObj = BeautifulSoup(html, 'html.parser') title = bsObj.find('h1').get_text() print('Scraping &#123;&#125; in process &#123;&#125;'.format(title, os.getpid())) links = get_links(bsObj) #Send these to the delegator for processing foundUrlsQueue.put(links)processes = []taskQueue = Queue()foundUrlsQueue = Queue()processes.append(Process(target=task_delegator, args=(taskQueue, foundUrlsQueue,)))processes.append(Process(target=scrape_article, args=(taskQueue, foundUrlsQueue,)))processes.append(Process(target=scrape_article, args=(taskQueue, foundUrlsQueue,)))for p in processes: p.start() 此爬虫与最初制造的铲运机在结构上存在一些差异。不是每个进程或线程从它们被分配的起点开始按照自己的随机游走，而是一起对网站进行完整的覆盖爬行。每个进程都可以从队列中提取任何“任务”，而不仅仅是它们自己找到的链接。 Multiprocess Crawling—Another Approach所有讨论的多线程和进程爬行的方法都假定您需要对子线程和进程进行某种“父级指导”。您可以同时启动它们，也可以同时结束它们，还可以在它们之间发送消息或共享内存。 但是，如果您的爬虫的设计方式是不需要指导或通信的呢?也许没有什么理由开始疯狂import _thread。 例如，假设您想并行爬行两个类似的网站。您已经编写了一个爬行器，它可以爬行这两个网站中的任何一个，由一个小的配置更改或命令行参数决定。你完全没有理由不能简单地做以下事情: 12$ python my_crawler.py website1$ python my_crawler.py website2 瞧，您刚刚启动了一个多进程web爬虫程序，同时节省了您的CPU维护要引导的父进程的开销! 当然，这种方法也有缺点。如果您想以这种方式在同一个网站上运行两个web爬虫程序，您需要某种方法来确保它们不会意外地开始抓取相同的页面。解决方案可能是创建一个URL规则(“爬虫1抓取博客页面，爬虫2抓取产品页面”)或以某种方式分割站点。 或者，您可以通过某种中间数据库来处理这种协调。在进入一个新的链接之前，爬虫程序可能会向数据库发出一个请求，询问这个页面是否已被爬行?爬虫程序使用数据库作为进程间通信系统。当然，如果没有仔细考虑，如果数据库连接很慢，这种方法可能会导致竞争条件或延迟(只有在连接到远程数据库时才可能出现问题)。 您可能还会发现，这种方法的可伸缩性不太好。使用Process模块可以动态地增加或减少爬行站点的进程数量，甚至可以存储数据。手动启动它们需要一个人亲自运行脚本或一个单独的管理脚本(无论是bash脚本、cron作业还是其他东西)来完成此任务。然而，这是我过去成功地使用过的一种方法。对于小型的one - off项目，它是快速获取大量信息的好方法，尤其是跨多个网站。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-15]]></title>
    <url>%2F2019%2F08%2F09%2FWebScraping15%2F</url>
    <content type="text"><![CDATA[当处理具有大型开发堆栈的web项目时，通常只有堆栈的后面才会定期进行测试。现在的大多数编程语言(包括Python)都有某种类型的测试框架，但是网站前端常常被排除在这些自动化测试之外，尽管它们可能是项目中唯一面向客户的部分。 部分问题在于，网站常常是许多标记语言和编程语言的混合体。您可以为JavaScript的各个部分编写单元测试，但是如果与之交互的HTML发生了更改，以致JavaScript无法在页面上执行预期的操作，那么这是没有用的，即使它可以正常工作。 前端网站测试的问题常常被留到以后再考虑，或者委托给级别较低的程序员，他们最多配备一个检查表和一个bug跟踪器。然而，只要稍微提前一点努力，您就可以用一系列单元测试替换这个检查表，并使用web scraper替换人眼。 想象一下:web开发的测试驱动开发。每天进行测试，确保web界面的所有部分都按预期运行。每当有人添加新的网站功能或更改元素的位置时，都会运行一组测试。本章介绍了测试的基础知识，以及如何使用基于python的web scraper测试各种各样的网站，从简单到复杂。 An Introduction to Testing如果您以前从未为您的代码编写过测试，那么现在就开始吧。拥有一组可以运行的测试，以确保您的代码按照预期执行(至少，就您编写测试的目的而言)，可以节省您的时间和担心，并使发布新的更新变得容易。 What Are Unit Tests单词test和单元测试(Unit Tests)经常可以互换使用。通常，当程序员提到“编写测试”时，他们真正的意思是“编写单元测试”。另一方面，当一些程序员提到编写单元测试时，他们实际上是在编写其他类型的测试。 虽然定义和实践往往因公司而异，但单元测试通常具有以下特征: 每个单元测试测试组件功能的一个方面。例如，如果从银行帐户中取出负数美元，它可能会确保抛出适当的错误消息。 通常，单元测试根据它们所测试的组件分组在同一个类中。您可能要对从银行帐户中提取的负美元值进行测试，然后对透支银行帐户的行为进行单元测试。 每个单元测试都可以完全独立地运行，单元测试所需的任何设置或拆卸都必须由单元测试本身处理。类似地，单元测试不能干扰其他测试的成功或失败，并且它们必须能够以任何顺序成功运行。 每个单元测试都可以完全独立地运行，单元测试所需的任何设置或拆卸都必须由单元测试本身处理。类似地，单元测试不能干扰其他测试的成功或失败，并且它们必须能够以任何顺序成功运行。 每个单元测试通常至少包含一个断言。例如，单元测试可能断言2 + 2的答案是4。有时候，单元测试可能只包含一个失败状态。例如，如果抛出异常，它可能会失败，但是如果一切顺利，则默认通过。 单元测试与大部分代码是分开的。虽然它们必须导入和使用它们正在测试的代码，但是它们通常保存在单独的类和目录中。 尽管许多其他类型的测试可以编写—例如集成测试和验证测试—本章主要关注单元测试。随着最近的测试驱动开发的推进，单元测试不仅变得非常流行，而且它们的长度和灵活性使它们易于作为示例使用，而且Python具有一些内置的单元测试功能，您将在下一节中看到。 Python unittestPython的单元测试模块unittest随所有标准Python安装一起打包。只需导入并扩展unittest。TestCase，它将执行以下操作: 提供在每个单元测试之前和之后运行的setUp和tearDown函数 提供几种类型的“断言”语句，以允许测试通过或失败 运行所有以test_开头的函数作为单元测试，忽略那些没有作为测试前缀的函数 下面提供了一个简单的单元测试来确保2 + 2 = 4，Python: 123456789101112131415import unittestclass TestAddition(unittest.TestCase): def setUp(self): print('Setting up the test') def tearDown(self): print('Tearing down the test') def test_twoPlusTwo(self): total = 2+2 self.assertEqual(4, total);if __name__ == '__main__': unittest.main(argv=[''], exit=False) 输出为： 12345678.Setting up the testTearing down the test----------------------------------------------------------------------Ran 1 test in 0.003sOK 虽然setUp和tearDown在这里没有提供任何有用的功能，但是出于演示的目的，它们被包含了进来。注意，这些函数在每个单独的测试之前和之后运行，而不是在类中的所有测试之前和之后运行。 这表明测试运行成功，2 + 2确实等于4。 Running unittest in Jupyter Notebooks 只有当直接在其中执行时，__name__ == &#39;__main__&#39;才为true而不是通过import语句。这允许您使用unittest.TestCase，直接从命令行扩展的TestCase类。 在Jupyter Notebooks上，情况有点不同。由Jupyter创建的argv参数可能会在单元测试中导致错误，并且，由于unittest框架在测试运行后默认退出Python(这会在notebook内核中导致问题)，我们还需要防止这种情况发生。 在Jupyter Notebooks中，你将使用以下方法进行单元测试: 123if __name__ == '__main__': unittest.main(argv=[''], exit=False) %reset ：第二行将所有argv变量(命令行参数)设置为一个空字符串，unnittest.main将忽略这个空字符串。它还可以防止unittest在运行测试之后退出。 %reset行非常有用，因为它重置了内存并销毁了Jupyter Notebooks中所有用户创建的变量。如果没有它，您在记事本中编写的每个单元测试都将包含以前运行的所有测试的所有方法，这些测试也继承了unittest.TestCase，包括setUp和tearDown方法。这也意味着每个单元测试将运行之前单元测试中的所有方法! 使用%reset确实为用户在运行测试时创建了一个额外的手动步骤。运行测试时，笔记本会提示用户，询问他们是否确定要重置内存。只需输入y并按回车键即可。 Testing Wikipedia测试你网站的前端(不包括JavaScript，我们将在下一篇文章中讨论)就像把Python unittest库和web scraper结合起来一样简单: 12345678910111213141516171819202122from urllib.request import urlopenfrom bs4 import BeautifulSoupimport unittestclass TestWikipedia(unittest.TestCase): bs = None def setUpClass(): url = 'http://en.wikipedia.org/wiki/Monty_Python' TestWikipedia.bs = BeautifulSoup(urlopen(url), 'html.parser') def test_titleText(self): pageTitle = TestWikipedia.bs.find('h1').get_text() self.assertEqual('Monty Python', pageTitle); def test_contentExists(self): content = TestWikipedia.bs.find('div',&#123;'id':'mw-content-text'&#125;) self.assertIsNotNone(content)if __name__ == '__main__': unittest.main(argv=[''], exit=False) %reset 这一次有两个测试:第一个测试页面的标题是否是预期的“Monty Python”，第二个测试确保页面有一个内容div。 输出： 123456..----------------------------------------------------------------------Ran 2 tests in 3.263sOKOnce deleted, variables cannot be recovered. Proceed (y/[n])? y 注意，页面的内容只加载一次，并且全局对象bs在测试之间共享。这是通过使用unittest指定的函数setUpClass来实现的，setUpClass在类的开始时只运行一次(与setUp不同，setUp在每个单独的测试之前运行)。使用setUpClass而不是setUp可以节省不必要的页面加载;您可以获取内容一次并对其运行多个测试。 setUpClass和setUp之间的一个主要架构区别是，setUpClass是一个静态方法，它属于类本身并具有全局类变量，而setUp是一个实例函数，它属于类的一个特定实例。这就是为什么setUp可以在self上设置属性，而setUpClass只能访问TestWikipedia类的静态类属性。 虽然一次测试一个页面可能并没有那么强大或有趣，但是您可能还记得第3章，构建能够迭代地遍历网站所有页面的web爬虫程序相对比较容易。当您将web爬虫程序与对每个页面作出断言的单元测试组合在一起时，会发生什么? 重复运行测试的方法有很多，但是对于要在页面上运行的每一组测试，必须小心地只加载每个页面一次，而且还必须避免一次在内存中保存大量信息。下面的设置就是这样做的: 12345678910111213141516171819202122232425262728293031323334353637383940414243from urllib.request import urlopenfrom bs4 import BeautifulSoupimport unittestimport reimport randomfrom urllib.parse import unquoteclass TestWikipedia(unittest.TestCase): def test_PageProperties(self): self.url = 'http://en.wikipedia.org/wiki/Monty_Python' #Test the first 10 pages we encounter for i in range(1, 10): self.bs = BeautifulSoup(urlopen(self.url), 'html.parser') titles = self.titleMatchesURL() self.assertEqual(titles[0], titles[1]) self.assertTrue(self.contentExists()) self.url = self.getNextLink() print('Done!') def titleMatchesURL(self): pageTitle = self.bs.find('h1').get_text() urlTitle = self.url[(self.url.index('/wiki/')+6):] urlTitle = urlTitle.replace('_', ' ') urlTitle = unquote(urlTitle) return [pageTitle.lower(), urlTitle.lower()] def contentExists(self): content = self.bs.find('div',&#123;'id':'mw-content-text'&#125;) if content is not None: return True return False def getNextLink(self): # Returns random link on page, using technique from Chapter 3 links = self.bs.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')) randomLink = random.SystemRandom().choice(links) return 'https://wikipedia.org&#123;&#125;'.format(randomLink.attrs['href']) if __name__ == '__main__': unittest.main(argv=[''], exit=False) %reset 有几件事需要注意。首先，这个类中只有一个实际的测试。其他函数在技术上只是辅助函数，尽管它们要做大量的计算工作来确定测试是否通过。因为测试函数执行断言语句，所以测试的结果被传递回发生断言的测试函数。 此外，contentExists返回一个布尔值，而titleMatchesURL返回值本身进行计算。要查看为什么要返回值而不仅仅是布尔值，请比较布尔断言的结果: 123456789101112131415======================================================================FAIL: test_PageProperties (__main__.TestWikipedia)----------------------------------------------------------------------Traceback (most recent call last):File &quot;15-3.py&quot;, line 22, in test_PagePropertiesself.assertTrue(self.titleMatchesURL())AssertionError: False is not truewith the results of an assertEquals statement:======================================================================FAIL: test_PageProperties (__main__.TestWikipedia)----------------------------------------------------------------------Traceback (most recent call last):File &quot;15-3.py&quot;, line 23, in test_PagePropertiesself.assertEquals(titles[0], titles[1])AssertionError: &apos;lockheed u-2&apos; != &apos;u-2 spy plane&apos; 哪个更容易调试?(在本例中，当文章http://wikipedia.org/wiki/u-2%20spy%20plane重定向到标题为“Lockheed U-2”的文章时，错误是由于重定向而发生的。) Testing with Selenium与第11章中的Ajax抓取一样，JavaScript在进行网站测试时也面临着特殊的挑战。幸运的是，Selenium有一个非常好的框架来处理特别复杂的网站;事实上，这个库最初是为网站测试而设计的! 虽然显然是用同一种语言编写的，但是Python单元测试和Selenium单元测试的语法却出奇地少有共同点。Selenium不要求它的单元测试作为函数包含在类中;它的断言语句不需要括号;测试无声地通过，只在失败时产生某种消息: 1234driver = webdriver.PhantomJS()driver.get('http://en.wikipedia.org/wiki/Monty_Python')assert 'Monty Python' in driver.titledriver.close() 当运行时，这个测试应该不会产生任何输出。通过这种方式，Selenium测试可以比Python单元测试更随意地编写，断言语句甚至可以集成到常规代码中，如果不满足某些条件，代码执行可以终止。 Interacting With the Site最近，我想通过当地一家小企业的网站的联系方式联系它，但是发现HTML表单被破坏了;当我点击submit按钮时，什么也没有发生。经过一点调查，我发现他们使用了一个简单的mailto表单，这个表单的目的是向他们发送包含表单内容的电子邮件。幸运的是，我能够使用这些信息给他们发送电子邮件，解释他们表单的问题，并雇佣他们，尽管存在技术问题。如果我要编写一个使用或测试此表单的传统scraper，那么我的scraper很可能只是复制表单的布局，并直接发送一封电子邮件来绕过表单。我怎么能测试表单的功能,并确保它是完全通过浏览器工作吗? 虽然前几章已经讨论了导航链接、提交表单和其他类型的交互活动，但在其核心，我们所做的一切都是为了绕过浏览器界面，而不是使用它。另一方面，Selenium可以直接输入文本、单击按钮，并通过浏览器(在本例中是headless PhantomJS浏览器)执行所有操作，并检测诸如破损的表单、编码糟糕的JavaScript、HTML打印错误以及其他可能阻碍实际客户的问题。 这种测试的关键是Selenium elements。这个对象在第11章中被短暂地遇到，并通过如下调用返回: 1usernameField = driver.find_element_by_name('username') 正如您可以在浏览器中对网站的各个元素执行许多操作一样，Selenium也可以对任何给定元素执行许多操作。其中包括: 12345myElement.click()myElement.click_and_hold()myElement.release()myElement.double_click()myElement.send_keys_to_element('content to enter') 除了对元素执行一次性操作外，还可以将操作字符串组合成操作链，这些操作链可以在程序中存储和执行一次或多次。操作链非常有用，因为它们可以方便地串接多个操作的长集合，但是它们在功能上与在元素上显式调用操作相同，如上面的示例所示。 要查看这种差异，请查看http://pythonscraping.com/pages/files/form.html中的表单页面(前面在第10章中用作示例)。我们可以通过以下方式填写表格并提交: 12345678910111213141516171819202122232425262728293031from selenium import webdriverfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options)driver.get('http://pythonscraping.com/pages/files/form.html')firstnameField = driver.find_element_by_name('firstname')lastnameField = driver.find_element_by_name('lastname')submitButton = driver.find_element_by_id('submit')### METHOD 1 ###firstnameField.send_keys('Ryan')lastnameField.send_keys('Mitchell')submitButton.click()################### METHOD 2 ####actions = ActionChains(driver).click(firstnameField).send_keys('Ryan').click(lastnameField).send_keys('Mitchell').send_keys(Keys.RETURN)#actions.perform()################print(driver.find_element_by_tag_name('body').text)driver.close() 方法1在两个字段上调用send_keys，然后单击submit按钮。方法2使用一个操作链在每个字段中单击并输入文本，这在调用执行方法后按顺序进行。无论使用第一种方法还是第二种方法，该脚本都以相同的方式运行，并打印以下行: 1Hello there, Ryan Mitchell! 除了用于处理命令的对象之外，这两个方法还有另一个变体:注意，第一个方法单击Submit按钮，而第二个方法在提交文本框时使用Return键提交表单。因为有很多方法可以考虑完成相同操作的事件序列，所以有很多方法可以使用Selenium完成相同的操作。 Drag and drop 单击按钮并输入文本是一回事，但是Selenium真正的亮点在于它能够处理相对新颖的web交互形式。Selenium允许轻松地操作拖放接口。使用它的拖放函数需要指定一个源元素(要拖拽的元素)和一个偏移量来拖拽它，或者一个目标元素来拖拽它。 演示页面位于http://pythonscraping.com/pages/javascript/draggable.html展示了这类接口的一个例子: 1234567891011121314151617181920212223242526272829303132from selenium import webdriverfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.chrome.options import Optionsimport unittestclass TestAddition(unittest.TestCase): driver = None def setUp(self): chrome_options = Options() chrome_options.add_argument("--headless") self.driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options) url = 'http://pythonscraping.com/pages/javascript/draggableDemo.html' self.driver.get(url) def tearDown(self): self.driver.close() def test_drag(self): element = self.driver.find_element_by_id("draggable") target = self.driver.find_element_by_id("div2") actions = ActionChains(self.driver) actions.drag_and_drop(element, target).perform() self.assertEqual("You are definitely not a bot!", self.driver.find_element_by_id("message").text)if __name__ == '__main__': unittest.main(argv=[''], exit=False) %reset 演示页面上的message div中打印出两条消息。第一个说: 1Prove you are not a bot, by dragging the square from the blue area to the red area! 然后，快速地，在任务完成后，内容再次打印出来，现在读取: 1You are definitely not a bot! 正如演示页面所示，在许多验证码中，拖拽元素来证明你不是机器人是一个常见的主题。尽管机器人能够拖拽物体已经有很长一段时间了(这只是一个点击、按住和移动的问题)，但不知何故，用拖拽来验证人性的想法不会消失。此外，这些可拖动的CAPTCHA库很少使用任何机器人难以执行的任务，比如将小猫的图片拖放到奶牛的图片上(这要求您在解析指令时将图片标识为小猫和奶牛);相反，它们通常涉及数字排序或其他一些相当琐碎的任务，如前面示例中的任务。 当然，它们的优势在于，它们有如此多的变体，但却很少被使用;没人会费心去做一个能打败所有对手的机器人。无论如何，这个例子应该足以说明为什么不应该在大型网站上使用这种技术。 Taking screenshots 除了通常的测试功能之外，Selenium还有一个有趣的技巧，它可能会让您的测试(或让您的老板印象深刻)变得更容易一些:截屏。是的，可以从单元测试运行中创建照片证据，而不需要实际按下PrtScn键: 123driver = webdriver.PhantomJS()driver.get('http://www.pythonscraping.com/')driver.get_screenshot_as_file('tmp/pythonscraping.png') 该脚本导航到http://pythonscraping.com，然后将主页的屏幕截图存储在本地tmp文件夹中(要正确存储此文件夹，必须已经存在该文件夹)。截图可以保存为多种图像格式。 unittest or SeleniumPython unittest的语法严密性和冗长性可能是大多数大型测试套件所需要的，而Selenium测试的灵活性和强大功能可能是测试某些网站特性的唯一选择。那么该用哪个呢?秘诀是:你不必做出选择。Selenium可以很容易地获得关于网站的信息，而unittest可以评估这些信息是否符合通过测试的标准。您没有理由不能将Selenium工具导入到Python unittest中，将两者的优点结合起来。 例如，下面的脚本为网站的draggable界面创建了一个单元测试，断言它正确地说:“您不是一个机器人!“在一个元素被拖到另一个元素之后: 123456789101112131415161718192021222324252627282930from selenium import webdriverfrom selenium.webdriver import ActionChainsfrom selenium.webdriver.chrome.options import Optionsimport unittestclass TestDragAndDrop(unittest.TestCase): driver = None def setUp(self): chrome_options = Options() chrome_options.add_argument("--headless") self.driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options) url = 'http://pythonscraping.com/pages/javascript/draggableDemo.html' self.driver.get(url) def tearDown(self): self.driver.close() def test_drag(self): element = self.driver.find_element_by_id('draggable') target = self.driver.find_element_by_id('div2') actions = ActionChains(self.driver) actions.drag_and_drop(element, target).perform() self.assertEqual('You are definitely not a bot!', self.driver.find_element_by_id('message').text)if __name__ == '__main__': unittest.main(argv=[''], exit=False) %reset 实际上，网站上的任何内容都可以通过Python的unittest和Selenium的组合进行测试。事实上，结合第13章中的一些图像处理库，您甚至可以截屏并逐像素测试它应该包含什么内容!]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-14]]></title>
    <url>%2F2019%2F08%2F07%2FWebScraping14%2F</url>
    <content type="text"><![CDATA[没有什么比抓取一个站点、查看输出以及在浏览器中看不到清晰可见的数据更令人沮丧的了。或者提交一个应该完全没问题但被web服务器拒绝的表单。或者你的IP地址被不明原因的网站屏蔽。这是一些最困难的错误来解决,不仅因为它们可以意想不到的(一个脚本,很好的工作在一个站点可能不会工作在另一个,看似相同,网站),而是因为他们故意不要有任何的错误消息或使用堆栈跟踪。你被认定为一个机器人，被拒绝了，你不知道为什么。 在这本书中，我写了很多在网站上做棘手事情的方法(提交表单、提取和清理困难的数据、执行JavaScript等等)。本章有点包罗万象，因为这些技术源自各种各样的主题(HTTP头文件、CSS和HTML表单等)。然而，它们都有一个共同点:它们都是为了克服设置的一个障碍，该障碍的唯一目的是防止站点的自动web抓取。无论这些信息现在对您有多有用，我强烈建议您至少浏览一下这一章。你永远不知道什么时候它可能会帮助你解决一个困难的bug或者完全阻止一个问题。 A Note on Ethics在本书的前几章中，我讨论了web抓取所存在的法律灰色地带，以及一些道德准则。老实说，这一章在道德上可能是我最难写的一章。我的网站一直被机器人、垃圾邮件发送者、网页抓取器和各种各样不受欢迎的虚拟访客所困扰，也许你们的网站就是这样。那么，为什么要教人们如何建造更好的机器人呢? 我认为这一章很重要，包括以下几个原因: 抓取一些不想被抓取的网站，完全有道德和法律上合理的理由。在我之前的一份工作中，我是一名网络搜集者，我从一些网站上自动收集信息，这些网站在未经客户同意的情况下将客户的姓名、地址、电话号码和其他个人信息发布到互联网上。我使用这些抓取的信息向网站发出正式的请求来删除这些信息。为了避免竞争，这些网站警惕地保护这些信息。然而,我的工作,以确保我公司年代的匿名客户(其中一些缠扰者,是家庭暴力的受害者,或者有其他很好的理由要保持低调)web抓取一个令人信服的理由,我很感激我有必要的技能来做这项工作。 尽管建立一个防抓取网站几乎是不可能的(或者至少是一个合法用户仍然可以轻松访问的网站)，我希望本章的信息将帮助那些想要保护自己的网站免受恶意攻击的人。在整个过程中，我将指出每种web抓取技术的一些弱点，您可以使用它们来保护自己的站点。请记住，今天web上的大多数机器人只是在广泛地扫描信息和漏洞，即使使用本章描述的几个简单技术，也可能会阻止99%的机器人。然而，它们每个月都在变得越来越复杂，最好做好准备。 和大多数程序员一样，我不认为隐瞒任何教育信息是一件绝对积极的事情。 当您阅读本章时，请记住，其中许多脚本和描述的技术不应该在您能找到的每个站点上运行。这不仅不是一件好事，而且你可能会收到一封警告信，甚至更糟(有关收到警告信后该怎么办的更多信息，请参阅第18章)。但是，每次我们讨论一项新技术的时候，我不会用这个问题来反复强调你。所以，在本章剩下的部分，正如哲学家阿甘曾经说过的:这就是我要说的。That’s all I have to say about that Looking Like a Human对于不想被抓取的网站来说，最根本的挑战是如何区分机器人和人类。尽管许多站点使用的技术(例如，你可以做一些相当简单的事情，让你的机器人看起来更像人类。 Adjust Your Headers在本书中，您已经使用了Python请求库来创建、发送和接收HTTP请求，例如在第10章中处理网站上的表单。请求库也非常适合设置头文件。HTTP头是属性或首选项的列表，在每次向web服务器发出请求时由您发送。HTTP定义了几十种模糊的头类型，其中大多数都不常用。然而，大多数主流浏览器在启动任何连接时都一致使用以下7个字段(用我自己浏览器中的示例数据显示) Host https://www.google.com/ Connection keep-alive Accept text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8 User-Agent Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 Referrer https://www.google.com/ Accept-Encoding gzip, deflate, sdch Accept-Language en-US,en;q=0.8 下面是使用默认urllib库的典型Python scraper可能发送的头文件: Accept-Encoding identity User-Agent Python-urllib/3.4 如果你是一个网站管理员，试图阻止刮板，你更可能让哪一个? 幸运的是，可以使用请求库完全定制头文件。网站https://www.whatismybrowser.com非常适合测试服务器可以查看的浏览器属性。您将刮这个网站，以验证您的cookie设置与以下脚本: 1234567891011121314import requestsfrom bs4 import BeautifulSoupsession = requests.Session()headers = &#123;'User-Agent':'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)'\ 'AppleWebKit 537.36 (KHTML, like Gecko) Chrome', 'Accept':'text/html,application/xhtml+xml,application/xml;'\ 'q=0.9,image/webp,*/*;q=0.8'&#125;url = 'https://www.whatismybrowser.com/'\'developers/what-http-headers-is-my-browser-sending'req = session.get(url, headers=headers)bs = BeautifulSoup(req.text, 'html.parser')print(bs.find('table',&#123;'class':'table-striped'&#125;).get_text) 输出： 12345678910111213141516171819202122&lt;bound method Tag.get_text of &lt;table class="table table-striped"&gt;&lt;tr&gt;&lt;th&gt;ACCEPT&lt;/th&gt;&lt;td&gt;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;ACCEPT_ENCODING&lt;/th&gt;&lt;td&gt;gzip, deflate&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;CONNECTION&lt;/th&gt;&lt;td&gt;keep-alive&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;HOST&lt;/th&gt;&lt;td&gt;www.whatismybrowser.com&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;th&gt;USER_AGENT&lt;/th&gt;&lt;td&gt;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5)AppleWebKit 537.36 (KHTML, like Gecko) Chrome&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&gt; 输出应该显示头现在是代码中的headers dictionary对象中设置的相同的头。 虽然网站可以根据HTTP头文件中的任何属性来检查人性化，但我发现，通常唯一真正重要的设置是用户代理。不管你在做什么项目，最好把这个设置为比Python-urllib/3.4更不显眼的东西。此外，如果你遇到一个非常可疑的网站，填充一个常用但很少检查的标题，比如Accept-Language，可能是让它相信你是一个人的关键。 标题改变你看世界的方式 假设您想为一个研究项目编写一个机器学习语言翻译程序，但是缺少大量的可测试的翻译文本。许多大型站点根据标题中指定的语言首选项提供相同内容的不同翻译。简单地改变接受语言: 改变 Accept-Language: en-US，为Accept-Language: fr可能会让你从那些有规模和预算来处理翻译的网站上获得一个Bonjour(大型国际公司通常是一个不错的选择)。标题还可以提示网站更改其显示内容的格式。例如，移动设备在浏览网页时经常看到一个精简版的网站，缺少横幅广告、Flash和其他干扰。如果您尝试将用户代理更改为类似于下面的内容，您可能会发现站点变得更容易抓取，如： 123User-Agent:Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X)AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257Safari/9537.53 Handing Cookies with JavaScript正确处理cookie可以缓解许多刮擦问题，尽管cookie也可能是一把双刃剑。使用cookie跟踪网站进程的网站可能会试图切断显示异常行为的抓取器，比如过快填写表单或访问过多页面。虽然这些行为可以通过关闭和重新打开站点的连接来伪装，甚至更改IP地址(有关如何做到这一点的更多信息，请参阅第17章)，但是如果cookie泄露了您的身份，那么您的伪装可能是徒劳的。 cookie也可以用于抓取站点。如第10章所示，在站点上保持登录要求您能够在页面之间保存和呈现cookie。有些网站甚至不要求你每次登录时都要获得一个新版本的cookie，只要持有一个旧版本的登录cookie并访问该网站就足够了。如果您正在抓取单个目标网站或少量目标网站，我建议检查这些网站生成的cookie，并考虑您可能希望您的scraper处理哪些cookie。各种浏览器插件可以在您访问和移动站点时显示cookie是如何设置的。EditThisCookie是我最喜欢的Chrome扩展之一。 当然,因为它是无法执行的JavaScript,请求图书馆将无法处理许多饼干由现代跟踪软件,比如Google Analytics设置只有在执行客户端脚本(或有时基于页面的事件,例如单击按钮,浏览页面时发生)。要处理这些问题，您需要使用Selenium和PhantomJS包(我们在第11章介绍了它们的安装和基本用法)。 您可以通过访问任何站点(本例中为http://pythonscraping.com)并在webdriver上调用get_cookies()来查看cookie: 12345678910from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', chrome_options=chrome_options)driver.get('http://pythonscraping.com')driver.implicitly_wait(1)print(driver.get_cookies()) 输出： 1[&#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1565265310, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_gid&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;GA1.2.1307876323.1565178910&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1628250910, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_ga&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;GA1.2.2125567117.1565178910&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1565178970, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_gat&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;has_js&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;] 要操作cookie，可以调用delete_cookie()、add_cookie()和delete_all_cookie()函数。此外，您还可以保存和存储cookie，以便在其他web刮削器中使用。下面是一个例子，让你了解这些功能是如何协同工作的: 123456789101112131415161718192021222324252627from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionschrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', chrome_options=chrome_options)driver.get('http://pythonscraping.com')driver.implicitly_wait(1)savedCookies = driver.get_cookies()print(savedCookies)driver2 = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', chrome_options=chrome_options)driver2.get('http://pythonscraping.com')driver2.delete_all_cookies()for cookie in savedCookies: driver2.add_cookie(cookie)driver2.get('http://pythonscraping.com')driver.implicitly_wait(1)print(driver2.get_cookies()) 输出： 123[&#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1565265912, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_gid&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;GA1.2.872837726.1565179512&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1628251512, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_ga&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;GA1.2.1200894088.1565179512&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1565179572, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_gat&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;has_js&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;][&#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;has_js&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;has_js&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1565179572, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_gat&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;1&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1628251530, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_ga&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;GA1.2.1200894088.1565179512&apos;&#125;, &#123;&apos;domain&apos;: &apos;pythonscraping.com&apos;, &apos;expiry&apos;: 1565265930, &apos;httpOnly&apos;: False, &apos;name&apos;: &apos;_gid&apos;, &apos;path&apos;: &apos;/&apos;, &apos;secure&apos;: False, &apos;value&apos;: &apos;GA1.2.872837726.1565179512&apos;&#125;] 在本例中，第一个webdriver检索一个网站，打印cookie，然后将它们存储在变量savedcookie中。第二个webdriver加载相同的网站，删除自己的cookie，并添加来自第一个webdriver的cookie。这里有一些技术说明: 在添加cookie之前，第二个web驱动程序必须先加载网站。这使得Selenium知道cookie属于哪个域，即使加载网站的行为对scraper没有任何用处。 在加载每个cookie之前进行检查，看看域是否以句点(.)字符开始。这是PhantomJS的一个怪处，添加cookie中的所有域都必须以句点开头(例如，.pythonscraping.com)，即使 PhantomJS web驱动程序中的所有cookie实际上都遵循这条规则。如果您正在使用其他浏览器驱动程序，如Chrome或Firefox，则不需要这样做。 完成此操作后，第二个web驱动程序应该具有与第一个相同的cookie。根据谷歌分析，第二个webdriver现在与第一个相同，它们将以相同的方式被跟踪。如果第一个webdriver已登录到站点，第二个webdriver也将登录。 Timing Is Everything如果您做得太快，一些保护良好的网站可能会阻止您提交表单或与网站交互。即使这些安全功能没有到位，从一个网站上下载大量信息的速度明显快于普通人，这是一个让你自己被注意到并被屏蔽的好方法。 因此，尽管多线程编程可能是加载页面更快的好方法，允许您在一个线程中处理数据，而在另一个线程中重复加载页面，但这对于编写好的抓取器来说是一个糟糕的策略。您应该始终将单个页面的负载和数据请求保持在最低限度。如果可能的话，试着把它们间隔几秒钟，即使你不得不添加额外的： 12import timetime.sleep(3) 在页面加载之间是否需要这额外的几秒钟通常可以通过实验找到。很多次我从一个网站,努力获取数据不得不证明自己不是“机器人”每隔几分钟(用手解决验证码,粘贴我的新获得的饼干在刮刀的网站将抓取本身视为“证明其人性”),但添加一个时间。睡眠解决了我的问题，让我可以无限期地勉强度日。有时候你必须放慢速度才能走得快! Sometimes you have to slow down to go fast! Common Form Security Features多年来，人们已经使用了许多litmus测试，并在不同程度上成功地将web抓取器与使用浏览器的人分离开来。尽管如果一个机器人下载了一些对公众开放的文章和博客文章并没有什么大不了的，但是如果一个机器人创建了成千上万的用户帐户并开始向你的站点的所有成员发送垃圾邮件，这将是一个大问题。Web表单,特别是形式处理帐户创建和登录,对美国国家安全构成了重大威胁和计算开销,如果他们容易受到机器人的滥用,因此它在许多网站所有者的利益(或至少他们认为)试图限制访问网站。这些以表单和登录为中心的反机器人安全措施可能对web抓取器构成重大挑战。请记住，这只是在为这些表单创建自动化机器人时可能遇到的一些安全措施的部分概述。复习第13章，关于处理验证码和图像处理，以及第17章，关于处理头和IP地址，以获得更多关于处理保护良好的表单的信息。 Hidden Input Field ValuesHTML表单中的隐藏字段允许浏览器可以查看字段中包含的值，但用户不可见(除非他们查看站点的源代码)。随着越来越多的人使用cookie来存储变量，并在网站上传递这些变量，隐藏字段一度不再受欢迎，直到人们发现了它们的另一个优秀用途:防止抓取器提交表单。 在Facebook登录页面上显示这些隐藏字段的工作示例。虽然表单只有三个可见字段(用户名、密码和提交按钮)，但它在后台向服务器传递了大量信息。 隐藏字段用于防止web抓取，主要有两种方式:可以在表单页面上使用随机生成的变量填充字段，服务器希望将该变量发布到表单处理页面。如果表单中没有这个值，服务器可以合理地假设提交不是从表单页面有机地发起的，而是由机器人直接发布到处理页面的。规避此度量的最佳方法是首先抓取表单页面，收集随机生成的变量，然后从那里发布到处理页面。第二种方法是某种蜜罐。如果表单包含一个具有无害名称(如用户名或电子邮件地址)的隐藏字段，那么编写得不好的机器人可能会填写该字段并试图提交它，而不管该字段是否对用户隐藏。任何具有实际值(或与表单提交页面上的默认值不同的值)的隐藏字段都应该被忽略，用户甚至可能被从站点中屏蔽。 简而言之:有时有必要检查表单所在的页面，看看是否遗漏了服务器可能期望的内容。如果您看到几个隐藏字段，通常带有大量随机生成的字符串变量，web服务器可能会在表单提交时检查它们的存在。此外，可能还有其他检查，以确保表单变量最近只被使用过一次(这消除了将它们简单地存储在脚本中并在一段时间内反复使用它们的可能性)，或者两者都使用。 Avoiding Honeypots虽然CSS在很大程度上使区分有用信息和无用信息(例如，通过读取id和class_tags)变得非常简单，但是对于web scraper来说，它有时也会有问题。如果web表单上的字段通过CSS对用户隐藏，那么可以合理地假设访问该站点的普通用户将无法填写它，因为它不会显示在浏览器中。如果填充了表单，则很可能有一个机器人在工作，并且post将被丢弃。 这不仅适用于表单，还适用于链接、图像、文件和站点上的任何其他项目，这些项目可以由机器人读取，但是一般通过浏览器访问站点的用户是看不到的。对站点上“隐藏”链接的页面访问可以很容易地触发服务器端脚本，该脚本将阻塞用户的IP地址，将该用户记录到站点之外，或者采取一些其他操作来防止进一步访问。事实上，许多商业模式正是基于这个概念。 以位于http://pythonscraping.com/pages/itsatrap.html的页面为例。这个页面包含两个链接，一个被CSS隐藏，另一个可见。此外，它还包含一个包含两个隐藏字段的表单: 12345678910111213141516171819202122232425262728&lt;html&gt;&lt;head&gt;&lt;title&gt;A bot-proof form&lt;/title&gt;&lt;/head&gt;&lt;style&gt;body &#123;overflow-x:hidden;&#125;.customHidden &#123;position:absolute;right:50000px;&#125;&lt;/style&gt;&lt;body&gt;&lt;h2&gt;A bot-proof form&lt;/h2&gt;&lt;a href="http://pythonscraping.com/dontgohere" style="display:none;"&gt;Go here!&lt;/a&gt;&lt;a href="http://pythonscraping.com"&gt;Click me!&lt;/a&gt;&lt;form&gt;&lt;input type="hidden" name="phone" value="valueShouldNotBeModified"/&gt;&lt;p/&gt;&lt;input type="text" name="email" class="customHidden"value="intentionallyBlank"/&gt;&lt;p/&gt;&lt;input type="text" name="firstName"/&gt;&lt;p/&gt;&lt;input type="text" name="lastName"/&gt;&lt;p/&gt;&lt;input type="submit" value="Submit"/&gt;&lt;p/&gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt; 这三个元素以三种方式对用户隐藏: 第一个链接隐藏在一个简单的CSS display:none属性中。 电话字段是一个隐藏的输入字段。 将电子邮件字段向右移动5万像素(大概是从所有人的显示器屏幕上移开的)，并隐藏显示信息的滚动条，这样就隐藏了电子邮件字段。 幸运的是，由于Selenium呈现它访问的页面，所以它能够区分页面上可视的元素和不可见的元素。元素是否出现在页面上可以由is_display()函数确定。 1234567891011121314151617from selenium import webdriverfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.webdriver.chrome.options import Optionsdriver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', chrome_options=chrome_options)driver.get('http://pythonscraping.com/pages/itsatrap.html')links = driver.find_elements_by_tag_name('a')for link in links: if not link.is_displayed(): print('The link &#123;&#125; is a trap'.format(link.get_attribute('href')))fields = driver.find_elements_by_tag_name('input')for field in fields: if not field.is_displayed(): print('Do not change value of &#123;&#125;'.format(field.get_attribute('name'))) 输出： 123The link http://pythonscraping.com/dontgohere is a trapDo not change value of phoneDo not change value of email 尽管您可能不希望访问您找到的任何隐藏链接，但是您将希望确保提交任何预先填充的隐藏表单值(或者让Selenium为您提交它们)和表单的其余部分。总而言之，忽略隐藏字段是危险的，尽管在与它们交互时必须小心。 The Human Checklist在这一章里有很多信息，事实上在这本书里，关于如何建造一个看起来不那么像铲运机而更像人的铲运机。如果你一直被网站屏蔽，你不知道为什么，这里有一个清单，你可以用来补救这个问题: 首先，如果从web服务器接收到的页面是空白的、缺少信息，或者不是您所期望的(或者在您自己的浏览器中已经看到)，那么很可能是由于在站点上执行JavaScript来创建页面造成的。审查第十一章。 如果您正在向网站提交表单或发出POST请求，请检查页面以确保网站期望您提交的所有内容都以正确的格式提交。使用Chrome的Inspector面板等工具查看发送到站点的实际POST请求，以确保您拥有所有内容，并且“有机”请求看起来与机器人发送的请求相同。 如果您试图登录一个网站，但无法使登录“坚持”，或者该网站正在经历其他奇怪的“状态”行为，请检查您的cookie。确保在每次页面加载之间都正确地保存了cookie，并且每个请求都将您的cookie发送到站点。 如果您从客户端收到HTTP错误，特别是403个禁止错误，这可能表明网站已经将您的IP地址标识为bot，并且不愿意接受任何其他请求。您将需要等待直到您的IP地址从列表中删除，或者获得一个新的IP地址(移动到别的地方或见第17章)。为了确保您不会再次被阻塞，请尝试以下操作: 确保你的爬虫不会在网站上移动得太快。快速抓取是一种不好的做法，它给web管理员的服务器带来了沉重的负担，可能会让您陷入法律麻烦，而且是抓取器被列入黑名单的头号原因。给刮刀添加延迟，让它们在夜间运行。记住:匆忙编写程序或收集数据是糟糕的项目管理的标志;提前计划，避免像这样的混乱。 最明显的一条:改变你的标题!有些网站会屏蔽任何自称为scraper的东西。如果你不确定什么是合理的标题值，复制你自己浏览器的标题。 确保你没有点击或访问任何人类通常无法访问的东西(有关更多信息“Avoiding Honeypots”)。 如果你发现自己要通过许多困难的障碍才能进入网站，可以考虑联系网站管理员，让他们知道你在做什么。尝试给webmaster@&lt;domain name&gt;或admin@&lt;domain name&gt;发送电子邮件，以获得使用抓取器的许可。管理员也是人，您可能会对他们共享数据的顺从程度感到惊讶。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-13]]></title>
    <url>%2F2019%2F08%2F05%2FWebScraping13%2F</url>
    <content type="text"><![CDATA[从谷歌的自动驾驶汽车到识别假币的自动售货机，机器视觉是一个有着深远目标和影响的巨大领域。本章主要关注该领域的一个小方面:文本识别——具体地说，如何识别和使用通过使用各种Python库在网上找到的基于文本的图像。 当您不希望文本被机器人发现和读取时，使用图像代替文本是一种常见的技术。当电子邮件地址部分或全部呈现为图像时，经常可以在联系人表单中看到这一点。这取决于它的技巧，甚至可能不会被人类观众注意到，但机器人很难阅读这些图像，而这项技术足以阻止大多数垃圾邮件发送者获取你的电子邮件地址。 当然，验证码利用了这样一个事实:用户可以读取安全图像，但大多数机器人不能。有些验证码比其他的更难，这个问题我们将在本书后面讨论。 但验证码并不是抓取器需要图片到文本翻译帮助的唯一地方。即使在今天这个时代，许多文件都是从硬拷贝中扫描出来放到网上的，这就使得这些文件即使“隐藏在普通的视线中”，对大多数互联网来说也是不可访问的。“如果没有图像-文本功能，让人们能够访问这些文件的唯一方法就是手工输入——没有人有时间这么做。 将图像转换成文本被称为光学字符识别(OCR)。一些主要的库可以执行OCR，许多其他库支持它们或构建在它们之上。这个库系统有时会变得相当复杂，所以我建议您在尝试本章中的任何练习之前阅读下一节。 Overview of LibrariesPython是一种出色的语言，用于图像处理和读取、基于图像的机器学习，甚至图像创建。虽然许多库可以用于图像处理，但我们将重点关注两个库:Pillow和Tesseract。在处理和处理web上的图像时，这两个库构成了一个强大的互补组合。Pillow执行第一次扫描、清洗和过滤图像，Tesseract试图将这些图像中的形状与它的已知文本库匹配起来。本章将介绍它们的安装和基本用法，以及一些示例 Pillow尽管Pillow可能不是功能最齐全的图像处理库，但它拥有你可能需要的所有功能，而且有些——除非你打算用Python重写Photoshop，在这种情况下，你读错了书!Pillow还有一个优势，就是它是文档更丰富的第三方库之一，而且非常容易开箱即用。 从python2的Python图像库(PIL)中分离出来。x，枕头增加支持Python 3. x。和它的前辈一样，Pillow可以让你很容易地导入和操作各种各样的滤镜、蒙版，甚至是像素特定的转换图像: 12345from PIL import Image, ImageFilterkitten = Image.open('kitten.jpg')blurryKitten = kitten.filter(ImageFilter.GaussianBlur)blurryKitten.save('kitten_blurred.jpg')blurryKitten.show() 在前面的示例中，图像kitten.jpg将在您的默认图像查看器中打开，其中添加了一个blur，并且还将以更模糊的状态保存为同一目录中的kitten_blur .jpg。您将使用Pillow对图像进行预处理，使其更具机器可读性，但是正如前面提到的，除了这些简单的过滤器应用程序外，您还可以使用库做许多其他事情。要了解更多信息，请查看Pillow文档。 TesseractTesseract是一个OCR库。由谷歌(一家以OCR和机器学习技术闻名的公司)赞助，Tesseract被广泛认为是可用的最好的、最准确的开源OCR系统。除了准确，它还非常灵活。可以训练它识别任意数量的字体(只要这些字体内部相对一致，您很快就会看到)。还可以扩展它来识别任何Unicode字符。本章使用命令行程序Tesseract及其第三方Python包装器pytesseract。两者都将被显式地命名为这两个中的一个，所以要知道，当您看到Tesseract时，我指的是命令行软件，当您看到pytesseract时，我特别指的是它的第三方Python包装器。 在CMD中运行，pip install pytesseract，我的文件会放在D:\Anaconda3\Lib\site-packages\pytesseract中 打开Tesseractwindow下载链接 ，下载最新版本，下载时会有语言选项，按需求来选择语言数据。我下载到了D:\Tesseract-OCR这里，因此，打开D:\Anaconda3\Lib\site-packages\pytesseract\pytesseract.py，如下代码修改一下。 123tesseract_cmd = 'pytesseract.exe'#修改成一下tesseract_cmd = 'D:/Tesseract-OCR/tesseract.exe' 就大功告成了。Pytesseract除了返回如上面的代码示例所示的图像的OCR结果外，还有几个有用的特性。它可以估计框文件(每个字符边界的像素位置): 1print(pytesseract.image_to_boxes(Image.open('files/text_2.png'))) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586T 23 60 34 77 0h 23 60 36 77 0i 39 60 49 77 0s 52 60 67 77 0i 76 60 78 77 0s 80 60 91 73 0s 99 60 109 73 0o 111 60 122 73 0m 125 60 142 73 0e 144 60 156 73 0t 164 56 172 77 0e 164 60 170 77 0x 171 60 182 73 0t 184 60 195 73 0, 196 56 206 77 0w 216 60 229 77 0r 216 60 233 73 0i 235 60 241 73 0t 243 60 252 77 0t 253 60 259 77 0e 260 60 272 73 0n 275 60 285 73 0i 295 60 297 77 0n 300 60 310 73 0A 317 56 329 77 0r 317 60 333 77 0i 335 60 341 73 0a 343 60 345 77 0l 347 60 363 77 0, 367 56 369 62 0t 378 60 384 77 0h 387 60 397 77 0a 399 60 410 73 0t 412 60 418 77 0w 425 60 437 77 0i 425 60 442 73 0l 444 60 451 77 0l 455 60 457 77 0b 467 60 477 77 0e 479 60 491 73 0r 500 60 506 73 0e 507 60 519 73 0a 520 60 532 73 0d 534 60 545 77 0b 555 60 565 77 0y 567 55 578 73 0T 23 31 36 48 0e 35 31 46 44 0s 48 31 59 44 0s 50 31 71 48 0e 60 31 71 44 0r 72 31 84 44 0a 87 31 93 44 0c 94 31 105 44 0t 107 31 118 44 0. 118 31 130 48 0H 140 31 153 48 0e 156 31 168 44 0r 171 31 177 44 0e 178 31 189 44 0a 198 31 209 44 0r 212 31 218 44 0e 219 31 230 44 0s 239 31 249 44 0o 251 31 262 44 0m 265 31 282 44 0e 284 31 296 44 0s 304 31 315 44 0y 316 26 327 44 0m 329 31 346 44 0b 334 26 355 48 0o 349 31 360 48 0l 362 31 373 44 0s 376 31 378 48 0: 380 31 396 44 0! 407 31 410 48 0| 409 26 420 49 0@ 413 26 435 48 0# 436 31 449 48 0$ 451 29 462 49 0% 464 31 482 48 0* 485 39 494 48 0&amp; 496 31 510 48 0* 512 41 519 48 0( 512 26 528 49 0) 522 26 536 48 0 它还可以返回所有数据的完整输出，如置信度评分、页码、行号、框号等信息: 1print(pytesseract.image_to_data(Image.open('files/text_2.png'))) 后两个文件的默认输出是空格或制表符分隔的字符串文件，但也可以作为字典或字节字符串(如果UTF-8解码不够)得到输出: 12345678from PIL import Imageimport pytesseractfrom pytesseract import Outputprint(pytesseract.image_to_data(Image.open('files/textOriginal.png'),output_type=Output.DICT))print('-------------------')print(pytesseract.image_to_string(Image.open('files/textOriginal.png'), output_type=Output.BYTES)) 图片是这样的： 输入结果: 123&#123;&apos;level&apos;: [1, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5, 5, 5, 5, 5, 5], &apos;page_num&apos;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], &apos;block_num&apos;: [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], &apos;par_num&apos;: [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], &apos;line_num&apos;: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2], &apos;word_num&apos;: [0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6], &apos;left&apos;: [0, 23, 23, 23, 23, 76, 99, 164, 215, 295, 317, 378, 425, 467, 500, 555, 23, 23, 140, 198, 239, 304, 407], &apos;top&apos;: [0, 26, 26, 26, 26, 26, 30, 26, 26, 26, 26, 26, 26, 26, 26, 26, 54, 55, 55, 59, 59, 55, 54], &apos;width&apos;: [600, 555, 555, 555, 44, 15, 57, 42, 70, 15, 52, 40, 32, 24, 45, 23, 513, 107, 49, 32, 57, 92, 129], &apos;height&apos;: [103, 51, 51, 22, 17, 17, 13, 21, 17, 17, 21, 17, 17, 17, 17, 22, 23, 17, 17, 13, 13, 22, 23], &apos;conf&apos;: [&apos;-1&apos;, &apos;-1&apos;, &apos;-1&apos;, &apos;-1&apos;, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, &apos;-1&apos;, 96, 96, 96, 96, 90, 64], &apos;text&apos;: [&apos;&apos;, &apos;&apos;, &apos;&apos;, &apos;&apos;, &apos;This&apos;, &apos;is&apos;, &apos;some&apos;, &apos;text,&apos;, &apos;written&apos;, &apos;in&apos;, &apos;Arial,&apos;, &apos;that&apos;, &apos;will&apos;, &apos;be&apos;, &apos;read&apos;, &apos;by&apos;, &apos;&apos;, &apos;Tesseract.&apos;, &apos;Here&apos;, &apos;are&apos;, &apos;some&apos;, &apos;symbols:&apos;, &apos;!|@#$%&amp;*()&apos;]&#125;-------------------b&apos;This is some text, written in Arial, that will be read by\nTesseract. Here are some symbols: !|@#$%&amp;*()\n\x0c&apos; 本章结合使用了pytesseract库和命令行以及通过子进程库从Python触发Tesseract。虽然pytesseract库非常有用和方便，但是它不能执行一些Tesseract函数，所以最好熟悉所有方法。 NumPy虽然对于简单的OCR并不需要NumPy，但是如果您想训练Tesseract识别本章后面介绍的其他字符集或字体，就需要它。在后面的一些代码示例中，您还将使用它来完成简单的数学任务(比如加权平均)。 即使您不打算运行任何使用它的代码示例，我也强烈建议安装它或将它添加到您的Python库中。它使Python的内置数学库更加完善，并具有许多有用的特性，特别是对于数字列表的操作。 1234import numpy as npnumbers = [100, 102, 98, 97, 103]print(np.std(numbers))print(np.mean(numbers)) Processing Well-Formatted Text如果幸运的话，您需要处理的大部分文本将相对干净，格式也很好。格式良好的文本通常满足几个要求，尽管“混乱”和“格式良好”之间的界限可能是主观的。 一般来说，格式良好的文本: 用一种标准字体书写(不包括手写字体、草书字体或过分装饰的字体); 如复制或拍照，线条极清晰，无复制伪影或暗斑; 排列整齐，没有倾斜的字母;和 不运行的图像，也没有切断文字或边缘的图像 有些东西可以在预处理中固定下来。例如，可以将图像转换为灰度，可以调整亮度和对比度，还可以根据需要裁剪和旋转图像。然而，一些基本的限制可能需要更广泛的培训。 您可以从命令行运行Tesseract来读取这个文件，并将结果写入文本文件: 1tesseract text.tif textoutput | cat textoutput.txt 输出是关于Tesseract库的一行信息，表示它正在运行，然后是新创建的textoutput.txt的内容: 123Tesseract Open Source OCR Engine v3.02.02 with LeptonicaThis is some text, written in Arial, that will be read byTesseract. Here are some symbols: !@#$%&quot;&amp;‘() 您可以看到，虽然符号^和*分别被解释为双引号和单引号，但是结果基本上是准确的。不过，总的来说，这可以让你相当舒服地阅读文本。 在模糊图像文本、创建一些JPG压缩工件并添加一个轻微的背景渐变之后，结果会变得更糟 Tesseract无法很好地处理这幅图像，主要是由于背景渐变，产生如下输出: 12This is some text, written In Arlal, that&quot;Tesseract. Here are some symbols: _ 请注意，一旦背景渐变使文本更难区分，文本就会被切断，而且每一行的最后一个字符都是错误的，因为Tesseract徒劳地试图理解它。此外，JPG构件和模糊使得Tesseract很难区分小写i和大写i以及数字1。 这就是使用Python脚本首先清理图像的地方。使用Pillow库，您可以创建一个阈值过滤器来去除背景中的灰色，显示文本，并使图像更清晰，以便Tesseract读取。 1234567891011from PIL import Imageimport pytesseractdef cleanFile(filePath, newFilePath): image = Image.open(filePath) #Set a threshold value for the image, and save image = image.point(lambda x: 0 if x &lt; 143 else 255) image.save(newFilePath) return imageimage = cleanFile('files/textBad.png', 'files/textCleaned.png')#call tesseract to do OCR on the newly created imageprint(pytesseract.image_to_string(image)) 生成的图像，自动创建为textclean.png，如图所示: 除了一些难以辨认或缺少标点符号外，文本至少对我们来说是可读的。Tesseract竭尽全力: 12This us some text‘ written In Anal, that will be read byTesseract Here are some symbols: !@#$%&quot;&amp;&apos;() 句号和逗号非常小，它们是这幅图像争吵的第一个受害者，几乎从我们和Tesseract的视野中消失。还有一个不幸的误解是把Arial误译成Anal，这是Tesseract把r和i解释成单个字符n的结果。 尽管如此，这还是比之前的版本有了改进，在之前的版本中，几乎有一半的文本被剪掉了。Tesseract最大的缺点似乎是背景亮度不同。Tesseract的算法试图在阅读文本之前自动调整图像的对比度，但是您可以使用Pillow library这样的工具自己完成这项工作，从而获得更好的结果。 在提交到Tesseract之前，您一定要修复那些倾斜的、大面积非文本的或有其他问题的图像。 Adjusting Images Automatically在前面的例子中，为了让Tesseract读取图像，实验中选择143作为理想阈值，将所有图像像素调整为黑色或白色。但是，如果您有许多图像，所有的图像都有稍微不同的灰度问题，并且无法合理地手动调整所有这些图像，情况会怎样呢?找到最好的解决方案的一种方法(或者至少是一个很好的一个)是运行超正方体对一系列图像调整到不同的价值观和算法选择一个最好的结果,以某种组合的字符串的字符数和/或超正方体可以阅读,和信心,它读取这些字符。 确切地说，您使用的算法可能会因应用程序的不同而略有不同，但这是迭代图像处理阈值以找到“最佳”设置的一个例子: 12345678910111213141516171819202122232425262728293031323334import pytesseractfrom pytesseract import Outputfrom PIL import Imageimport numpy as npdef cleanFile(filePath, threshold): image = Image.open(filePath) #Set a threshold value for the image, and save image = image.point(lambda x: 0 if x&lt;threshold else 255) return imagedef getConfidence(image): data = pytesseract.image_to_data(image, output_type=Output.DICT) text = data['text'] confidences = [] numChars = [] for i in range(len(text)): if int(data['conf'][i]) &gt; -1: confidences.append(data['conf'][i]) numChars.append(len(text[i])) return np.average(confidences, weights=numChars), sum(numChars) filePath = 'files/textBad.png'start = 80step = 5end = 200for threshold in range(start, end, step): image = cleanFile(filePath, threshold) scores = getConfidence(image) print('threshold: &#123;&#125;, confidence: &#123;&#125;, numChars '.format(str(threshold), str(scores[0]), str(scores[1]))) 这个脚本有两个功能: cleanFile 接受一个原始的“坏”文件和一个阈值变量来运行PIL阈值工具。它处理文件并返回PIL图像对象。 getConfidence 接受清洗后的PIL图像对象，并通过Tesseract运行它。它计算每个已识别字符串的平均置信度(由该字符串中的字符数加权)，以及已识别字符的数量。 通过改变阈值，得到每个值的置信度和识别字符的个数，得到输出: 123456789101112131415161718192021222324threshold: 80, confidence: 61.8333333333 numChars 18threshold: 85, confidence: 64.9130434783 numChars 23threshold: 90, confidence: 62.2564102564 numChars 39threshold: 95, confidence: 64.5135135135 numChars 37threshold: 100, confidence: 60.7878787879 numChars 66threshold: 105, confidence: 61.9078947368 numChars 76threshold: 110, confidence: 64.6329113924 numChars 79threshold: 115, confidence: 69.7397260274 numChars 73threshold: 120, confidence: 72.9078947368 numChars 76threshold: 125, confidence: 73.582278481 numChars 79threshold: 130, confidence: 75.6708860759 numChars 79threshold: 135, confidence: 76.8292682927 numChars 82threshold: 140, confidence: 72.1686746988 numChars 83threshold: 145, confidence: 75.5662650602 numChars 83threshold: 150, confidence: 77.5443037975 numChars 79threshold: 155, confidence: 79.1066666667 numChars 75threshold: 160, confidence: 78.4666666667 numChars 75threshold: 165, confidence: 80.1428571429 numChars 70threshold: 170, confidence: 78.4285714286 numChars 70threshold: 175, confidence: 76.3731343284 numChars 67threshold: 180, confidence: 76.7575757576 numChars 66threshold: 185, confidence: 79.4920634921 numChars 63threshold: 190, confidence: 76.0793650794 numChars 63threshold: 195, confidence: 70.6153846154 numChars 65 无论是对结果的平均信心，还是识别的字符数，都有一个明显的趋势。两者都趋向于在145的阈值附近达到峰值，这接近手动找到的143的“理想”结果。 阈值为140和145都给出了可识别字符的最大数量(83)，但是对于那些找到的字符，阈值145给出了最高的可信度，因此您可能希望使用该结果并返回在该阈值下识别的文本，该文本是图像包含哪些文本的“最佳猜测”。 当然，仅仅找到“大多数”字符并不一定意味着所有这些字符都是真实的。在某些阈值下，Tesseract可以将单个字符分割成多个字符，或者将图像中的随机噪声解释为实际上不存在的文本字符。在这种情况下，您可能希望更多地依赖于每个得分的平均信心。 例如，如果您发现以下结果(部分): 12threshold: 145, confidence: 75.5662650602 numChars 83threshold: 150, confidence: 97.1234567890 numChars 82 如果只丢失一个字符，您的信心就增加了20%以上，并且假设阈值为145的结果是不正确的，或者可能分割了一个字符，或者发现了一些不存在的东西，那么您很可能会毫不犹豫地接受这个结果。 这是一些预先的实验，以完善您的阈值选择算法可能会派上用场。例如,你可能想要选择的分数的乘积的信心和字符的数量最大化(在这种情况下,145仍然赢得了6272的产物,和在我们的虚构的示例中,阈值150会赢的产品7964)和其他指标。注意，这种类型的选择算法也适用于任意的PIL工具值，而不仅仅是阀值。此外，您还可以使用它来选择两个或多个值，方法是改变每个值的值，并以类似的方式选择最佳结果得分。显然，这种类型的选择算法是计算密集型的。你在每张图片上运行PIL和Tesseract很多次，但是如果你提前知道理想的阈值，你只需要运行一次。 请记住，当您开始处理正在处理的图像时，您可能会注意到理想值中的模式。实际上，您可能只需要尝试130到180之间的阈值，而不是尝试80到200之间的每个阈值。您甚至可以采用另一种方法，选择阈值，例如，第一次迭代间隔20个阈值，然后使用贪婪算法，通过减小在前一次迭代中找到的最佳解决方案之间阈值的步长，来获得最佳结果。当您处理多个变量时，这也可能是最有效的。 Scraping Text from Images on Websites使用Tesseract从硬盘上的图像中读取文本似乎并不那么令人兴奋，但是当与web scraper一起使用时，它可以成为一个强大的工具。图像可能会无意中混淆网站上的文本(与本地餐馆网站上菜单的JPG副本一样)，但它们也可能有意地隐藏文本，在下一个示例中我将展示这一点。尽管Amazon的robots.txt文件允许抓取站点的产品页面，但通常不会被路过的机器人获取图书预览。这是因为图书预览是通过用户触发的Ajax脚本加载的，图像被小心地隐藏在div层之下。对于一般的网站访问者来说，它们可能更像Flash演示而不是图像文件。当然，即使你能看到这些图片，作为文本来阅读也不是一件小事。下面的脚本完成了这一壮举:它导航到托尔斯泰《伊凡·伊里奇之死》的大字印刷版本，打开阅读器，收集图像url，然后系统地下载、阅读和打印每个url的文本。请注意，这段代码依赖于Amazon清单以及Amazon网站的几个架构特性才能正确运行。如果该列表下降或被替换，请使用预览功能免费替换另一本书的URL(我发现大字体、无衬线字体工作得很好)。 因为这是一个相对复杂的代码，它借鉴了前几章的多个概念，所以我在整个代码中添加了注释，以便更容易理解正在发生的事情: 12345678910111213141516171819202122232425262728293031323334353637383940import timefrom urllib.request import urlretrievefrom PIL import Imageimport pytesseractfrom selenium import webdriverfrom PIL import Image# Create new Selenium driverdriver = webdriver.Chrome(executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver')driver.get( 'https://www.amazon.com/Death-Ivan-Ilyich-Nikolayevich-Tolstoy/dp/1427027277')time.sleep(2)# Click on the book preview buttondriver.find_element_by_id('imgBlkFront').click()imageList = []# Wait for the page to loadtime.sleep(5)while 'pointer' in driver.find_element_by_id('sitbReaderRightPageTurner').get_attribute('style'): # While the right arrow is available for clicking, turn through pages driver.find_element_by_id('sitbReaderRightPageTurner').click() time.sleep(2) # Get any new pages that have loaded (multiple pages can load at once, # but duplicates will not be added to a set) pages = driver.find_elements_by_xpath( '//div[@class=\'pageImage\']/div/img') if not len(pages): print('No pages found') for page in pages: image = page.get_attribute('src') print('Found image: &#123;&#125;'.format(image)) if image not in imageList: urlretrieve(image, 'page.jpg') imageList.append(image) print(pytesseract.image_to_string(Image.open('page.jpg')))driver.quit() 虽然这个脚本理论上可以在任何类型的Selenium webdriver上运行，但我发现它目前在Chrome上运行得最可靠。正如您以前使用Tesseract阅读器时所经历的那样，它打印了本书的许多长段落，大部分都很清晰，如第一章的预览所示 123456789101112Chapter IDuring an Interval In the Melvmskl trial In the largebuilding of the Law Courts the members and publicprosecutor met in [van Egorowch Shebek‘s privateroom, where the conversation turned on the celebratedKrasovski case. Fedor Vasillevich warmly maintainedthat it was not subject to their jurisdiction, IvanEgorovich maintained the contrary, while Peterivanowch, not havmg entered into the discussmn atthe start, took no part in it but looked through theGazette which had Just been handed in.“Gentlemen,” he said, “Ivan Ilych has died!&quot; 然而，很多单词都有明显的错误，比如“Melvmsl”而不是“Melvinski”，“discussmn”而不是“discussion”。许多这类错误都可以通过根据字典上的单词列表进行猜测来纠正(也许还可以根据相关专有名词如“Melvinski”进行添加)。 偶尔一个错误可能跨越整个单词，如在文本的第3页: 1it is he who is dead and not 1. 在这种情况下，单词“I”被字符“1”替换。“除了单词字典，马尔科夫链分析在这里可能很有用。如果文本的任何部分包含一个非常不常见的短语(“and not 1”)，那么可以假定文本实际上是更常见的短语(“and not I”)。 当然，这些字符替换遵循可预测的模式是有帮助的:vi变成w, I变成1。如果这些替换在您的文本中频繁发生，您可能会创建一个列表，用于尝试新单词和短语，选择最有意义的解决方案。一种方法可能是替换经常混淆的字符，并使用与字典中的单词匹配的解决方案，或者使用可识别(或最常见)的n-gram。如果您采用这种方法，请务必阅读第9章，以获得更多关于文本处理和自然语言处理的信息。虽然本例中的文本是一种常见的sans-serif字体，并且Tesseract应该能够相对轻松地识别它，但是有时进行一些再培训也有助于提高准确性。下一节将讨论另一种方法，使用少量的前期投资来解决文本混乱的问题。通过向Tesseract提供大量具有已知值的文本图像集合，可以教会Tesseract在未来以更高的精度和准确度识别相同的字体，即使文本中偶尔会出现背景和定位问题。 Reading CAPTCHAs and Training Tesseract尽管大多数人都熟悉CAPTCHA这个词，但是很少有人知道它代表什么:(Completely Automated Public Turing Test to Tell Computers and Humans Apart)完全自动化的公共图灵测试来告诉计算机和人类分开。它笨拙的首字母缩略词暗示了它在阻碍完全可用的web界面方面所扮演的相当笨拙的角色，因为人类和非人类机器人常常难以解决CAPTCHA测试。 具有讽刺意味的是，在过去的60年里，我们从使用这些测试来测试机器到使用它们来测试我们自己，结果好坏参半。谷歌最近关闭了他们臭名昭著的困难的reCAPTCHA，这在很大程度上是因为它倾向于屏蔽合法的网站用户。其他大多数验证码都比较简单。例如，Drupal是一个流行的基于php的内容管理系统，它有一个流行的CAPTCHA模块，可以生成不同难度的CAPTCHA图像。默认图像如图所示。 与其他验证码相比，是什么让这个验证码对人类和机器来说如此容易阅读? 字符之间不会重叠，也不会横向交叉到彼此的空间中。也就是说，可以在每个字符周围绘制一个整洁的矩形，而不重叠任何其他字符。 没有任何背景图像、线条或其他干扰OCR程序的垃圾。 从这张图中并不明显，但是在字体上有一些变化验证码使用。它在干净的无衬线字体之间交替使用(如字符中所示)“4”和“M”)以及手写字体(如字符“M”所示，“C”和“3”)。 白色的背景与深色的文字形成了强烈的对比。 不过，这个验证码确实抛出了一些曲线，这使得OCR程序阅读起来很有挑战性: 同时使用字母和数字，增加了潜在字符的数量。 字母的随机倾斜可能会让OCR软件感到困惑，但对人类来说仍然很容易阅读。 相对奇怪的手写字体带来了特殊的挑战，“C”和“3”多了几行，小写字母“m”也小得不寻常，需要额外的训练才能让电脑熟悉。 当您使用命令在此映像上运行Tesseract时 1tesseract captchaExample.png output 你得到这个output.txt文件: 14N\,,,C&lt;3 它得到了正确的4、C和3，但是它显然不能很快填满CAPTCHAprotected字段。 Training Tesseract为了训练Tesseract识别文字，无论是晦涩难读的字体还是验证码，你需要给每个字符提供多个例子。这部分你可能会想要排一个好的播客或电影，因为这将是几个小时相当无聊的工作。第一步是将验证码的多个示例下载到一个目录中。编译的示例数量将取决于验证码的复杂性;我在CAPTCHA培训中使用了100个样本文件(总共500个字符，平均每个符号大约有8个示例)，这似乎非常有效。 我建议用它所代表的CAPTCHA解决方案(例如，4MmC3.jpg)来命名图像。我发现这有助于在大量文件之间进行快速的错误检查;您可以以缩略图的形式查看所有文件，并轻松地将图像与其图像名称进行比较。这也大大有助于后续步骤中的错误检查。 第二步是准确地告诉Tesseract每个字符是什么以及它在图像中的位置。这包括为每个CAPTCHA映像创建一个框文件。一个框文件是这样的: 123454 15 26 33 55 0M 38 13 67 45 0m 79 15 101 26 0C 111 33 136 60 03 147 17 176 45 0 第一个符号是所表示的字符，接下来的四个数字表示勾勒出图像的矩形框的坐标，最后一个数字是用于使用多页文档进行培训的页码(对于我们来说是0)。 显然，手工创建这些box文件并不有趣，但是各种工具可以帮助您解决这个问题。我喜欢在线工具Tesseract OCR Chopper，因为它不需要安装或额外的库，可以在任何有浏览器的机器上运行，而且相对容易使用。上传图片，如果需要额外的框，单击底部的Add按钮，如果需要，调整框的大小，并将新的.box文件文本复制粘贴到一个新文件中。 框文件必须以纯文本形式保存，扩展名为. Box。与图像文件一样，根据它们所代表的CAPTCHA解决方案(例如，4mmc .box)来命名框文件也很方便。同样，这使得根据文件名对.box文件文本的内容进行双重检查变得很容易，如果按照文件名对数据目录中的所有文件进行排序，则再次针对与之匹配的图像文件进行检查。同样，您需要创建大约100个这样的文件，以确保您拥有足够的数据。此外，Tesseract有时会因为不可读而丢弃文件，所以您可能希望在此之上有一些缓冲空间。如果你发现你的OCR结果不像你想的那么好，或者Tesseract被某些字符绊倒，这是一个很好的调试步骤，创建额外的训练数据，然后再试一次。 在创建一个充满.box文件和图像文件的数据文件夹之后，将该数据复制到备份文件夹中，然后再对其进行任何操作。尽管在数据上运行培训脚本不太可能删除任何内容，但是如果要在.box文件的创建中花费数小时的时间，那么这比后悔要安全得多。此外，能够删除一个充满已编译数据的杂乱目录，然后重试也是一件好事。 执行所有数据分析和创建Tesseract所需的培训文件需要六个步骤。有一些工具可以为您提供相应的源映像和.box文件，但遗憾的是，目前还没有针对Tesseract 3.02的工具。我用Python编写了一个解决方案，它对一个包含图像和框文件的文件进行操作，并自动创建所有必要的培训文件。 这个程序的初始设置和步骤可以在类的_init__和runAll方法中看到: 12345678910111213def __init__(self): languageName = 'eng' fontName = 'captchaFont' directory = '&lt;path to images&gt;'def runAll(self): self.createFontFile() self.cleanImages() self.renameFiles() self.extractUnicode() self.runShapeClustering() self.runMfTraining() self.runCnTraining() self.createTessData() 这里只需要设置三个变量，它们非常简单: languageName Tesseract使用三个字母的语言代码来理解它所查看的语言。在大多数情况下，您可能希望使用英语的eng。 fontName 所选字体的名称。这可以是任何东西，但必须是一个没有空格的单词。 directory 包含所有映像和框文件的目录。我建议您将此路径设置为绝对路径，但如果使用相对路径，则需要将其设置为运行Python代码的位置的相对路径。如果它是绝对的，则可以在机器上的任何位置运行代码。 让我们看一下使用的各个函数。 createFontFile创建一个必要的文件，font_properties，让Tesseract知道您正在创建的新字体: 1captchaFont 0 0 0 0 0 该文件由字体名称组成，后跟1和0，表示是否应该考虑斜体、粗体或其他版本的字体。(使用这些属性训练字体是一项有趣的练习，但不幸超出了本书的范围。) cleanImages 创建所有找到的图像文件的高对比度版本，将它们转换为灰度，并执行其他操作，使OCR程序更容易读取图像文件。如果您正在处理CAPTCHA图像中的视觉垃圾，这些垃圾在后期处理中可能很容易过滤掉，那么可以在这里添加额外的处理。 renameFiles 用Tesseract要求的名称重命名所有.box文件及其对应的图像文件(这里的文件编号是连续的数字，以便将多个文件分开): &lt;languageName&gt;.&lt;fontName&gt;.exp&lt;fileNumber&gt;.box &lt;languageName&gt;.&lt;fontName&gt;.exp&lt;fileNumber&gt;.tiff extractUnicode查看所有创建的.box文件，并确定要训练的可用字符的总数。生成的Unicode文件将告诉您找到了多少个不同的字符，这可能是快速查看是否遗漏了什么内容的好方法。 接下来的三个函数runshapeclu、runMfTraining和runCtTraining分别创建文件shapetable、pfftable和normproto。这些都提供了关于每个字符的几何形状和形状的信息，还提供了统计信息，Tesseract使用这些信息来计算给定字符是一种或另一种类型的概率。 最后，Tesseract对每个编译后的数据文件夹重新命名，以所需的语言名称作为前缀(例如，shapetable被重命名为eng.shapetable)，并将所有这些文件编译成最终的培训数据文件eng.traineddata。 这一小节我是真没看懂。。。代码也没跑通，放弃了 Retrieving CAPTCHAs and Submitting Solutions许多流行的内容管理系统经常被预先编程为具有这些用户注册页面的已知位置的机器人的注册发送垃圾邮件。例如，在http://pythonscraping.com网站上，即使是验证码(公认的弱验证码)也无法抑制注册的涌入。那么这些机器人是怎么做到的呢?我们已经成功地解决了硬盘上的图像验证码问题，但是我们如何才能制造出一个功能齐全的机器人呢?本节结合了前几章中介绍的许多技术。如果你还没有读过，你至少应该浏览一下第十章。 大多数基于图像的验证码有以下几个特性: 它们是由服务器端程序动态生成的图像。它们可能具有与传统图像不同的图像源，比如&lt;img src=&quot;WebForm.aspx? &quot;id=8AP85CQKE9TJ&quot;&gt;，但可以像任何其他图像一样下载和操作。 图像的解决方案存储在服务器端数据库中。 如果你花了太长时间来解决验证码，很多验证码都会失效。这对于机器人来说通常不是问题，但是排队验证码解决方案供以后使用，或者其他可能会延迟验证码被请求到提交解决方案之间的时间的实践，可能不会成功。 实现此目的的一般方法是将CAPTCHA图像文件下载到您的硬盘驱动器，清除它，使用Tesseract解析图像，并在适当的表单参数下返回解决方案。 作者在http://pythonscraping.com/humans-only上创建了一个页面，其中只有CAPTCHAprotected注释表单，用于编写一个要击败的机器人。这个机器人使用命令行Tesseract库，而不是pytesseract包装器(尽管它也可以很容易地使用)，看起来像这样: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSoupimport subprocessimport requestsfrom PIL import Imagefrom PIL import ImageOpsdef cleanImage(imagePath): image = Image.open(imagePath) image = image.point(lambda x: 0 if x&lt;143 else 255) borderImage = ImageOps.expand(image,border=20,fill='white') borderImage.save(imagePath)html = urlopen('http://www.pythonscraping.com/humans-only')bs = BeautifulSoup(html, 'html.parser')#Gather prepopulated form valuesimageLocation = bs.find('img', &#123;'title': 'Image CAPTCHA'&#125;)['src']formBuildId = bs.find('input', &#123;'name':'form_build_id'&#125;)['value']captchaSid = bs.find('input', &#123;'name':'captcha_sid'&#125;)['value']captchaToken = bs.find('input', &#123;'name':'captcha_token'&#125;)['value']captchaUrl = 'http://pythonscraping.com'+imageLocationurlretrieve(captchaUrl, 'captcha.jpg')cleanImage('captcha.jpg')p = subprocess.Popen(['tesseract', 'captcha.jpg', 'captcha'], stdout= subprocess.PIPE,stderr=subprocess.PIPE)p.wait()f = open('captcha.txt', 'r')#Clean any whitespace characterscaptchaResponse = f.read().replace(' ', '').replace('\n', '')print('Captcha solution attempt: '+captchaResponse)if len(captchaResponse) == 5: params = &#123;'captcha_token':captchaToken, 'captcha_sid':captchaSid, 'form_id':'comment_node_page_form', 'form_build_id': formBuildId, 'captcha_response':captchaResponse, 'name':'Ryan Mitchell', 'subject': 'I come to seek the Grail', 'comment_body[und][0][value]': '...and I am definitely not a bot'&#125; r = requests.post('http://www.pythonscraping.com/comment/reply/10', data=params) responseObj = BeautifulSoup(r.text, 'html.parser') if responseObj.find('div', &#123;'class':'messages'&#125;) is not None: print(responseObj.find('div', &#123;'class':'messages'&#125;).get_text())else: print('There was a problem reading the CAPTCHA correctly!') 输出为： 12Captcha solution attempt: EM3;y楼There was a problem reading the CAPTCHA correctly! 注意，这个脚本在两种情况下失败:如果Tesseract没有从图像中提取恰好五个字符(因为您知道这个CAPTCHA的所有有效解决方案都必须有五个字符)，或者它提交了表单，但是CAPTCHA被错误地解决了。第一种情况发生的概率大约为50%，此时它不需要提交表单，并且会出现错误消息。第二种情况发生的概率约为20%，总准确率约为30%(或遇到的每个字符的准确率约为80%，超过5个字符)。 尽管这看起来很低，但请记住，通常不限制用户尝试验证码的次数，而且大多数不正确的尝试都可以在不需要实际发送表单的情况下终止。当发送表单时，验证码在大多数情况下是准确的。如果这还不能使您信服，还请记住，简单的猜测将使您的准确率达到0.00001%。运行一个程序三到四次而不是猜测9亿次是相当节省时间的检索!]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-12]]></title>
    <url>%2F2019%2F08%2F02%2FWebScraping12%2F</url>
    <content type="text"><![CDATA[JavaScript一直以来都是web爬行器的祸根。在internet的古老历史上，您可以保证向web服务器发出的请求将获取与用户发出相同请求时在web浏览器中看到的相同的数据。随着JavaScript和Ajax内容生成和加载变得越来越普遍，这种情况变得越来越不常见。在第11章中，您了解了解决这个问题的一种方法:使用Selenium自动化浏览器并获取数据。这是一件容易的事。它几乎一直都在工作。问题是，当您有一个像Selenium一样强大和有效的锤子时，每个web刮削问题看起来都很像钉子。在本章中，您将看到完全删除JavaScript(不需要执行甚至加载它!)，直接进入数据的来源:生成它的API。 A Brief Introduction to APIs尽管有无数的书籍、演讲和指南都写过关于REST、GraphQL、JSON和XML api的复杂之处，他们的核心都基于一个简单的概念。API定义了一种标准化的语法，允许软件的一个部分与另一个部分通信，即使它们可能是用不同的语言编写的，或者以其他不同的结构编写的。 本节重点介绍web API(特别是允许web服务器与浏览器通信的API)，并使用术语API专门指代这种类型。但是您可能要记住，在其他上下文中，API也是一个通用术语，例如，它允许Java程序与运行在同一台机器上的Python程序通信。API并不总是“跨internet”的，也不一定涉及任何web技术。 Web api通常由使用经过良好宣传和文档化的公共服务的开发人员使用。例如，ESPN为运动员信息、比赛分数等提供api。谷歌在其开发人员部分中有几十个api，用于语言翻译、分析和地理定位。 例如，下面提供pathparam作为路由路径中的参数： 1http://example.com/the-api-route/pathparam 这提供了pathparam作为参数param1的值： 1http://example.com/the-api-route?param1=pathparam 这两种将变量数据传递给API的方法都经常被使用，尽管像计算机科学中的许多主题一样，关于变量应该在何时何地通过路径或参数传递的哲学争论也很激烈。 来自API的响应通常以JSON或XML格式返回。JSON在现代比XML流行得多，但是您可能仍然会看到一些XML响应。许多api允许您更改响应类型，通常使用另一个参数来定义您想要的响应类型。 下面是一个json格式的API响应示例: 1&#123;"user":&#123;"id": 123, "name": "Ryan Mitchell", "city": "Boston"&#125;&#125; 下面是一个xml格式的API响应示例: 1&lt;user&gt;&lt;id&gt;123&lt;/id&gt;&lt;name&gt;Ryan Mitchell&lt;/name&gt;&lt;city&gt;Boston&lt;/city&gt;&lt;/user&gt; HTTP Methods and APIs在前一节中，您了解了向服务器发出GET请求以获取信息的api。通过HTTP从web服务器请求信息有四种主要方法: GET POST PUT DELETE 从技术上讲，这四个选项不止一个(如HEAD、OPTIONS和CONNECT)，但是它们很少在api中使用，您不太可能看到它们。绝大多数api都将自己局限于这四个方法，甚至是这四个方法的子集。通常可以看到只使用GET或只使用GET和POST的api。 GET是通过浏览器中的地址栏访问网站时使用的工具。GET是在调用http://freegeoip.net/json /50.78.253.58时使用的方法。您可以将GET看作是这样说的:“嘿，web服务器，请检索/获取此信息。”，注意：此写法已经失效。详细见下方代码。 根据定义，GET请求不更改服务器数据库中的信息。没有什么是存储;没有修改。信息只被读取。 POST是您在填写表单或提交信息时使用的工具，这些信息可能是提供给服务器上的后端脚本的。每次你登录一个网站，你都在用你的用户名和(希望是)加密密码发出一个帖子请求。如果您使用API发出POST请求，您的意思是“请将此信息存储在数据库中”。 PUT在与网站交互时不太常用，但有时在api中使用。PUT请求用于更新对象或信息。例如，一个API可能需要一个POST请求来创建一个新用户，但是如果您想要更新该用户的电子邮件地址，它可能需要一个PUT请求。 DELETE用于删除对象。例如，如果将DELETE请求发送到http://myapi.com/user/23，它将删除具有ID的用户在公共api中不经常遇到DELETE方法，公共api主要用于传播信息或允许用户创建或发布信息，而不是允许用户从数据库中删除这些信息。 与GET请求不同，POST、PUT和DELETE请求允许您在请求体中发送信息，除了您请求数据的URL或路由之外。 就像您从web服务器接收到的响应一样，主体中的数据通常被格式化为JSON，或者不太常见的XML格式，并且该数据的格式由API的语法定义。例如，如果您正在使用在博客文章上创建评论的API，您可能会向其发出PUT请求: 1http://example.com/comments?post=123 request body 如下： 1234&#123;"title": "Great post about APIs!", "body": "Very informative. Really helped meout with a tricky technical challenge I was facing. Thanks for taking the timeto write such a detailed blog post about PUT requests!", "author": &#123;"name": "RyanMitchell", "website": "http://pythonscraping.com", "company": "O'Reilly Media"&#125;&#125; 注意，blog post(123)的ID作为URL中的一个参数传递，您正在创建的新评论的内容将在请求体中传递。参数和数据可以同时在参数和主体中传递。需要哪些参数以及传递这些参数的位置同样由API的语法决定。 Parsing JSON在本章中，您已经了解了各种类型的api及其功能，并了解了来自这些api的JSON响应示例。现在让我们看看如何解析和使用这些信息。 在本章的开头，您看到了freegeoip.net IP的示例，它将IP地址解析为物理地址 注意：此域名已经修改成 1http://api.ipstack.com/134.201.250.155?access_key = YOUR_ACCESS_KEY 其中YOUR_ACCESS_KEY是注册后可获得的准许码。 您可以获取该请求的输出，并使用Python的json解析函数对其进行解码: 12345678import jsonfrom urllib.request import urlopenaccess_key = "your access key"def getCountry(ipAddress): response = urlopen('http://api.ipstack.com/'+ipAddress+'?access_key=' + access_key).read().decode('utf-8') responseJson = json.loads(response) return responseJson.get('country_code')print(getCountry('50.78.253.58')) 输出为：US 使用的JSON解析库是Python核心库的一部分。只要在顶部输入import json，一切就都设置好了!与许多可能将JSON解析为特殊JSON对象或JSON节点的语言不同，Python使用了一种更灵活的方法将JSON对象放入字典，将JSON数组放入列表，将JSON字符串放入字符串，等等。通过这种方式，访问和操作存储在其中的值非常容易。 下面快速演示了Python的JSON库如何处理JSON字符串中可能遇到的值: 12345678910import jsonjsonString = '&#123;"arrayOfNums":[&#123;"number":0&#125;,&#123;"number":1&#125;,&#123;"number":2&#125;],"arrayOfFruits":[&#123;"fruit":"apple"&#125;,&#123;"fruit":"banana"&#125;,&#123;"fruit":"pear"&#125;]&#125;'jsonObj = json.loads(jsonString)print(jsonObj.get('arrayOfNums'))print(jsonObj.get('arrayOfNums')[1])print(jsonObj.get('arrayOfNums')[1].get('number') +jsonObj.get('arrayOfNums')[2].get('number'))print(jsonObj.get('arrayOfFruits')[2].get('fruit')) 输出如下： 1234[&#123;'number': 0&#125;, &#123;'number': 1&#125;, &#123;'number': 2&#125;]&#123;'number': 1&#125;3pear Undocumented APIs到目前为止，在本章中，我们只讨论了文档化的api。它们的开发人员希望它们被公众使用，发布关于它们的信息，并假设api将被其他开发人员使用。但是绝大多数api根本没有任何已发布的文档。但是为什么要创建一个没有任何公共文档的API呢?正如本章开头所提到的，这一切都与JavaScript有关。 传统上，当用户请求页面时，动态网站的web服务器有几个任务: 处理用户请求网站页面的GET请求 从出现在该页面的数据库中检索数据 将数据格式化为页面的HTML模板 将格式化的HTML发送给用户 随着JavaScript框架变得越来越普遍，由服务器处理的许多HTML创建任务转移到了浏览器中。服务器可能发送硬编码将HTML模板加载到用户的浏览器中，但是会发出单独的Ajax请求来加载内容并将其放入该HTML模板中的正确位置。所有这些都将发生在浏览器/客户机端。 这最初是web抓取器的一个问题。他们习惯于向HTML页面发出请求，然后返回一个包含所有内容的HTML页面。相反，他们现在得到了一个没有任何内容的HTML模板。Selenium被用来解决这个问题。现在，程序员的web scraper可以成为浏览器，请求HTML模板，执行任何JavaScript，允许所有数据加载到它的位置，然后再从页面中抓取数据。由于HTML都已加载完毕，因此它本质上简化为一个以前解决过的问题，即解析和格式化现有HTML的问题。 然而，由于整个内容管理系统(过去只驻留在web服务器中)已经从本质上转移到浏览器客户机，即使是最简单的网站也可能膨胀为几兆字节的内容和十几个HTTP请求。 此外，当使用Selenium时，用户不必关心的所有“附加功能”都会加载。调用跟踪程序，加载侧边栏广告，调用跟踪程序为侧边栏广告。图像，CSS，第三方字体数据-所有这些都需要加载。这似乎好当你使用浏览器浏览网页的时候,但是,如果您正在编写一个web刮刀需要快速行动,收集具体数据,并将尽可能少的web服务器上的负载,可以装载一百倍比你需要的数据。 但是，所有这些JavaScript、Ajax和web现代化都有一线希望:因为服务器不再将数据格式化为HTML，所以它们常常充当数据库本身的瘦包装器。这个薄薄的包装器只是从数据库中提取数据，然后通过API将数据返回给页面。 当然，除了网页本身，任何人或任何东西都不打算使用这些api，因此开发人员将它们放在文档中，并假设(或希望)没有人会注意到它们。但它们确实存在。 例如，纽约时报网站通过JSON加载所有搜索结果。如果你访问链接： 1https://query.nytimes.com/search/sitesearch/#/python 这将显示搜索词“python”的最新新闻文章。“如果你使用urllib或请求库来抓取这个页面，你将不会找到任何搜索结果。它们分别通过API调用加载: 12https://query.nytimes.com/svc/add/v1/sitesearch.json?q=python&amp;spotlight=true&amp;facet=true 如果您要用Selenium加载这个页面，那么您将发出大约100个请求，并在每次搜索中传输600 - 700kb的数据。直接使用API，您只发出一个请求，并且只传输大约60kb的格式化良好的数据。 在前几章中，您已经使用了Chrome检查器来检查HTML页面内容，但是现在您将把它用于一个稍微不同的目的:检查用于构造该页面的调用的请求和响应。 为此，打开Chrome inspector窗口并单击Network选项卡: 注意，您需要在加载页面之前打开此窗口。它在关闭时不跟踪网络呼叫。当页面加载时，每当浏览器回调web服务器以获取呈现页面的附加信息时，您将看到实时出现一行。这可能包括一个API调用。 寻找未文档化的api可能需要一些侦探工作(要完成侦探工作，一般来说，当你看到它的时候你就知道了。 API调用往往有几个特性，这些特性对于在网络调用列表中定位它们很有用: 它们通常包含JSON或XML。您可以使用search/filter字段过滤请求列表。 对于GET请求，URL将包含传递给它们的参数值。例如，如果您正在寻找返回搜索结果的API调用，或者正在加载特定页面的数据，那么这将非常有用。只需使用您使用的搜索词、页面ID或其他标识信息过滤结果。 它们通常是XHR类型。 Documenting Undocumented APIsapi可能并不总是显而易见的，特别是在具有许多特性的大型站点中，这些特性可能在加载单个页面时进行数百次调用。然而，只要稍加练习，就能更容易地在大海捞针。 在您发现正在进行API调用之后，在某种程度上对它进行文档化通常是很有用的，特别是当您的抓取器严重依赖于该调用时。您可能希望在网站上加载几个页面，在inspector console network选项卡中过滤目标API调用。通过这样做，您可以看到调用如何在页与页之间更改，并标识它接受和返回的字段。 每个API调用都可以通过注意以下字段来识别和记录: HTTP method used Inputs Path parameters Headers(including cookies) Body content (for PUT and POST calls) Outputs Response headers (including cookies set) Response body type Response body fields Finding and Documenting APIs Automatically定位和记录api的工作看起来有些单调和算法化。这主要是因为事实就是如此。虽然一些网站可能会试图混淆浏览器获取数据的方式，这让任务变得有点棘手，但寻找和记录api基本上是一个程序化的任务。 书的作者在https://github.com/chell/apiscraper创建了一个GitHub存储库，它尝试从这个任务中去掉一些繁重的工作。 它使用Selenium、ChromeDriver和一个名为BrowserMob Proxy的库来加载页面、抓取域中的页面、分析页面加载期间发生的网络流量，并将这些请求组织成可读的API调用。 apicall.py 包含定义API调用(路径、参数等)的属性，以及决定两个API调用是否相同的逻辑。 apiFinder.py 主要的爬行class。py和consoleservice.py用于启动查找api的过程。 browser.py 只有三种方法——initialize、get和close——但是包含相对复杂的功能，可以将BrowserMob代理服务器和硒。滚动整个页面，以确保整个页面已加载、保存HTTP Archive (HAR)文件到适当的位置进行处理。 consoleservice.py 处理来自控制台的命令并启动主APIFinder类。 harParser.py 解析HAR文件并提取API调用。 html_template.html 提供用于在浏览器中显示API调用的模板。 README.md Git自述文件页面。 从https://bmp.lightbody.net/下载BrowserMob代理二进制文件，并将提取的文件放在apiscraper项目目录中。 在撰写本文时，BrowserMob代理的当前版本是2.1.4，因此该脚本将假定二进制文件位于相对于根项目目录的browsermob-proxy- Proxy -2.1.4/bin/browsermob-proxy。如果不是这样，您可以在运行时提供一个不同的目录，或者(可能更容易)修改apiFinder.py中的代码。 下载ChromeDriver并将其放在apiscraper项目目录中。 你需要安装以下Python库: tldextract selebium browsermob-proxy 设置完成后，就可以开始收集API调用了。打字: 1python consoleservice.py -h 会给你一个列表的选择开始: usage: consoleService.py [-h] [-u [U]] [-d [D]] [-s [S]] [-c [C]] [—p] optional arguments: -h, —help show this help message and exit -u [U] Target URL. If not provided, target directory will be scanned for har files. -d [D] Target directory (default is “hars”). If URL is provided, directory will store har files. If URL is not provided, directory will be scanned. -s [S] Search term -c [C] Count of pages to crawl (with target URL only) —p Flag, remove unnecessary parameters (may dramatically increase run time) 您可以为单个搜索项在单个页面上搜索API调用。例如，您可以在http://target.com上搜索一个返回产品数据的API页面，以填充产品页面: 1$ python consoleservice.py -u https://www.target.com/p/rogue-one-a-star-wars-\story-blu-ray-dvd-digital-3-disc/-/A-52030319 -s &quot;Rogue One: A Star Wars Story&quot; 这返回的信息，包括一个URL，为一个API返回该页面的产品数据: 123456URL: https://redsky.target.com/v2/pdp/tcin/52030319METHOD: GETAVG RESPONSE SIZE: 34834SEARCH TERM CONTEXT: c&quot;:&quot;786936852318&quot;,&quot;product_description&quot;:&#123;&quot;title&quot;:&quot;Rogue One: A Star Wars Story (Blu-ray + DVD + Digital) 3 Disc&quot;,&quot;long_description&quot;:... 使用-i标志，可以从提供的URL开始爬取多个页面(默认为一个页面)。这对于搜索特定关键字的所有网络流量非常有用，或者，通过省略-s搜索词标志，收集加载每个页面时发生的所有API流量。 所有收集到的数据都存储为一个HAR文件，存储在项目根目录的默认目录/ HAR中，不过可以使用-d标志更改该目录。 如果没有提供URL，还可以传入一个预先收集的HAR文件目录，用于搜索和分析。 本项目提供了许多其他功能，包括: 不必要的参数删除(删除不影响API调用返回值的GET或POST参数) 多种API输出格式(命令行、HTML、JSON) 区分表示单独API路由的路径参数和仅作为相同API路由的GET参数的路径参数 随着我和其他人继续使用它进行web抓取和API收集，还计划进行进一步的开发。 Combining APIs with Other Data Sources尽管许多现代web应用程序存在的理由是采用现有数据并以更吸引人的方式对其进行格式化，但我认为在大多数情况下，这样做并不有趣。如果您使用API作为惟一的数据源，那么您所能做的最好的事情就是复制其他人已经存在的数据库，而且基本上已经发布了。更有趣的是，以一种新颖的方式将两个或多个数据源组合在一起，或者使用API作为一种工具，从一个新的角度查看被抓取的数据。 让我们看一个例子，看看如何将来自api的数据与web抓取结合使用，以了解世界上哪些地方对Wikipedia贡献最大。 如果你在维基百科上花了很多时间，你可能会看到一篇文章的修订历史页面，其中显示了最近的编辑列表。如果用户在编辑时登录到Wikipedia，则显示他们的用户名。如果没有登录，则记录它们的IP地址，如图。 历史页面上提供的IP地址是121.97.110.145。通过使用freegeoip.net API，在撰写本文时，该IP地址来自菲律宾的奎松(IP地址有时会在地理上发生变化)。 这些信息本身并不那么有趣，但是如果您可以收集关于Wikipedia编辑的许多地理数据点，以及它们发生在哪里，情况会怎样呢?几年前，我就是这么做的，并使用谷歌的地理图库创建了一个有趣的图表，显示了在英文维基百科上编辑的位置，以及维基百科是用其他语言编写的。 创建一个基本脚本来抓取Wikipedia，查找修订历史页面，然后在这些修订历史页面上查找IP地址并不困难。使用第3章修改过的代码，下面的脚本就可以做到这一点: 1234567891011121314151617181920212223242526272829303132333435363738394041from urllib.request import urlopenfrom bs4 import BeautifulSoupimport jsonimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(articleUrl)) bs = BeautifulSoup(html, 'html.parser') return bs.find('div', &#123;'id':'bodyContent'&#125;).findAll('a', href=re.compile('^(/wiki/)((?!:).)*$'))def getHistoryIPs(pageUrl): #Format of revision history pages is: #http://en.wikipedia.org/w/index.php?title=Title_in_URL&amp;action=history pageUrl = pageUrl.replace('/wiki/', '') historyUrl = 'http://en.wikipedia.org/w/index.php?title=&#123;&#125;&amp;action=history'.format(pageUrl) print('history url is: &#123;&#125;'.format(historyUrl)) html = urlopen(historyUrl) bs = BeautifulSoup(html, 'html.parser') #finds only the links with class "mw-anonuserlink" which has IP addresses #instead of usernames ipAddresses = bs.findAll('a', &#123;'class':'mw-anonuserlink'&#125;) addressList = set() for ipAddress in ipAddresses: addressList.add(ipAddress.get_text()) return addressListlinks = getLinks('/wiki/Python_(programming_language)')while(len(links) &gt; 0): for link in links: print('-'*20) historyIPs = getHistoryIPs(link.attrs['href']) for historyIP in historyIPs: print(historyIP) newLink = links[random.randint(0, len(links)-1)].attrs['href'] links = getLinks(newLink) 输出为： 1234567891011121314151617181920212223242526272829303132333435363738394041history url is: http://en.wikipedia.org/w/index.php?title=Programming_paradigm&amp;action=history223.104.186.241213.207.90.15892.115.222.143213.108.115.552605:a601:e0c:6300:996d:68c0:fb03:af2c192.117.105.4731.203.136.191168.216.130.1332a02:c7d:a492:f200:e126:2b36:53ca:513a37.238.238.36197.255.127.246110.55.67.15193.80.242.22042.111.56.168223.230.96.108113.162.8.24939.36.182.41--------------------history url is: http://en.wikipedia.org/w/index.php?title=Object-oriented_programming&amp;action=history113.199.249.237205.251.185.2501.22.150.73121.58.212.157217.225.8.24162.204.116.16112.200.199.62117.239.185.50103.252.25.104103.74.23.139103.241.244.362605:a601:474:600:2088:fbde:7512:53b2122.181.5.16224.93.131.140119.152.87.8493.136.125.20827.251.109.234223.230.215.145103.16.68.215170.142.177.246-------------------- 这个程序使用了两个主要功能:getLinks(同时也是用于第三章),和新的getHistoryIPs搜索所有链接的内容与类mw-anonuserlink(表明一个匿名用户提供一个IP地址,而不是用户名)并返回一组。 这段代码还使用了一种有点随意的(但对于本例来说是有效的)搜索模式来查找可以从中检索修订历史的文章。它首先检索由起始页面链接到的所有Wikipedia文章的历史记录(在本例中，是关于Python编程语言的文章)。然后，它随机选择一个新的起始页面，并检索该页面链接到的所有文章的修订历史页面。它将一直持续到没有链接的页面。 现在您已经有了以字符串形式检索IP地址的代码，您可以将其与上一节中的getCountry函数结合使用，以便将这些IP地址解析为国家。您将稍微修改getCountry，以便解释导致404 Not Found错误的无效或格式错误的IP地址(例如，在撰写本文时，FreeGeoIP无法解决IPv6，这可能会触发这样的错误): 12345678910111213141516171819202122def getCountry(ipAddress): try: response = urlopen( 'http://freegeoip.net/json/&#123;&#125;'.format(ipAddress)).read().decode('utf-8') except HTTPError: return None responseJson = json.loads(response) return responseJson.get('country_code') links = getLinks('/wiki/Python_(programming_language)')while(len(links) &gt; 0): for link in links: print('-'*20) historyIPs = getHistoryIPs(link.attrs["href"]) for historyIP in historyIPs: country = getCountry(historyIP) if country is not None: print('&#123;&#125; is from &#123;&#125;'.format(historyIP, country)) newLink = links[random.randint(0, len(links)-1)].attrs['href'] links = getLinks(newLink) Here’s the sample output: 123456789101112131415-------------------history url is: http://en.wikipedia.org/w/index.php?title=Programming_paradigm&amp;action=history68.183.108.13 is from US86.155.0.186 is from GB188.55.200.254 is from SA108.221.18.208 is from US141.117.232.168 is from CA76.105.209.39 is from US182.184.123.106 is from PK212.219.47.52 is from GB72.27.184.57 is from JM49.147.183.43 is from PH209.197.41.132 is from US174.66.150.151 is from US More About APIs本章介绍了现代api通常用于访问web上的数据的几种方法，以及如何使用这些api构建更快更强大的web抓取器。如果您正在寻找构建api而不是仅仅使用它们，或者如果您想了解更多关于它们的构造和语法的理论，我推荐Leonard Richardson、Mike Amundsen和Sam Ruby (O Reilly)编写的RESTful Web api。本书对在web上使用api的理论和实践提供了一个强有力的概述。此外，Mike Amundsen有一个很有意思的视频系列，为Web设计api (O Reilly)，它教你如何创建自己的api，如果你决定以一种方便的格式向公众提供你的抓取数据，这是一件很有用的事情。 虽然有些人可能会哀叹JavaScript和动态网站的无所不在，使传统的抓取和解析HTML页面的做法过时了，但我对我们的新机器人统治者表示欢迎。由于动态网站不太依赖HTML页面供人使用，而更多地依赖严格格式化的JSON文件供HTML使用，这为每个试图获得干净、格式良好的数据的人提供了便利。web不再是偶尔带有多媒体和CSS装饰的HTML页面的集合。它是数百种文件类型和数据格式的集合，一次可以快速生成数百个页面，您可以通过浏览器使用这些页面。真正的技巧通常是越过你面前的页面，抓住数据的来源。更多的]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-11]]></title>
    <url>%2F2019%2F07%2F30%2FWebScraping11%2F</url>
    <content type="text"><![CDATA[客户端脚本语言是在浏览器本身中运行的语言，而不是在web服务器上运行的语言。客户端语言的成功取决于浏览器正确解释和执行该语言的能力。 部分原因是很难让每个浏览器制造商都同意一个标准，客户端语言比服务器端语言少得多。对于web抓取来说，这是一件好事:需要处理的语言越少越好 在大多数情况下，您在网上只会经常遇到两种语言:Action Script (Flash应用程序使用的脚本)和JavaScript。与10年前相比，ActionScript现在的使用频率要低得多，它经常被用来流媒体文件，作为在线游戏的平台，或者显示没有人想看介绍页面的网站的介绍页面。无论如何，由于对抓取Flash页面的需求不大，本章将重点放在现代web页面中普遍存在的客户端语言:JavaScript。 到目前为止，JavaScript是web上最常见、最受支持的客户端脚本语言。它可以用来收集用户跟踪的信息，无需重新加载页面即可提交表单，嵌入多媒体，甚至可以支持整个在线游戏。即使看起来很简单的页面也常常包含多个JavaScript片段。你可以发现它嵌入script标签之间的网页的源代码： 123&lt;script&gt; alert("This creates a pop-up using JavaScript");&lt;/script&gt; A Brief Introduction to JavaScript至少对您正在抓取的代码中发生的事情有一些了解是非常有用的。记住这一点，熟悉自己是个好主意JavaScript JavaScript是一种弱类型语言，其语法常常与c++和Java相比较。尽管语法的某些元素，如操作符、循环和数组，可能是类似的，但是这种语言的弱类型和类似脚本的性质可能使它成为一些程序员难以处理的问题。 例如，下面递归地计算斐波那契数列中的值，并将它们打印到浏览器的开发人员控制台： 12345678910&lt;script&gt; function fibonacci(a, b)&#123; var nextNum = a + b; console.log(nextNum+" is in the Fibonacci sequence"); if(nextNum &lt; 100)&#123; fibonacci(b, nextNum); &#125; &#125; fibonacci(1, 1);&lt;/script&gt; 注意，所有变量都是通过在它们前面加上var来标识的。这类似于PHP中的$符号，或Java或c++中的类型声明(int、String、List等)。Python的不同寻常之处在于它没有这种显式的变量声明。 12345678910111213141516&lt;script&gt;var fibonacci = function() &#123; var a = 1; var b = 1; return function() &#123; var temp = b; b = a + b; a = temp; return b; &#125;&#125;var fibInstance = fibonacci();console.log(fibInstance() + " is in the Fibonacci sequence");console.log(fibInstance() + " is in the Fibonacci sequence");console.log(fibInstance() + " is in the Fibonacci sequence");&lt;/script&gt; 乍一看，这似乎有点吓人，但是如果你从lambda表达式的角度来考虑(在第2章中讨论过)，它就会变得很简单，变量fibonacci被定义为一个函数。函数的值返回一个函数，该函数在斐波那契数列中打印越来越大的值。每次调用它时，它都会返回Fibonaccicalculating函数，该函数将再次执行并增加函数中的值。 虽然乍一看似乎有些复杂，但有些问题，比如计算斐波那契值，倾向于像这样的模式。在处理用户操作和回调时，将函数作为变量传递也是非常有用的，在阅读JavaScript时，熟悉这种编程风格是值得的。 Common JavaScript LibrariesjQueryjQuery是一个非常常见的库，70%最流行的internet站点和大约30%的internet站点都使用它。使用jQuery的站点很容易识别，因为它的代码中包含了对jQuery的导入: 123&lt;script src="http://ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"&gt;&lt;/script&gt; 如果在站点上找到jQuery，在抓取它时必须小心。jQuery擅长动态创建仅在JavaScript执行后才出现的HTML内容。如果使用传统方法抓取页面内容，则只检索JavaScript创建内容之前出现的预加载页面(一会有更详细的介绍)。 此外，这些页面更可能包含动画、交互式内容和嵌入式媒体，这可能使抓取具有挑战性。 Google Analytics大约50%的网站使用谷歌分析，这使得它可能是最常见的JavaScript库和互联网上最流行的用户跟踪工具。无论是http://pythonscraping.com还是http://www.oreilly.com/都使用了谷歌分析。 确定一个页面是否使用谷歌分析是很容易的。它将在底部有JavaScript，类似于以下内容(摘自O ‘Reilly媒体网站): 123456789101112131415&lt;!-- Google Analytics --&gt;&lt;script type="text/javascript"&gt;var _gaq = _gaq || [];_gaq.push(['_setAccount', 'UA-4591498-1']);_gaq.push(['_setDomainName', 'oreilly.com']);_gaq.push(['_addIgnoredRef', 'oreilly.com']);_gaq.push(['_setSiteSpeedSampleRate', 50]);_gaq.push(['_trackPageview']);(function() &#123; var ga = document.createElement('script'); ga.type ='text/javascript'; ga.async = true; ga.src = ('https:' ==document.location.protocol ? 'https://ssl' : 'http://www') +'.google-analytics.com/ga.js'; var s =document.getElementsByTagName('script')[0];s.parentNode.insertBefore(ga, s); &#125;)();&lt;/script&gt; 此脚本处理特定于谷歌分析程序的cookie，用于跟踪页面之间的访问。对于旨在执行JavaScript和处理cookie的web抓取器(如使用Selenium的web抓取器，本章稍后将对此进行讨论)，这有时可能是一个问题。 如果一个站点使用谷歌分析或类似的web分析系统，而您不希望该站点知道它正在被爬行或被刮除，请确保丢弃用于分析的任何cookie或完全丢弃cookie。 Google Maps如果你在互联网上呆过一段时间，你几乎肯定会在一个网站上看到嵌入的谷歌地图。它的API使得在任何站点上嵌入带有自定义信息的地图变得极其简单。 如果您正在抓取任何类型的位置数据，了解谷歌映射的工作原理可以很容易地获得格式良好的经纬度坐标，甚至地址。在谷歌映射中，最常用的表示位置的方法之一是通过标记(也称为pin)。 使用如下代码，可以将标记插入任何谷歌映射: 12345var marker = new google.maps.Marker(&#123; position: new google.maps.LatLng(-25.363882,131.044922), map: map, title: 'Some marker text'&#125;); Python使提取google.maps之间发生的所有坐标实例变得很容易。获取纬度/经度坐标列表。 Ajax and Dynamic HTML到目前为止，我们与web服务器通信的唯一方法是通过检索新页面向它发送某种HTTP请求。如果您曾经在没有重新加载页面的情况下提交过表单或从服务器检索过信息，那么您可能使用过使用Ajax的网站。 与一些人所认为的相反，Ajax不是一种语言，而是一组用于完成特定任务的技术(仔细想想，就像web抓取一样)。Ajax代表Asynchronous(异步)JavaScript和XML，用于从web服务器发送和接收信息，而不需要发出单独的页面请求。 You should never say, “This website will be written in Ajax.” It would be correct to say, “This form will use Ajax to communicate with the web server.” 与Ajax一样，动态HTML (DHTML)是一组用于共同目的的技术。DHTML是HTML代码、CSS语言，或者两者都随着客户机端脚本更改页面上的HTML元素而更改。按钮可能只在用户移动光标之后才出现，背景颜色可能在单击时发生变化，或者Ajax请求可能触发要加载的新内容块。 请注意，虽然“动态”这个词通常与“移动”或“更改”之类的词相关联，但是交互式HTML组件、移动图像或嵌入式媒体的出现并不一定使页面DHTML成为动态的，即使它看起来可能是动态的。此外，internet上一些最无趣、看起来静态的页面可以在后台运行DHTML进程，这些进程依赖于使用JavaScript来操纵HTML和CSS。 如果您抓取了许多网站，您很快就会遇到这样的情况:您在浏览器中查看的内容与您在从站点检索的sourcec ode中看到的内容不匹配。您可能会查看scraper的输出，然后挠挠头，试图找出浏览器中相同页面上所看到的所有内容都消失到哪里去了。 web页面也可能有一个加载页面，它会将您重定向到另一个结果页面，但是您会注意到，当重定向发生时，页面的URL不会更改。这两种情况都是由于scraper无法执行JavaScript而导致的，而JavaScript正是在页面上实现这一神奇的功能的原因。如果没有JavaScript, HTML就只是停留在那里，站点可能看起来与web浏览器中的非常不同，web浏览器执行JavaScript时没有问题。页面可能使用Ajax或DHTML来更改/加载内容，但在这种情况下，只有两种解决方案:直接从JavaScript中提取内容;或者使用能够执行JavaScript本身的Python包，并在浏览器中查看网站时对其进行抓取。 Executing JavaScript in Python with SeleniumSelenium是一个功能强大的web抓取工具，最初是为网站测试开发的。现在，当需要准确地描述web站点(如在浏览器中出现的站点)时，也可以使用它。Selenium通过自动化浏览器来加载网站、检索所需的数据，甚至可以截屏或断言网站上发生了某些操作。 Selenium不包含自己的web浏览器;它需要与第三方浏览器集成才能运行。例如，如果您使用Firefox运行Selenium，您将看到屏幕上打开一个Firefox实例，导航到网站，并执行代码中指定的操作。虽然这看起来很整洁，但是我更喜欢脚本在后台静静地运行，所以我使用了一个名为PhantomJS的工具来代替实际的浏览器。 PhantomJS是一种headless浏览器。它将网站加载到内存中，并在页面上执行JavaScript，但不向用户呈现网站的任何图形。通过将Selenium与PhantomJS结合起来，您可以运行一个非常强大的web scraper，它可以轻松地处理cookie、JavaScript、header和其他所有您需要的东西。 您可以从其网站下载Selenium库，或者使用第三方安装程序(如pip)从命令行安装它。 虽然很多页面都使用Ajax加载数据(尤其是谷歌)，但我在http://pythonscraping.com/pages/javascript/ajaxDemo.html中创建了一个示例页面来运行爬虫。这个页面包含一些示例文本，硬编码到页面HTML中，经过两秒钟的延迟后，这些文本将被ajax生成的内容所替代。如果您要使用传统方法抓取此页面的数据，您只会得到加载页面，而不会得到您想要的数据。 Selenium库是对象WebDriver上调用的API。WebDriver.程序有点像浏览器，因为它可以加载网站，但它也可以像BeautifulSoup对象一样被用来查找页面元素，与页面上的元素交互(发送文本、单击等)，以及执行其他操作来驱动web爬虫。 123456789101112from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsimport timechrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options)driver.get('http://pythonscraping.com/pages/javascript/ajaxDemo.html')time.sleep(3)print(driver.find_element_by_id('content').text)driver.close() 如果：time.sleep(3)运行，那么输出如下： 12Here is some important text you want to retrieve!A button to click! 如果注释掉time.sleep(3)，那么输出如下： 1This is some content that will appear on the page while it&apos;s loading. You don&apos;t care about scraping this. Selenium Selectors 前几章中，您已经使用BeautifulSoup选择器(如find和find_all)选择了页面元素。Selenium使用一组全新的选择器来查找web驱动程序DOM中的元素，尽管它们的名称相当简单。 在这个例子中，您使用了选择器find_element_by_id，尽管下面的其他选择器也可以工作: 12driver.find_element_by_css_selector('#content')driver.find_element_by_tag_name('div') 当然，如果希望在页面上选择多个元素，这些元素选择器中的大多数都可以使用元素返回Python元素列表： 12driver.find_elements_by_css_selector('#content')driver.find_elements_by_css_selector('div') 如果你仍然想使用BeautifulSoup来解析这些内容，你可以使用Web‐方法所查看的，该函数返回页面的源代码 123pageSource = driver.page_sourcebs = BeautifulSoup(pageSource, 'html.parser')print(bs.find(id='content').get_text()) 这将使用PhantomJS库创建一个新的Selenium web驱动程序，它告诉WebDriver加载一个页面，然后暂停执行三秒钟，然后查看页面以检索(希望已加载)内容。 根据安装PhantomJS的位置，在创建新的PhantomJS Web驱动程序时，可能还需要显式地将Selenium指向正确的方向: 12driver = webdriver.PhantomJS(executable_path='path/to/driver/'\ 'phantomjs-1.9.8-macosx/bin/phantomjs') 虽然这个解决方案有效，但它的效率有些低，而且实现它可能会在很大程度上导致问题。页面加载时间是不一致的，这取决于任何特定毫秒的服务器负载，并且连接速度会发生自然的变化。虽然这个页面加载应该只需要2秒多一点的时间，但是您给了它整整3秒的时间来确保它完全加载。更有效的解决方案是反复检查已加载的页面上是否存在特定的元素，并仅在该元素存在时返回。 12345678910111213141516171819from selenium import webdriverfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECchrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options)driver.get('http://pythonscraping.com/pages/javascript/ajaxDemo.html')try: element = WebDriverWait(driver, 10).until( EC.presence_of_element_located((By.ID, 'loadedButton')))finally: print(driver.find_element_by_id('content').text) driver.close() 个脚本有几个新的导入，最著名的是WebDriverWait和expected_con，这两个变量在这里组合在一起，形成Selenium所称的(implicit wait)隐式等待。 隐式等待与显式等待的区别在于，它在DOM在继续之前发生，而显式等待定义了硬编码时间，如前一个示例所示，它的等待时间为3秒。在隐式等待中，触发DOM状态由expected_condition定义(注意，这里的导入转换为EC，这是一种用于简洁的常见约定)。Selenium库中的预期条件可以有很多，包括以下内容: 弹出一个警告框。 元素(例如文本框)被放入选定的状态(selected state)。 页面的标题更改，或文本现在显示在页面或特定元素中。 元素现在对DOM可见，或者元素从DOM中消失。 大多数这些预期的条件要求您首先指定要监视的元素。元素是使用定位器指定的。注意，定位器与选择器不同(有关选择器的更多信息，请参阅刚刚介绍的“Selenium选择器”)。定位器是一种抽象的查询语言，使用By对象，可以以多种方式使用，包括创建选择器。 在下面的代码中，定位器用于查找ID loadedButton的元素： 1EC.presence_of_element_located((By.ID, 'loadedButton')) 定位器也可以用来创建选择器，使用find_element WebDriver函数: 1print(driver.find_element(By.ID, 'content').text) 当然，这在功能上相当于示例代码中的行: 1print(driver.find_element_by_id('content').text) 如果你不需要使用定位器，就不要使用;它将为您节省导入。然而，这个方便的工具用于各种应用程序，并且具有很大的灵活性。 以下定位器选择策略可以与By对象一起使用: ID : 在本例中使用;根据元素的HTML id属性查找元素。 CLASS_NAME : 用于根据其HTML类属性查找元素。为什么这个函数是CLASS_NAME而不是CLASS?使用form object.CLASS会给Selenium的Java库带来问题，其中.class是一个保留方法。为了保持不同语言之间Selenium语法的一致性，使用了CLASS_NAME。 CSS_SELECTOR : 使用#idName、. classname、tagName约定，根据类、id或标记名称查找元素。 LINK_TEXT : 根据包含的文本查找HTML&lt;a&gt; 标记。例如，可以使用(By.LINK_TEXT, “NEXT”)，”NEXT”标签会被选择出来。 PARTIAL_LINK_TEXT : 类似于LINK_TEXT，但匹配部分字符串。 NAME : 根据名称属性查找HTML标记。这对于HTML表单非常方便。 TAG_NAME : 根据标记名称查找HTML标记。 使用XPath表达式(其语法将在下面描述)来选择匹配的元素。 XPath Syntax XPath (XML Path的缩写)是一种查询语言，用于导航和选择XML文档的某些部分。W3C于1999年创建，在处理XML文档时，偶尔会在Python、Java和c#等语言中使用它。 虽然BeautifulSoup不支持XPath，但本书中的许多其他库(如Scrapy和Selenium)都支持。它通常可以与CSS选择器(如mytag#idname)以相同的方式使用，尽管它被设计用于更通用的情况特别是XML文档而不是HTML文档。 XPath语法有四个主要概念: 根节点和非根节点 /div只有在div节点位于文档的根节点时才会选择它 //div选择文档中任何位置的所有div。 属性选择 //@href选择具有href属性的任何节点。 //a[@href=&#39;http://google.com&#39;]选择文档中指向谷歌的所有链接。 按位置选择节点 //a[3]选择文档中的第三个链接。 //table[last()]选择文档中的最后一个表。 //a[position() &lt; 3]选择文档中的前三个链接。 星号(*)匹配任何一组字符或节点，可用于各种情况 //table/tr/*选择所有表中tr标记的所有子元素(这对于同时使用th和td标记选择单元格非常有用)。 //div[@*]选择所有具有任何属性的div标记。 XPath语法还有许多高级特性。多年来，它已经发展成为一种相对复杂的查询语言，具有布尔逻辑、函数(如position())和这里没有讨论的各种操作符。 如果有HTML或XML选择问题无法通过这里显示的函数解决，请参阅Microsoft的XPath语法页面。 Additional Selenium Webdrivers在前一节中，PhantomJS驱动程序与Selenium一起使用。在大多数情况下，没有什么理由让浏览器在屏幕上弹出并开始抓取web，所以像PhantomJS这样的无头网络驱动程序可以很方便。然而，使用不同类型的web浏览器可能对运行爬虫很有用，原因如下: 故障排除。如果您的代码正在运行PhantomJS，并且失败了，如果没有看到前面的页面，可能很难诊断失败。您还可以暂停代码执行，并像往常一样随时与web页面进行交互。 测试可能依赖于特定的浏览器来运行。 一个异常挑剔的网站或脚本在不同的浏览器上的表现可能略有不同。您的代码可能无法在PhantomJS中工作。 现在，许多官方和非官方的组织都参与了为每个主要浏览器创建和维护Selenium web驱动程序。Selenium组管理这些web驱动程序的集合，以便于参考。 1234firefox_driver = webdriver.Firefox('&lt;path to Firefox webdriver&gt;')chrome_driver = webdriver.Chrome('&lt;path to Chrome webdriver&gt;')safari_driver = webdriver.Safari('&lt;path to Safari webdriver&gt;')ie_driver = webdriver.Ie('&lt;path to Internet Explorer webdriver&gt;') Handling Redirects客户端重定向是由JavaScript在浏览器中执行的页面重定向，而不是在发送页面内容之前在服务器上执行的重定向。在web浏览器中访问页面时，有时很难区分它们。重定向可能发生得非常快，以至于您没有注意到加载时间上的任何延迟，并且假设客户端重定向实际上是服务器端重定向。 然而，当抓取web时，差别是明显的。服务器端重定向，取决于它是如何处理的，可以很容易地由Python的urllib库遍历，而不需要Selenium的任何帮助(关于这方面的更多信息，请参阅第3章)。 Selenium能够像处理其他JavaScript执行一样处理这些JavaScript重定向;然而，这些重定向的主要问题是何时停止页面执行—也就是说，如何判断页面何时完成重定向。在http://pythonscraping.com/pages/javascript/redirectDemo1.html上的演示页面给出了这种重定向的示例，其中有一个两秒钟的暂停。 通过在页面初始加载时“监视”DOM中的元素，然后反复调用该元素，直到Selenium抛出StaleElementReferenceException，您可以以一种聪明的方式检测重定向;元素不再附加到页面的DOM，网站已重定向: 12345678910111213141516171819202122232425262728from selenium import webdriverfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.remote.webelement import WebElementfrom selenium.common.exceptions import StaleElementReferenceExceptionimport timedef waitForLoad(driver): elem = driver.find_element_by_tag_name("html") count = 0 while True: count += 1 if count &gt; 20: print("Timing out after 10 seconds and returning") return time.sleep(.5) try: elem == driver.find_element_by_tag_name("html") except StaleElementReferenceException: returnchrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options)driver.get("http://pythonscraping.com/pages/javascript/redirectDemo1.html")waitForLoad(driver)print(driver.page_source)driver.close() 此脚本每半秒检查一次页面，超时时间为10秒，不过用于检查时间和超时的时间可以根据需要轻松地向上或向下调整。 或者，您可以编写一个类似的循环来检查页面的当前URL，直到URL发生更改，或者它匹配您正在寻找的特定URL。 等待元素出现和消失是Selenium中的一项常见任务，您还可以使用前面的按钮加载示例中使用的相同的WebDriverWait函数。在这里，您为它提供了一个15秒的超时和一个XPath选择器，该选择器查找页面主体内容来完成相同的任务: 123456789101112131415161718from selenium.webdriver.common.by import Byfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.support import expected_conditions as ECfrom selenium.common.exceptions import TimeoutExceptionchrome_options = Options()chrome_options.add_argument("--headless")driver = webdriver.Chrome( executable_path='C:\Program Files (x86)\Google\Chrome\Application\chromedriver.exe', options=chrome_options)driver.get('http://pythonscraping.com/pages/javascript/redirectDemo1.html')try: bodyElement = WebDriverWait(driver, 15).until(EC.presence_of_element_located( (By.XPATH, '//body[contains(text(), "This is the page you are looking for!")]'))) print(bodyElement.text)except TimeoutException: print('Did not find the element') A Final Note on JavaScript现在大多数网站都使用JavaScript。对我们来说幸运的是，在许多情况下，这种Java脚本的使用不会影响您如何抓取页面。例如，JavaScript可能仅限于为其跟踪工具供电、控制站点的一小部分或操作下拉菜单。如果JavaScript确实影响了您抓取站点的方式，那么可以使用Selenium之类的工具轻松地执行JavaScript，以便生成您在本书第一部分中学习过的简单HTML页面。 记住:仅仅因为一个站点使用JavaScript并不意味着所有传统的web抓取工具都将消失。JavaScript的最终目的是生成可以由浏览器呈现的HTML和CSS代码，或者通过HTTP请求和响应与服务器进行动态通信。一旦使用了Selenium，页面上的HTML和CSS就可以像使用任何其他网站代码一样被读取和解析，HTTP请求和响应可以通过前面几章中的技术由代码发送和处理，甚至无需使用Selenium。 此外，JavaScript甚至可以成为web抓取器的一个优势，因为它作为一个浏览器端内容管理系统的使用可能会向外部世界公开有用的api，让您更直接地获取数据。有关这方面的更多信息，请参见第十二章。 如果您仍然难以处理特别复杂的JavaScript情况，您可以在第14章中找到关于Selenium的信息，并直接与动态网站交互，包括拖放界面。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-10]]></title>
    <url>%2F2019%2F07%2F29%2FWebScraping10%2F</url>
    <content type="text"><![CDATA[当抓取的基础知识时，首先出现的问题之一是:“我如何在登录屏幕后访问信息?”“互联网正日益向互动、社交媒体和用户生成内容迈进。表单和登录是这类站点的一个组成部分，几乎不可能避免。幸运的是，它们也相对容易处理。 在此之前，我们与示例抓取器中的web服务器的大多数交互都是使用HTTP GET请求信息。本章重点介绍POST方法，它将信息推送到web服务器进行存储和分析。 表单基本上为用户提供了一种提交POST请求的方法，web服务器可以理解和使用该方法。就像网站上的链接标签帮助用户格式化请求一样，HTML表单帮助他们格式化POST请求。当然，只要编写一点代码，就可以自己创建这些请求并使用scraper提交它们。 Python Requests Library虽然只使用Python核心库就可以导航web表单，但是有时候稍微添加一点语法糖份会让您的生活更加甜蜜。当您开始使用urllib执行比基本GET请求更多的操作时，查看Python核心库之外的内容可能会有所帮助。 请求库非常擅长处理复杂的HTTP请求、cookie、标头等等。以下是创建者Kenneth Reitz对Python核心工具的一些要求: Python的标准urllib2模块提供了您需要的大部分HTTP功能，但是API已经完全被破坏了。它是为不同的时代和不同的网络而建的。它需要大量的工作(甚至方法覆盖)来执行最简单的任务。 事情不应该是这样的。在Python中不 Submitting a Basic Form大多数web表单由几个HTML字段、一个submit按钮和一个action页面组成，实际的表单处理就是在这个页面上完成的。HTML字段通常由文本组成，但也可能包含文件上载或其他非文本内容。 大多数流行的网站在他们的robots.txt文件(章节)中为了安全起见而阻止访问他们的登录表单，我构建了一系列不同类型的表单和pythonscraping.com的登录，您可以在这些表单上运行web scraper。http://pythonscraping.com/pages/files/form.html是这些表单中最基本的位置。 整个表格内容如下: 12345&lt;form method="post" action="processing.php"&gt;First name: &lt;input type="text" name="firstname"&gt;&lt;br&gt;Last name: &lt;input type="text" name="lastname"&gt;&lt;br&gt;&lt;input type="submit" value="Submit"&gt;&lt;/form&gt; 这里需要注意几件事:首先，两个输入字段的名称分别是firstname和lastname。这是很重要的。这些字段的名称决定了提交表单时提交到服务器的变量参数的名称。如果希望模拟表单在发布自己的数据时所采取的操作，则需要确保变量名匹配。 需要注意的第二件事是，表单的操作位于processing.php(绝对路径是http://pythonscraping.com/files/processing.php)。对表单的任何POST请求都应该在这个页面上发出，而不是在表单本身所在的页面上。记住:HTML表单的目的只是帮助网站访问者格式化正确的请求，以便发送到执行实际操作的页面。除非您正在对请求本身进行格式化研究，否则不需要在表单所在的页面上花费太多精力。使用请求库提交表单可以用四行代码完成，包括导入和打印内容的指令(是的，很简单): 1234import requestsparams = &#123;'firstname': 'Ryan', 'lastname': 'Mitchell'&#125;r = requests.post("http://pythonscraping.com/pages/processing.php", data=params)print(r.text) 提交表单后，脚本应返回页面内容: 1Hello there, Ryan Mitchell! 这个脚本可以应用于internet上遇到的许多简单表单。例如，O ‘Reilly Media newsletter的注册表单如下: 123456789101112&lt;form action="http://post.oreilly.com/client/o/oreilly/forms/ quicksignup.cgi" id="example_form2" method="POST"&gt; &lt;input name="client_token" type="hidden" value="oreilly"/&gt; &lt;input name="subscribe" type="hidden" value="optin"/&gt; &lt;input name="success_url" type="hidden" value="http://oreilly.com/store/ newsletter-thankyou.html"/&gt; &lt;input name="error_url" type="hidden" value="http://oreilly.com/store/ newsletter-signup-error.html"/&gt; &lt;input name="topic_or_dod" type="hidden" value="1"/&gt; &lt;input name="source" type="hidden" value="orm-home-t1-dotd"/&gt; &lt;fieldset&gt; &lt;input class="email_address long" maxlength="200" name="email_addr" size="25" type="text" value="Enter your email here"/&gt; &lt;button alt="Join" class="skinny" name="submit" onclick="return addClickTracking('orm','ebook','rightrail','dod' );" value="submit"&gt;Join&lt;/button&gt; &lt;/fieldset&gt;&lt;/form&gt; 尽管一开始看起来很吓人，但请记住，在大多数情况下(我们将在稍后讨论例外情况)，您只需要寻找两件事: 要与数据一起提交的字段(或多个字段)的名称(在本例中，名称为email_addr) 表单本身的动作属性;也就是表单发布到的页面(在本例中是http://post.oreilly.com/client/o/oreilly/forms/quicksignup.cgi) 需添加所需的信息，并运行它: 12345import requestsparams = &#123;'email_addr': 'ryan.e.mitchell@gmail.com'&#125;r = requests.post("http://post.oreilly.com/client/o/oreilly/forms/quicksignup.cgi",data=params)print(r.text) 在本例中，返回的网站是另一个需要填写的表单，然后才能将其添加到O ‘Reilly的邮件列表中，但是同样的概念也可以应用到该表单中。但是，如果您想在家里尝试此功能，我要求您永远使用您的权限，而不是用无效的注册向发布者发送垃圾邮件。 Radio Buttons, Checkboxes, and Other Inputs并非所有web表单都是文本字段的集合，然后是submit按钮。标准HTML包含各种可能的表单输入字段:单选按钮、复选框和选择框等等。HTML5添加了滑块(范围输入字段)、电子邮件、日期等等。有了自定义JavaScript字段，有了颜色选择器、日历和开发人员接下来想到的任何其他东西，可能性是无限的。 对应于Python参数对象： 1&#123;'thing1':'foo', 'thing2':'bar'&#125; 如果您被一个看起来很复杂的POST表单困住了，并且想要确切地看到浏览器正在向服务器发送哪些参数，最简单的方法是使用浏览器的inspector或developer工具来查看它们: 可以通过菜单访问Chrome开发工具View $\rightarrow$ Developer$\rightarrow$ Developer Tools.。它提供了浏览器在与当前网站交互时生成的所有查询的列表，可以作为详细查看这些查询组成的好方法 Submitting Files and Images虽然文件上传在互联网上很常见，但文件上传并不经常用于web抓取。但是，您可能希望为自己的站点编写一个包含文件上载的测试。无论如何，知道如何做是件有用的事。 在http://pythonscraping/files/form2.html中有一个练习文件上传表单。该页面的表格有以下标记: 1234&lt;form action="processing2.php" method="post" enctype="multipart/form-data"&gt;Submit a jpg, png, or gif: &lt;input type="file" name="uploadFile"&gt;&lt;br&gt;&lt;input type="submit" value="Upload File"&gt;&lt;/form&gt; 除了具有type属性文件的&lt;input&gt;标记外，它基本上与前面示例中使用的基于文本的表单相同。幸运的是，Python请求库使用表单的方式也很相似: 12345import requestsfiles = &#123;'uploadFile': open('files/python.png', 'rb')&#125;r = requests.post('http://pythonscraping.com/pages/processing2.php',files=files)print(r.text) Handling Logins and Cookies到目前为止，我们主要讨论的是允许您向站点提交信息或在表单之后立即在页面上查看所需信息的表单。这与登录表单有何不同?登录表单允许您在访问站点期间以永久登录状态存在。大多数现代网站都使用cookie来跟踪登录者和未登录者。站点对您的登录凭证进行身份验证后，将它们存储在浏览器的cookie中，cookie通常包含服务器生成的令牌、超时和跟踪信息。然后，站点使用这个cookie作为身份验证的一种证明，它会显示在您访问站点期间访问的每个页面上。在上世纪90年代中期cookie被广泛使用之前，让用户安全地进行身份验证并跟踪他们是网站面临的一个巨大问题。虽然cookie对于web开发人员来说是一个很好的解决方案，但是对于爬虫来说，它可能会有问题。你可以一整天都提交一个登录表单，但是如果你不跟踪表单发送给你的cookie，你访问的下一个页面就会表现得好像你从来没有登录过一样。 http://pythonscraping.com/pages/cookies/login.html中创建了一个简单的登录表单(用户名可以是任何东西，但是密码必须是“password”)。这个表单在http://pythonscraping.com/pages/cookies/welcome.php中处理，其中包含到主页面的链接http://pythonscraping.com/pages/cookies/profile.php。 如果您尝试在不先登录的情况下访问欢迎页面或配置文件页面，您将得到一条错误消息和指示，以便在继续之前先登录。在配置文件页面上，将对浏览器的cookie进行检查，查看其cookie是否设置在登录页面上。 12345678910import requestsparams = &#123;'username': 'Ryan', 'password': 'password'&#125;r = requests.post('http://pythonscraping.com/pages/cookies/welcome.php', params)print('Cookie is set to:')print(r.cookies.get_dict())print('Going to profile page...')r = requests.get('http://pythonscraping.com/pages/cookies/profile.php', cookies=r.cookies)print(r.text) 这里，您将登录参数发送到welcome页面，该页面充当登录表单的处理器。您可以从最后一个请求的结果中检索cookie，打印结果进行验证，然后通过设置cookie参数将其发送到配置文件页面。 这对于简单的情况非常有效，但是如果您处理的是一个更复杂的站点，它经常在没有警告的情况下修改cookie，或者您甚至不愿意一开始就考虑cookie，情况又会如何呢?请求session函数在这种情况下工作得很好: 1234567891011import requestssession = requests.Session()params = &#123;'username': 'username', 'password': 'password'&#125;s = session.post('http://pythonscraping.com/pages/cookies/welcome.php', params)print("Cookie is set to:")print(s.cookies.get_dict())print('Going to profile page...')s = session.get('http://pythonscraping.com/pages/cookies/profile.php')print(s.text) 在本例中，会话对象(通过调用request . session()检索)跟踪会话信息，比如cookie、头，甚至关于可能运行在HTTP之上的协议的信息，比如httpadapter。 session()是一个非常棒的库，仅次于Selenium，在不需要程序员思考或自己编写代码的情况下，它所处理内容的完整性。尽管坐下来让库完成所有工作可能很诱人，但是在编写时，始终注意cookie的外观和它们所控制的内容是非常重要的。它可以节省许多小时的痛苦调试或找出为什么一个网站的行为奇怪!]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Webscraping-9]]></title>
    <url>%2F2019%2F07%2F27%2FWebscraping9%2F</url>
    <content type="text"><![CDATA[到目前为止，您使用的数据通常都是数字或可数值的形式。在大多数情况下，您只是简单地存储了数据，而没有在事后进行任何分析。这一章试图解决英语这个棘手的问题。当你在图片搜索中输入可爱的小猫时，谷歌怎么知道你在寻找什么?因为文字围绕着可爱的小猫形象。当你在搜索栏中输入“死鹦鹉”时，YouTube怎么知道会弹出某个巨蟒剧团(Monty Python)的素描?因为每个上传的视频都有标题和描述文字。事实上，即使输入像“已故的鸟儿”这样的术语，python也会立即显示出同样的“死去的鹦鹉”草图，即使页面本身没有提到“已故的鸟儿”或“死去的鸟儿”。谷歌知道热狗是一种食物，而一只沸腾的小狗是完全不同的东西。如何?都是统计数字!虽然您可能不认为文本分析与您的项目有任何关系，但是理解它背后的概念对于各种机器学习都非常有用，对于用概率和算法术语建模真实世界问题的更一般的能力也非常有用。 例如，音乐服务可以识别包含特定歌曲录制的音频，即使该音频包含环境噪声或失真。谷歌的工作是基于图像本身自动字幕图像。通过比较已知的图片，例如热狗和其他热狗的图片，搜索引擎可以逐渐了解热狗的样子，并在显示的其他图片中观察这些模式。 Summarizing Data在第8章中，您研究了如何将文本内容分解为n-gram，即长度为n个单词的短语集。在一个基本的层次上，这可以用来确定哪些单词和短语在一段文本中最常用。此外，它还可以通过返回原始文本并围绕这些最流行的短语提取句子，从而创建听起来很自然的数据摘要。你将用来做这件事的一个例子文本是美国第九任总统威廉·亨利·哈里森的就职演说。哈里森的总统任期创造了白宫历史上的两项纪录:一项是最长的就职演说，另一项是任期最短的，只有32天。您将使用本文的全文作为本章中许多代码示例的源代码。 您将使用本文的全文作为本章中许多代码示例的源代码。 稍微修改第8章中用于查找代码的n-gram，您可以生成寻找2-gram集合的代码，并返回一个包含所有2-gram的计数器对象： 123456789101112131415161718192021222324252627282930313233343536373839404142from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringfrom collections import Counterdef cleanSentence(sentence): sentence = sentence.split(' ') sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence] sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == 'a' or word.lower() == 'i')] return sentencedef cleanInput(content): content = content.upper() content = re.sub('\n', ' ', content) content = bytes(content, 'UTF-8') content = content.decode('ascii', 'ignore') sentences = content.split('. ') return [cleanSentence(sentence) for sentence in sentences]def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputdef getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams)content = str( urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt').read(), 'utf-8')ngrams = getNgrams(content, 3)print(ngrams) 输出产生，部分： 123Counter(&#123;&apos;OF THE&apos;: 213, &apos;IN THE&apos;: 65, &apos;TO THE&apos;: 61, &apos;BY THE&apos;: 41,&apos;THE CONSTITUTION&apos;: 34, &apos;OF OUR&apos;: 29, &apos;TO BE&apos;: 26, &apos;THE PEOPLE&apos;: 24,&apos;FROM THE&apos;: 24, &apos;THAT THE&apos;: 23,... 在这2-gram中，“宪法”在演讲中似乎是一个相当受欢迎的主题，但“of the”、“to the”和“in the”似乎并不特别值得注意。你怎样才能以准确的方式自动删除不需要的单词? 幸运的是，有些人仔细研究了两者之间的差异“有趣”和“不有趣”的单词，他们的研究可以帮助我们做到这一点。杨百翰大学语言学教授马克·戴维斯坚持认为当代美国英语语料库(Corpus of Contemporary American English)，收录了近十年来美国流行出版物中的4.5亿多个单词。 5000个最常被发现的单词列表是免费的，幸运的是，这已经足够作为一个基本的过滤器来过滤掉最常见的2-gram单词。仅仅前100个单词就能极大地提高搜索结果，并添加了一个isCommon函数: 1234567891011121314151617def isCommon(ngram): commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL'] for word in ngram: if word in commonWords: return True return Falsedef getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): if not isCommon(content[i:i+n]): output.append(content[i:i+n]) return outputngrams = getNgrams(content, 3)print(ngrams) 这就产生了在正文中出现两次以上的2-gram文字: 1234Counter(&#123;&apos;UNITED STATES&apos;: 10, &apos;EXECUTIVE DEPARTMENT&apos;: 4,&apos;GENERAL GOVERNMENT&apos;: 4, &apos;CALLED UPON&apos;: 3, &apos;CHIEF MAGISTRATE&apos;: 3,&apos;LEGISLATIVE BODY&apos;: 3, &apos;SAME CAUSES&apos;: 3, &apos;GOVERNMENT SHOULD&apos;: 3,&apos;WHOLE COUNTRY&apos;: 3,... 需要注意的是，您使用的是相对现代的常用单词列表来过滤结果，考虑到文本编写于1841年，这可能并不合适。然而,因为你只使用第一个100个左右的单词列表你可以假设更稳定的随着时间的推移,说,过去100的字眼你似乎得到令人满意的结果,你可能能拯救自己的努力跟踪或创建一个从1841年最常见的单词列表(尽管这样的努力可能很有趣)。 既然已经从文本中提取了一些关键主题，那么这如何帮助您编写文本摘要呢?一种方法是搜索包含每个流行n-gram的第一个句子，理论是第一个实例将生成内容主体的令人满意的概述。最受欢迎的前五种2克格式会给出这些要点: 美国宪法是包含授予构成政府的几个部门权力的文书。 《宪法》所设的行政部门负责这一问题。 总政府没有利用各州的任何保留权利。 从退休,我总以为是我生命继续的残渣来填补这个伟大的首席执行官办公室和自由的国家,我出现在你面前,同胞们,把宪法规定的誓言作为其职责的性能的必要条件;我谨按照我国政府的惯例，并按照我认为是你们的期望，向你们简要介绍将指导我履行应尽职责的原则。 印刷机在必要时不得使用政府来“洗清罪行或粉饰罪行”。 当然，它可能不会很快在CliffsNotes上发表，但是考虑到最初的文档有217个句子长，而第4个句子(从退休后调用…)把主要的主题浓缩得相当好，这对第一次来说不算太糟。 对于较长的文本块，或者更多不同的文本，在检索一篇文章中最重要的句子时，查看3-gram甚至4-gram可能是值得的。在这种情况下，只有一个3-gram被多次使用，这是唯一的金属货币很难定义一个总统就职演说。对于较长的段落，使用3-gram可能是合适的。另一种方法是寻找包含最流行的n-gram的句子。当然，这些往往是较长的句子，所以如果这成为一个问题，您可以查找流行n-gram中单词百分比最高的句子，或者结合几种技术创建自己的评分指标。 Markow Models您可能听说过Markov文本生成器。他们已经成为流行的娱乐目的，因为在这可以是我的下一个推特!应用程序，以及他们用来产生真实的垃圾邮件来欺骗检测系统。所有这些文本生成器都基于马尔可夫模型，马尔可夫模型通常用于分析大量随机事件，其中一个离散事件之后是另一个具有一定概率的离散事件。例如，您可以构建天气系统的Markov模型，如图9-1所示。 在这个模型中，每一个晴天都有70%的机会第二天也是晴天，有20%的机会第二天是阴天，只有10%的机会下雨。如果当天下雨，那么第二天下雨的几率为50%，出现太阳的几率为25%，出现云层的几率为25%。 你可能会注意到这个马尔可夫模型中的几个属性: 所有指向任何一个节点的百分比之和必须精确到100%。不管这个系统有多复杂，它总是有100%的机会引领下一步走向其他地方。 虽然在任何给定的时间内，天气只有三种可能，但是您可以使用这个模型生成一个无限的天气状态列表。 只有当前节点的状态才会影响下一个节点的位置。如果你在阳光充沛的节点上，无论之前的100天是晴天还是雨天，第二天出现阳光的几率都是一样的:70%。 到达某些节点可能比到达其他节点更困难。这背后的数学原理相当复杂，但是应该很容易看出，在这个系统中，在任何给定的时间点上，雨天(少于“100%”的箭头指向雨天)比晴天或阴天更不可能达到这种状态。 显然，这是一个简单的系统，马尔可夫模型可以任意变大。谷歌页面排名算法部分基于马尔可夫模型，网站表示为节点，入站/出站链接表示为节点之间的连接。登陆特定节点的可能性表示站点的相对受欢迎程度。也就是说，如果我们的天气系统代表一个非常小的互联网，雨天的页面排名会很低，而多云的页面排名会很高。 记住所有这些，让我们回到一个更具体的例子:分析和编写文本。再次使用前面例子中分析过的William Henry Harrison的就职演说，您可以编写以下代码，根据文本的结构生成任意长的Markov链(链长设置为100) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from urllib.request import urlopenfrom random import randintdef wordListSum(wordList): sum = 0 for word, value in wordList.items(): sum += value return sumdef retrieveRandomWord(wordList): randIndex = randint(1, wordListSum(wordList)) for word, value in wordList.items(): randIndex -= value if randIndex &lt;= 0: return worddef buildWordDict(text): # Remove newlines and quotes text = text.replace('\n', ' '); text = text.replace('"', ''); # Make sure punctuation marks are treated as their own "words," # so that they will be included in the Markov chain punctuation = [',','.',';',':'] for symbol in punctuation: text = text.replace(symbol, ' &#123;&#125; '.format(symbol)); words = text.split(' ') # Filter out empty words words = [word for word in words if word != ''] wordDict = &#123;&#125; for i in range(1, len(words)): if words[i-1] not in wordDict: # Create a new dictionary for this word wordDict[words[i-1]] = &#123;&#125; if words[i] not in wordDict[words[i-1]]: wordDict[words[i-1]][words[i]] = 0 wordDict[words[i-1]][words[i]] += 1 return wordDicttext = str(urlopen('http://pythonscraping.com/files/inaugurationSpeech.txt') .read(), 'utf-8')wordDict = buildWordDict(text)#Generate a Markov chain of length 100length = 100chain = ['I']for i in range(0, length): newWord = retrieveRandomWord(wordDict[chain[-1]]) chain.append(newWord)print(' '.join(chain)) 这段代码的输出每次运行时都会发生变化，但下面是它将生成的荒谬文本的一个例子: 1I consider the operation . Without denying that celebrated Confederacy is a full participation in expressing to bring under a man , even by whatever pretense imposed , and that he is a man , and more wholesome the divorce , I consider it to limit the danger to them his objections . The idea of the public virtue , some of that in our citizens , compensated for a sovereignty acknowledged property of one measure of the evils of a free operations of the spirit which are others . It was wanting no other , and of Representatives with our 函数buildWordDict接收从internet检索到的文本字符串。然后，它会进行一些清理和格式化，删除引号，并在其他标点符号周围加上空格，以便有效地将其视为一个单独的单词。在此之后，它构建了一个二维字典—字典中的字典—其形式如下: 12&#123;word_a : &#123;word_b : 2, word_c : 1, word_d : 1&#125;,word_e : &#123;word_b : 5, word_d : 2&#125;,...&#125; 在这个示例字典中，“word_a”被找到了四次，其中两个实例后面跟着“word_b”，一个实例后面跟着“word_c”，一个实例后面跟着“word_d”。“Word_e”后面跟了7次，“word_b”后面跟了5次，“word_d”后面跟了2次。 如果我们要为这个结果绘制一个节点模型，表示word_a的节点将有50%的箭头指向word_b(四分之二的概率紧随其后)，25%的箭头指向word_c, 25%的箭头指向word_d。 在建立了这个字典之后，它可以用作一个查询表来查看下一步要做什么，无论您使用的是文本中的哪个单词。使用字典的示例字典，您当前可能使用word_e，这意味着您将把字典{word_b: 5, word_d: 2}传递给retrieveRandomWord函数。这个函数依次从字典中检索一个随机单词，并根据它出现的次数进行加权。 通过从一个随机的开始单词开始(在本例中是无所不在的I)，您可以轻松地遍历马尔可夫链，生成任意多的单词。这些马尔可夫链倾向于提高他们的现实主义，因为更多的文本收集，特别是从类似的写作风格的来源。虽然本例使用2- g来创建链(前一个单词预测下一个单词)，但是可以使用3克或更高阶的n-g，其中两个或多个单词预测下一个单词。 虽然很有趣，而且对于您在web抓取期间可能积累的兆字节的文本有很大的用处，但是像这样的应用程序会使您很难看到Markov链的实际一面。正如本节前面提到的，Markov chain模型描述了网站如何从一个页面链接到下一个页面。作为指针的这些链接的大量集合可以形成类似web的图，这些图对于存储、跟踪和分析非常有用。通过这种方式，马尔科夫链为如何考虑web爬行和web爬行器如何思考奠定了基础。 Natural Language Toolkit到目前为止，本章主要集中于文本正文中词语的统计分析。哪些词最受欢迎?哪些词是不寻常的?哪些单词可能会跟在其他单词后面?它们是如何组合在一起的?你所缺少的是理解，在你所能理解的范围内，这些词代表什么。自然语言工具包(NLTK)是一套Python库，用于识别和标记自然英语文本中的部分语音。它的发展始于2000年以来，在过去的15年里，世界各地的几十位开发商都做出了贡献 Statistical Analysis with NLTKNLTK非常适合在文本的各个部分中生成关于单词数量、单词频率和单词多样性的统计信息。如果您所需要的只是一个相对简单的计算(例如，在一段文本中使用的惟一单词的数量)，那么导入nltk可能有些多余——它是一个大型模块。然而，如果您需要对文本进行相对广泛的分析，您手边就有函数，可以提供您想要的任何度量。 NLTK的分析总是从Text对象开始。Text对象可以通过以下方式从简单的Python字符串创建: 1234from nltk import word_tokenizefrom nltk import Texttokens = word_tokenize('Here is some not very interesting text')text = Text(tokens) word_tokenize函数的输入可以是任何Python文本字符串。如果你手头没有长字符串，但仍然想尝试一下这些特性，NLTK的库中已经内置了相当多的书，可以使用导入函数来访问这些书: 1from nltk.book import * 这装载了九本书: 12345678910111213*** Introductory Examples for the NLTK Book ***Loading text1, ..., text9 and sent1, ..., sent9Type the name of the text or sentence to view it.Type: &apos;texts()&apos; or &apos;sents()&apos; to list the materials.text1: Moby Dick by Herman Melville 1851text2: Sense and Sensibility by Jane Austen 1811text3: The Book of Genesistext4: Inaugural Address Corpustext5: Chat Corpustext6: Monty Python and the Holy Grailtext7: Wall Street Journaltext8: Personals Corpustext9: The Man Who Was Thursday by G . K . Chesterton 1908 您将在以下所有示例中使用text6、“巨蟒与圣杯”(1975年电影的剧本)。 文本对象可以像普通Python数组一样进行操作，就好像它们是包含文本单词的数组一样。使用此属性，您可以计算文本中唯一的单词数量，并将其与总单词数量进行比较(请记住Python集合只包含唯一的值): 12&gt;&gt;&gt; len(text6)/len(set(text6))7.833333333333333 上面显示，脚本中的每个单词平均使用了大约8次。您还可以将文本放入频率分布对象中，以确定一些最常见的单词和各种单词的频率: 1234567&gt;&gt;&gt; from nltk import FreqDist&gt;&gt;&gt; fdist = FreqDist(text6)&gt;&gt;&gt; fdist.most_common(10)[(':', 1197), ('.', 816), ('!', 801), (',', 731), ("'", 421), ('[', 319), (']', 312), ('the', 299), ('I', 255), ('ARTHUR', 225)]&gt;&gt;&gt; fdist["Grail"]34 因为这是一个剧本，它是如何写的一些人工制品可以弹出。例如，所有大写字母中的“ARTHUR”经常出现，因为它出现在每个字母之前剧本中亚瑟王的台词。此外，冒号(:)出现在每一行之前，充当字符名称和字符行之间的分隔符。利用这个事实，我们可以看到电影中有1197行! 在前面的章节中，我们称之为2-gram，NLTK指的是bigram，您也可能听到3-gram被称为trigram。你可以创建，搜索，并列出2克非常容易: 12345&gt;&gt;&gt; from nltk import bigrams&gt;&gt;&gt; bigrams = bigrams(text6)&gt;&gt;&gt; bigramsDist = FreqDist(bigrams)&gt;&gt;&gt; bigramsDist[('Sir', 'Robin')]18 要搜索2-gram的”Sir Robin”，您需要将其分解为元组(“Sir”, “Robin”)，以匹配2-gram在频率分布中表示的方式。还有一个trigrams，其工作原理完全相同。对于一般情况，还可以导入ngrams模块 12345&gt;&gt;&gt; from nltk import ngrams&gt;&gt;&gt; fourgrams = ngrams(text6, 4)&gt;&gt;&gt; fourgramsDist = FreqDist(fourgrams)&gt;&gt;&gt; fourgramsDist[('father', 'smelt', 'of', 'elderberries')]1 在这里，调用ngrams函数将文本对象分解为n克，大小不限，由第二个参数控制。在本例中，您将文本分解为4-gram。然后，你可以证明“father smelt of elderberries”这句话在剧本中只出现过一次。 频率分布、文本对象和n-gram也可以在循环中迭代和操作。以下打印出所有以这个单词开头的4-grams单词“椰子”,例如: 123456from nltk.book import *from nltk import ngramsfourgrams = ngrams(text6, 4)for fourgram in fourgrams: if fourgram[0] == 'coconut': print(fourgram) NLTK库有大量的工具和对象，用于组织、计数、排序和度量大量文本。尽管我们仅仅触及了它们的使用的表面，但是这些工具中的大多数都是精心设计的，并且对于熟悉Python的人来说操作起来相当直观 Lexicographical Analysis with NLTK到目前为止，您已经比较并分类了所有遇到的单词，这些单词仅基于它们本身所代表的值。同音异义词和使用这些词的上下文之间没有区别。 尽管有些人可能会认为同音异义词很少有问题，但你可能会对它们出现的频率感到惊讶。大多数以英语为母语的人可能并不经常意识到一个单词是一个同音字，更不用说考虑到它可能会与另一个单词在不同的上下文中混淆。 “He was objective in achieving his objective of writing an objective philosophy, primarilyusing verbs in the objective case”对人类来说很容易分析，但可能会使web scraper认为同一个单词被使用了四次，从而导致它简单地丢弃关于每个单词背后含义的所有信息。 除了找出词性之外，能够区分一个词以一种方式和另一种方式使用可能也很有用。例如，您可能想要查找由普通英语单词组成的公司名称，或者分析某人对公司的看法。“ACME Products is good”和“ACME Products not bad”可以具有相同的基本含义，即使其中一个句子使用“good”，而另一个句子使用“good”“坏”。 Penn Treebank’s Tags NLTK默认使用由宾夕法尼亚大学的Penn Treebank项目。虽然有些标签是有意义的(例如，CC是一个coordinating conjunction（协调的连词）)，但是其他的标签可能会让人混淆(例如，RP是一个particle粒子)。使用以下资料作为本节所述标签的参考: 123456789101112131415161718192021222324252627282930313233343536CC Coordinating conjunctionCD Cardinal numberDT DeterminerEX Existential “there”FW Foreign wordIN Preposition, subordinating conjunctionJJ AdjectiveJJR Adjective, comparativeJJS Adjective, superlativeLS List item markerMD ModalNN Noun, singular or massNNS Noun, pluralNNP Proper noun, singularNNPS Proper noun, pluralPDT PredeterminerPOS Possessive endingPRP Personal pronounPRP$ Possessive pronounRB AdverbRBR Adverb, comparativeRBS Adverb, superlativeRP ParticleSYM SymbolTO “to”UH InterjectionVB Verb, base formVBD Verb, past tenseVBG Verb, gerund or present participleVBN Verb, past participleVBP Verb, non-third-person singular presentVBZ Verb, third person singular presentWDT wh-determinerWP Wh-pronounWP$ Possessive wh-pronounWRB Wh-adverb 了测量语言之外，NLTK还可以根据上下文和它自己的大型字典帮助查找单词中的含义。在基本水平上，NLTK可以识别词类: 12345678910111213141516171819202122&gt;&gt;&gt; from nltk.book import *&gt;&gt;&gt; from nltk import word_tokenize&gt;&gt;&gt; text = word_tokenize('Strange women lying in ponds distributing swords'\'is no basis for a system of government.')&gt;&gt;&gt; from nltk import pos_tag&gt;&gt;&gt; pos_tag(text)[('Strange', 'JJ'), ('women', 'NNS'), ('lying', 'VBG'), ('in', 'IN'), ('ponds', 'NNS'), ('distributing', 'VBG'), ('swords', 'NNS'), ('is', 'VBZ'), ('no', 'DT'), ('basis', 'NN'), ('for', 'IN'), ('a', 'DT'), ('system', 'NN'), ('of', 'IN'), ('government', 'NN'), ('.', '.')] 每个单词都被分成一个元组tuple，其中包含单词和标识词性的标记(有关这些标记的更多信息，请参阅前面的侧栏)。虽然这看起来像是一个简单的查找，但是通过下面的例子，正确执行任务所需的复杂性变得显而易见: 1234567891011text = word_tokenize('The dust was thick so he had to dust')pos_tag(text)[('The', 'DT'), ('dust', 'NN'), ('was', 'VBD'), ('thick', 'RB'), ('so', 'RB'), ('he', 'PRP'), ('had', 'VBD'), ('to', 'TO'), ('dust', 'VB')] 请注意，单词“dust”在句子中使用了两次:一次用作名词，一次用作动词。NLTK根据句子中的上下文正确识别这两种用法。NLTK通过使用由英语定义的上下文无关语法来识别词性。上下文无关语法是一组规则，这些规则定义了在有序列表中哪些内容可以遵循哪些内容。在本例中，它们定义了允许哪些词性跟随哪些词性。当遇到诸如dust之类的歧义词时，就会参考上下文无关语法的规则，并选择符合这些规则的适当词性。 web抓取中的一个常见问题是处理搜索。您可能正在从站点上抓取文本，希望能够搜索单词“谷歌”的实例，但是只有当它用作动词而不是专有名词时才可以。或者，您可能只查找公司谷歌的实例，而不希望依赖于人们正确使用大写字母来查找这些实例。这里，pos_tag函数非常有用: 123456789from nltk import word_tokenize, sent_tokenize, pos_tagsentences = sent_tokenize('Google is one of the best companies in the world. I constantly google myself to see what I\'m up to.')nouns = ['NN', 'NNS', 'NNP', 'NNPS']for sentence in sentences: if 'google' in sentence.lower(): taggedWords = pos_tag(word_tokenize(sentence)) for word in taggedWords: if word[0].lower() == 'google' and word[1] in nouns: print(sentence) 其中，调试过程： 123456789101112&gt;&gt;&gt; pos_tag(word_tokenize(sentence))[('Google', 'NNP'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('best', 'JJS'), ('companies', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('.', '.')] 这只打印包含单词“谷歌”(或“谷歌”)作为某种名词而不是动词的句子。当然，您可以更具体地要求只打印带有“NNP”(专有名词)标记的谷歌实例，但是即使是NLTK有时也会出错，根据应用程序的不同，最好给自己留一点余地。 自然语言的模糊性可以用NLTK的pos_tag函数来解决。通过搜索文本，不仅要搜索目标单词或短语的实例，还要搜索目标单词或短语的实例及其标记，您可以大大提高scraper搜索的准确性和有效性。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[add_password_comment]]></title>
    <url>%2F2019%2F07%2F26%2Fadd-password-comment%2F</url>
    <content type="text"><![CDATA[闲来无事，计划把我的博客在修饰一下，因此添加了文章加密与评论功能。 文章加密文章加密有很多方法，我比较喜欢这个Encrypt，不易破解，设置也简单。 下载插件先在git命令行上输入npm install --save hexo-blog-encrypt，下载插件。 修改配置在最初始的_config.yml上最后添加 1234# Security##encrypt: enable: true 如何使用在创建新的md文件后，格式修改成如下： 123456789---title: testdate: 2019-07-26 14:18:02tags: - hexopassword: mimamimaabstract: Welcome to my blog, enter password to read.message: Welcome to my blog, enter password to read.--- 注意，内容不允许为空 password: 是该博客加密使用的密码 abstract: 是该博客的摘要，会显示在博客的列表页 message: 这个是博客查看时，密码输入框上面的描述性文字 当然，还有许多高级的操作，我的需求就这么多，就不继续研究了，有兴趣可以看官网 评论功能我用的是Valine，首先在LeanCloud进行注册认证，其中认证需要手持身份证什么的，很麻烦。。。 创建应用名字随便起，接着点击设置： 然后在应用Key中进行复制，App ID与App Key两项，一会会用到。 修改配置打开..\themes\next\_config.yml找到valine处，具体设置如下： 123456789101112131415# Valine.# You can get your appid and appkey from https://leancloud.cn# more info please open https://valine.js.orgvaline: enable: true # When enable is set to be true, leancloud_visitors is recommended to be closed for the re-initialization problem within different leancloud adk version. appid: appkey: notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 有话您请说~ # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size visitor: true # leancloud-counter-security is not supported for now. When visitor is set to be true, appid and appkey are recommended to be the same as leancloud_visitors&apos; for counter compatibility. Article reading statistic https://valine.js.org/visitor.html comment_count: true # if false, comment count will only be displayed in post page, not in home page 其中appid与appkey处填写刚刚存好的字符串。 修改网页打开..\themes\next\layout\_partials\comments.swig，因为我只用了这一个评论系统，因此，以下为全部代码： 123456789101112131415161718&#123;% if page.comments %&#125; &lt;div class=&quot;comments&quot; id=&quot;comments&quot;&gt;&lt;/div&gt; &#123;% if (theme.valine and theme.valine.enable)%&#125; &lt;script src=&quot;//cdn1.lncld.net/static/js/3.0.4/av-min.js&quot;&gt;&lt;/script&gt; &lt;script src=&quot;//cdn.jsdelivr.net/npm/valine@1.1.6/dist/Valine.min.js&quot;&gt;&lt;/script&gt; &lt;script&gt; new Valine(&#123; av: AV, el: &apos;.comments&apos;, notify: true, // 邮件提醒 v1.1.4新增，下一步中有具体的邮箱设置 verify: true, app_id: &apos;&#123;&#123; theme.valine.appId &#125;&#125;&apos;, app_key: &apos;&#123;&#123; theme.valine.appKey &#125;&#125;&apos;, placeholder: &apos;ヾﾉ≧∀≦)o来啊，快活啊!&apos; &#125;); &lt;/script&gt; &#123;% endif %&#125;&#123;% endif %&#125; 至此设置完毕。 邮件提醒我是按照某个大佬一步一步走的，很顺利。 在云引擎\设置\代码库中，输入https://github.com/zhaojun1998/Valine-Admin 在云引擎\部署选择 Git源码部署 在云引擎\部署\源码部署\分支或版本号中，输入master，点击部署 在云引擎\设置\自定义环境变量中，添加一些变量名，以此博客为参考，设置如下： SITE_NAME： 网站名称 SITE_URL： 网站地址, 最后不要加/ SMTP_USER：SMTP 服务用户名，一般为邮箱地址。 SMTP_PASS：SMTP 密码，一般为授权码，而不是邮箱的登陆密码，请自行查询对应邮件服务商的获取方式 SENDER_NAME：随意 SMTP_SERVICE：邮件服务提供商，支持 QQ、163、126、Gmail、&quot;Yahoo&quot;、...... TEMPLATE_NAME：有rainbow与default ADMIN_URL： 网站名称 对于163邮箱的SMTP_PASS，寻找方式如下： 进入163邮箱，选择: 对于箭头指的进行勾选，系统会自动让你填写授权码： 休眠策略新版本与更改环境变量均需要重启容器后生效。 部署最新代码 : 重启容器: 免费版的 LeanCloud 容器，是有强制性休眠策略的，不能 24 小时运行： 每天必须休眠 6 个小时 30 分钟内没有外部请求，则休眠。 休眠后如果有新的外部请求实例则马上启动（但激活时此次发送邮件会失败）。 分析了一下上方的策略，如果不想付费的话，最佳使用方案就设置定时器，每天 7 - 23 点每 20 分钟访问一次，这样可以保持每天的绝大多数时间邮件服务是正常的。 首先需要添加环境变量，ADMIN_URL：Web主机域名，如图所示（添加后重启容器才会生效）： 然后点击云引擎 -\定时任务，新增定时器，按照图片上填写,0 */20 7-23 * * ?： 添加后要记得点击启用： 启用成功后，每 20 分钟在云引擎的 - 应用日志中可以看到提示： 如果有后续的更新，鄙人不才，只能看大佬的更新了。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F07%2F26%2Ftest%2F</url>
    <content type="text"><![CDATA[Welcome to this article, please enter password to read. Incorrect Password! No content to display! U2FsdGVkX1+/tx25kThw9cg7J9FgRHOVro9hFZNWS3csAj9xS+a2SvVb2/OfQIVHWR19cyvXTQ3h97nCqPQOdtUb0c+WDgCHM5GXdQUZDYdvP6OpYqxzqnL95ewNUVz2pXps1lcjS2MMj+KY4mx+DmSexPutROgOEqgvwAHJ6u2T1k0x86bxLsGJexlRjuB6qcxbUtYaQBm0G7ryH0aJi9Q1qdUc1GUQEoNqdGvAVIGu6i66DsUzWMgrx0N8R1VO8H9RHgxifjma0jS7XRsLYdX9FfjzZjjGbG+m75LUlbvpPwfUSzcycewHyG7VKMsZEksig8B32w4gsAZ9PLREPKAkJKIDIwxi7148JqEm01grMHZHD3Jd9O1GxHkWEP7WCY7eqeOHBy9c888nGo41uO6KgZy015Yj1iJb0S1dYNSyH8ANLEqbaMCA7WPfmUswPi0aZUP2AhseaJ95gMs+lzqfCqD1IPA/Q8VDqm1Kc6yoaeW/YlvfiOsRxRNblHXsn2beu7Q7Vkvvb6FRD6zhNMxIJiST3MoILRJ88vv3ymHmHR1+lXIoEYy1nLpP1c+LpdvAchXKs/zNyoHqIA98IlQ6Wr84SkNJRi51vXohmMwUySIAj9yrI4y3IaQw9tUN0AXJtA4FUjkdh+PNrJsDHmfwyBkSJLmHUcc0rPbnyyu9VTxPS7p1dMmZAyOAuF1YOdliuZTzHFeFoIqUeQ2Y4KSG1YxrTrIqSTtNMQuiKgS9D+rBLndDJhvKT2pxILLec6JEeYJurNy2AQZO2R3A1EMN9LWLBGCt6hamI4fyKSk=]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-8]]></title>
    <url>%2F2019%2F07%2F25%2FWebScraping8%2F</url>
    <content type="text"><![CDATA[到目前为止，在本书中，您已经忽略了格式糟糕的数据的问题，而是使用了一般格式良好的数据源，如果数据偏离了您的预期，则完全删除数据。但通常，在web抓取中，您不能对数据的来源或外观过于挑剔。由于标点符号错误、大小写不一致、换行符和拼写错误，脏数据在web上可能是一个大问题。本章将介绍一些工具和技术，通过改变编写代码的方式，以及在数据库中清理数据，帮助您从源头上预防问题。 Cleaning in Code在语言学中，n-gram是在文本或言语中使用的n个单词的序列。在进行自然语言分析时，通过查找常用的n-gram或经常一起使用的重复单词集，通常可以方便地分解一段文本。 本节的重点是获取格式正确的n-gram，而不是使用它们进行任何分析。稍后，在第9章中，您可以看到2-gram和3-gram在执行文本摘要和分析。下面返回了在Wikipedia关于Python编程语言的文章中找到的2克的列表: 12345678910111213141516from urllib.request import urlopenfrom bs4 import BeautifulSoupdef getNgrams(content, n): content = content.split(' ') output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputhtml = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')bs = BeautifulSoup(html, 'html.parser')content = bs.find('div', &#123;'id':'mw-content-text'&#125;).get_text()ngrams = getNgrams(content, 2)print(ngrams)print('2-grams count is: '+str(len(ngrams))) getNgrams函数接受一个输入字符串，并将其分解为一系列单词(假设所有单词都用空格分隔)，并将每个单词开头的n-gram(在本例中为2- gram)添加到数组中。 这将从文本中返回一些真正有趣和有用的2- gram： 1['of', 'free'], ['free', 'and'], ['and', 'open-source'], ['open-source', 'software'] 但它也会返回很多垃圾： 1['software\nOutline\nSPDX\n\n\n\n\n\n\n\n\nOperating', 'system\nfamilies\n\n\n\nAROS\nBSD\nDarwin\neCos\nFreeDOS\nGNU\nHaiku\nInferno\nLinux\nMach\nMINIX\nOpenSolaris\nPlan'], ['system\nfamilies\n\n\n\nAROS\nBSD\nDarwin\neCos\nFreeDOS\nGNU\nHaiku\nInferno\nLinux\nMach\nMINIX\nOpenSolaris\nPlan', '9\nReactOS\nTUD:OS\n\n\n\n\n\n\n\n\nDevelopment\n\n\n\nBasic'], ['9\nReactOS\nTUD:OS\n\n\n\n\n\n\n\n\nDevelopment\n\n\n\nBasic', 'For'] 此外，由于为遇到的每个单词都创建了2-gram(除了最后一个单词)，所以在撰写本文时，本文中有7,411个2-gram。不是一个非常容易管理的数据集！ 使用正则表达式删除转义字符(比如\n)和过滤删除任何Unicode字符，您可以稍微清理一下输出： 123456789101112131415161718import redef getNgrams(content, n): content = re.sub('\n|[[\d+\]]', ' ', content) content = bytes(content, 'UTF-8') content = content.decode('ascii', 'ignore') content = content.split(' ') content = [word for word in content if word != ''] output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputhtml = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')bs = BeautifulSoup(html, 'html.parser')content = bs.find('div', &#123;'id':'mw-content-text'&#125;).get_text()ngrams = getNgrams(content, 2)print(ngrams)print('2-grams count is: '+str(len(ngrams))) 这将用空格替换换行符的所有实例，删除诸如[123]之类的引用，并过滤行中多个空格导致的所有空字符串。然后，使用UTF-8编码内容，消除转义字符。 需要解释以下 1content = re.sub('\n|[[\d+\]]', ' ', content) 其中，\n|[[\d+\]]或前面是换行字符，后面的[[\d+\]]是说的是\[|\d+|\]这三个相或，为什么只有]前面加一个\这个是规定，表示对这个字符进行操作，但为什么[没有这样的讲究，只能说规定如此，而且我觉得[[\d+\]\n]也是等价的。 这些步骤大大提高了函数的输出，但仍存在一些问题： 1[&apos;years&apos;, &apos;ago(&apos;], [&apos;ago(&apos;, &apos;-&apos;], [&apos;-&apos;, &apos;-&apos;], [&apos;-&apos;, &apos;)&apos;], [&apos;)&apos;, &apos;Stable&apos;] 您可以通过删除每个单词前后的所有标点符号来改进这一点标点符号(剥离)。这保留了单词中的连字符，但是消除了空字符串之后只包含一个标点符号的字符串。 当然，标点符号本身是有意义的，简单地去掉它可能会丢失一些有价值的信息。例如，句点后面的空格可以被认为是一个完整句子或语句的结尾。您可能想要禁止n-gram在这样的stop上桥接，并且只考虑在句子中创建的那些。 例如，给定文本： 1Python features a dynamic type system and automatic memory management. It supports multiple programming paradigms... 2-gram[‘memory’, ‘management’]是有效的，但是2-gram[‘management’, ‘It’]无效。 现在，您有了一个更长的“清洁任务”列表，您正在介绍“清洁任务”的概念“句子”，你的程序已经变得越来越复杂，最好把它们移到四个不同的函数中: 12345678910111213141516171819202122232425262728293031323334353637from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringdef cleanSentence(sentence): sentence = sentence.split(' ') sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence] sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == 'a' or word.lower() == 'i')] return sentencedef cleanInput(content): content = content.upper() content = re.sub('\n|[[\d+\]]', ' ', content) content = bytes(content, "UTF-8") content = content.decode("ascii", "ignore") sentences = content.split('. ') return [cleanSentence(sentence) for sentence in sentences]def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputdef getNgrams(content, n): content = cleanInput(content) ngrams = [] for sentence in content: ngrams.extend(getNgramsFromSentence(sentence, n)) return(ngrams)html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')bs = BeautifulSoup(html, 'html.parser')content = bs.find('div', &#123;'id':'mw-content-text'&#125;).get_text()print(getNgrams(content, 2))print(len(getNgrams(content, 2))) 其中，.strip()，是只能删除开头或是结尾的字符，不能删除中间部分的字符。 getNgrams仍然是您进入程序的基本入口点。cleanInput和前面一样删除了换行和引用，但也根据句点和空格的位置将文本分割为“句子”。它还调用了cleanSentence，它将句子分成单词，去掉标点符号和空格，并删除除I和a之外的单个字符。 创建n个gram的关键行被移动到getNgramsFromSentence中，getngram对每个句子调用这个函数。这确保不会创建跨多个句子的n-gram。注意string.punctuation和string.whitespace的使用。获取Python中所有标点符号的列表。您可以查看字符串的输出。Python终端中的标点符号: 123&gt;&gt;&gt; import string&gt;&gt;&gt; print(string.punctuation)!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`&#123;|&#125;~ 通过在循环中遍历内容中的所有单词时使用item.strip(string. +string.whitespace)，单词两边的任何标点符号都将被删除，而连字符(标点符号被两边的字母包围)将保持不变。 这种努力的结果是更干净的2-gram： 1[['Python', 'Paradigm'], ['Paradigm', 'Object-oriented'], ['Object-oriented','imperative'], ['imperative', 'functional'], ['functional', 'procedural'],['procedural', 'reflective'],... Data Normalization每个人都遇到过设计糟糕的web表单:“输入您的电话号码。您的电话号码必须是‘xxx- xxxx -xxxx’格式。” 作为一名优秀的程序员，您可能会想，为什么他们不去掉我输入的非数字字符，然后自己来做呢?数据规范化是确保在语言或逻辑上彼此等价的字符串(如电话号码(555)123-4567和555.123.4567)显示为等价字符串，或至少将其进行比较的过程。 使用上一节中的n-gram代码，您可以添加数据规范化特性。这段代码的一个明显问题是它包含许多重复的2-gram。它每遇到2-gram就会被添加到列表中，没有记录它的频率。记录这些2-gram的频率(而不仅仅是它们的存在)不仅很有趣，而且在绘制清洗和数据归一化算法更改的效果时也很有用。如果数据被成功归一化，那么唯一n-gram的总数将减少，而找到的n-gram总数(即，确定为n—gram的唯一或非唯一项的数目)不会减少。换句话说，对于相同数量的n克，“木桶”会更少。 您可以通过修改收集n个g的代码来实现这一点，将它们添加到计数器对象而不是列表中： 123456789101112from collections import Counterdef getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams)print(getNgrams(content, 2)) 还有许多其他方法可以做到这一点，比如向dictionary对象添加n-grams，其中list的值按其被看到的次数计数。这有一个缺点，它需要更多的管理，并使排序变得棘手。但是，使用计数器对象也有一个缺点：它不能存储列表(列表是不可缓存的)，所以您需要首先在每个n-gram的列表理解中使用’ ‘.join(n-gram)将它们转换为字符串。 这里是结果： 1Counter(&#123;&apos;Python Software&apos;: 37, &apos;Software Foundation&apos;: 37, &apos;of the&apos;: 34,&apos;of Python&apos;: 28, &apos;in Python&apos;: 24, &apos;in the&apos;: 23, &apos;van Rossum&apos;: 20, &apos;to the&apos;:20, &apos;such as&apos;: 19, &apos;Retrieved February&apos;: 19, &apos;is a&apos;: 16, &apos;from the&apos;: 16,&apos;Python Enhancement&apos;: 15,... 在撰写本文时，总共有7275个2克，5628个独特的2克，其中最流行的2克是“软件基础”，其次是“Python Software.””。然而，对结果的分析表明，“Python Software.””以“Python软件”另外两次。同样的，” vanRossum “和” vanRossum”单独出现在列表中。 添加一行: 1content = content.upper() 对于cleanInput函数，保持找到的2-gram的总数稳定在7,275，同时将惟一的2-gram的数量减少到5,479。 除此之外，通常最好停下来，考虑一下需要花费多少计算能力来规范化数据。在许多情况下，单词的不同拼写是等价的，但是为了解决这种等价性，您需要检查每个单词，看看它是否匹配任何预编程的等价。 例如，Python first和Python 1st都出现在2-gram的列表中。然而，如果要制定一个总括规则，即all first、second、third等等都将被解析为1st、2nd、3rd等等(反之亦然)，那么每个单词将会被额外检查10次左右。同样，不一致地使用连字符(coordination对co-ordination)、拼写错误和其他自然语言的不一致也会影响n-gram的分组，如果这些不一致足够常见，可能会混淆输出的结果。对于带连字符的单词，一种解决方案可能是完全删除连字符，将单词视为一个字符串，这只需要一个操作。然而，这也意味着连字符短语(非常常见的现象)将被视为一个单词。走另一条路，将连字符作为空格可能是更好的选择。只是为偶尔的配合和配合攻击做好准备。 Cleaning After the Fact在代码中，您只能(或想)做这么多。此外，您可能正在处理一个没有创建的数据集，或者一个甚至在没有先查看数据集的情况下就知道如何清理的数据集。许多程序员在这种情况下的下意识反应是编写一个脚本，这可能是一个很好的解决方案。然而，第三方工具，如OpenRefine，不仅能够快速轻松地清理数据，而且允许非程序员轻松地查看和使用数据。 OpenRefineOpenRefine是一个开源项目，由Metaweb公司于2009年启动。谷歌于2010年收购Metaweb，将项目名称从Freebase Gridworks改为谷歌Refine。2012年，谷歌放弃了对Refine的支持，再次将名称更改为OpenRefine，欢迎任何人为项目的开发做出贡献。 要使用OpenRefine，需要将数据保存为CSV文件，或者，如果您将数据存储在数据库中，则可以将其导出到CSV文件。 Using OpenRefine在下面的例子中，您将使用从Wikipedia的“比较”中提取的数据文本编辑器”表;见图8 - 1。虽然这个表的格式相对较好，但是它包含了人们在很长一段时间内进行的许多编辑，因此它有一些格式上的小矛盾。此外，由于它的数据是由人而不是机器读取的，所以一些格式选择(例如，使用“Free”而不是“$0.00”)不适合编程输入。 关于OpenRefine要注意的第一件事是，每个列标签旁边都有一个箭头。此箭头提供了一个工具菜单，可与该列一起用于筛选、排序、转换或删除数据。 Filtering 数据过滤可以使用两种方法执行:Filters和facets。过滤器适用于使用正则表达式对数据进行过滤;例如，在“编程语言”列中，只显示包含三种或更多逗号分隔的编程语言的数据，如图8-2所示。可以通过操作右栏中的块轻松地组合、编辑和添加过滤器。它们还可以与facets组合。 可以通过操作右栏中的块轻松地组合、编辑和添加过滤器。它们还可以与facet组合。 facest非常适合包含或排除基于列的整个内容的数据。(例如，“显示所有使用GPL或MIT许可的行，这些行在2005年后首次发布”，如图8-3所示)。他们有内置的过滤工具。例如，对数值进行筛选可以使用幻灯片条选择要包含的值范围。 无论您如何过滤数据，它都可以在任何时候导出到OpenRefine支持的几种格式之一。这包括CSV, HTML(一个HTML表)，Excel和其他几种格式。 cleaning 只有在开始时数据相对干净时，才能成功地进行数据过滤。例如，在前一节的facet示例中，“First public release”facet中不会选择发布日期为01-01-2006的文本编辑器，该文本编辑器正在寻找一个值为2006的值，并忽略了看起来不像这个值的值。 使用OpenRefine表达式在OpenRefine中执行数据转换语言，称为GREL (G是OpenRefine的前一个名称遗留下来的，谷歌优化)。此语言用于创建基于简单规则转换单元格中的值的短lambda函数。例如: 1if(value.length() != 4, &quot;invalid&quot;, value) 当这个函数应用于“第一个稳定版本”列时，它将保存日期为YYYY格式的单元格的值，并将所有其他列标记为无效(图8-4)。 可以通过单击任何列标签旁边的下箭头并选择Edit cells$\rightarrow$Transform来应用任意的GREL语句。然而，将所有不太理想的值标记为无效，同时使它们易于发现，这对您没有多大好处。如果可能，最好尝试从格式糟糕的值中回收信息。这可以通过使用GREL的match函数来实现： 1value.match(&quot;.*([0-9]&#123;4&#125;).*&quot;).get(0) 这将尝试将字符串值与给定的正则表达式匹配。如果正则表达式与字符串匹配，则返回一个数组。任何与正则表达式中的捕获组匹配的子字符串(由表达式中的括号分隔，在本例中为[0-9]{4})都作为数组值返回。实际上，这段代码将在一行中找到四个小数的所有实例，并返回第一个实例。这通常足以从文本或格式糟糕的日期中提取年份。它还具有为不存在的日期返回null的优点。(在对空变量执行操作时，GREL不会抛出空指针异常。)使用单元格编辑和GREL可以进行许多其他数据转换。在OpenRefine的GitHub页面上可以找到该语言的完整指南。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-7]]></title>
    <url>%2F2019%2F07%2F24%2FWebScraping7%2F</url>
    <content type="text"><![CDATA[本章将介绍如何处理文档，无论您是将它们下载到本地文件夹，还是阅读它们并提取数据。您还将了解如何处理各种类型的文本编码，这甚至可以使阅读外语成为可能HTML页面。 人们很容易认为互联网主要是一个基于文本的网站集合，其中点缀着新颖的web 2.0多媒体内容，这些内容在web抓取的目的中基本上可以忽略。然而，这忽略了internet最基本的特性:传输文件的内容无关工具。 尽管互联网自20世纪60年代末以来就以某种形式存在，HTML直到1992年才首次出现。在那之前，互联网主要由电子邮件和文件传输组成;我们今天所知道的web页面的概念并不存在。换句话说，internet不是HTML文件的集合。它是许多类型的文档的集合，HTML文件通常用作展示它们的框架。由于无法读取各种文档类型，包括文本、PDF、图像、视频、电子邮件等，我们将丢失大量可用数据。 Document Encoding文档的编码告诉应用程序——无论它们是您计算机的操作系统还是您自己的Python代码——如何读取它。这种编码通常可以从它的文件扩展名推断出来，尽管这个文件扩展名不是由它的编码强制的。例如，我可以毫无问题地将myImage.jpg保存为myImage.txt—至少在我的文本编辑器尝试打开它之前是这样。幸运的是，这种情况很少见，为了正确读取文档，通常只需要知道文档的文件扩展名。 在基本级别上，所有文档都是用0和1编码的。最重要的是，编码算法定义了诸如“每个字符有多少位”或“每个像素有多少位代表颜色”之类的东西(对于图像文件)。除此之外，您可能还有一层压缩，或者一些空间缩减算法，就像PNG文件的情况一样。 尽管一开始处理非html文件可能看起来有些吓人，但是请放心，使用正确的库，Python将能够正确地处理任何格式的信息。文本文件、视频文件和图像文件之间的唯一区别是如何解释它们的0和1。本章介绍几种常见的文件类型:文本、CSV、pdf和Word文档。 注意，这些基本上都是存储文本的文件。有关处理图像的信息，我建议您通读本章，以便习惯处理和存储不同类型的文件，然后阅读第13章，了解关于图像处理的更多信息! Text将文件以纯文本形式存储在网上有些不同寻常，但在一些基本的网站或老式网站中，拥有大型文本文件存储库很受欢迎。例如，Internet Engineering Task Force (IETF)将其所有已发布的文档存储为HTML、PDF和文本文件(参见https://www.ietf.org/rfc/rfc1149.txt)。大多数浏览器都会很好地显示这些文本文件，您应该能够毫无问题地抓取它们。 1234from bs4 import BeautifulSoupfrom urllib.request import urlopentextPage = urlopen('http://www.pythonscraping.com/pages/warandpeace/chapter1.txt')print(textPage.read()) 通常，当您使用urlopen检索页面时，您将它转换为一个BeautifulSoup对象来解析HTML。在这种情况下，您可以直接读取页面。虽然完全有可能将其转换为一个BeautifulSoup对象，但这只会适得其反——因为没有HTML要解析，所以库将毫无用处。一旦文本文件以字符串的形式读入，您只需像将任何其他字符串读入Python那样分析它。当然，这里的缺点是您没有能力使用HTML标记作为上下文线索，指向实际文本的方向 Text Encoding and the Global Internet还记得我之前说过，正确读取文件只需要一个文件扩展名吗？奇怪的是，这个规则并不适用于所有最基本的文档:.txt文件。10次中有9次，使用前面描述的方法阅读文本会很好。然而，处理互联网上的文本可能是一件棘手的事情。接下来，我们将介绍英语和外语编码的基础知识，从ASCII到Unicode到ISO，以及如何处理它们。 a history of text encoding ASCII最早出现在20世纪60年代，当时比特非常昂贵，除了拉丁字母和一些标点符号外，没有理由对任何东西进行编码。由于这个原因，总共只有7位被用于编码128个大写字母、小写字母和标点符号。即使有这么多的创意，他们仍然留下了33个非打印字符，其中一些被使用，取代，和/或成为过时的技术变化多年。每个人都有足够的空间，对吧? 任何程序员都知道，7是一个奇怪的数字。它不是一个很好的2次方，但是它非常接近。在20世纪60年代，计算机科学家们就是否应该增加额外的位进行了争论，到底是为了方便得到一个漂亮的整数，还是为了实际的文件需要更少的存储空间。最后，7位赢了。然而，在现代计算中，每个7位序列的开头都加了一个额外的0，这就给我们留下了两个世界中最糟糕的情况——14%的文件变大了，而且只缺少128个字符的灵活性。 在20世纪90年代初，人们意识到存在着比英语更多的语言，如果电脑能显示这些语言那就太好了。一个名为Unicode联盟试图通过为任何文本文档、任何语言中需要使用的每个字符建立编码来实现一个通用的文本编码器。目标是包括从拉丁字母编写的这本书是,西里尔,中国象形图,数学和逻辑符号，甚至表情符号和各种各样的符号,如生物危害和标志。 您可能已经知道，生成的编码器被命名为UTF-8，它的意思是“通用字符集转换格式8位”。这里的8位并不是指每个字符的大小，而是指一个字符需要显示的最小大小。 UTF-8字符的实际大小是灵活的。它们的范围从1字节到4字节，这取决于它们在可能的字符列表中的位置(更流行的字符用更少的字节编码，更不常见的字符需要更多的字节)。 如何实现这种灵活的编码？使用7位加上最终无用的前导0起初看起来像是ASCII中的一个设计缺陷，但事实证明UTF-8具有巨大的优势。由于ASCII非常流行，Unicode决定利用这个领先的0位，以0开头声明所有字节，以表示字符中只使用一个字节，并使ASCII和UTF-8的两种编码方案相同。因此，以下字符在UTF-8和ASCII中都是有效的。 12301000001 - A01000010 - B01000011 - C 以下字符仅在UTF-8中有效，如果将文档解释为ASCII文档，则将呈现为不可打印： 除了UTF-8之外，还存在其他UTF标准，如UTF-16、UTF-24和UTF-32，尽管以这些格式编码的文档很少出现，除非在不寻常的情况下，这超出了本书的范围。 虽然ASCII的这个原始设计缺陷对UTF-8有一个主要的优势，但是这个劣势并没有完全消失。每个字符的前8位信息仍然只能编码128($2^7$)个字符，而不能编码完整的256个字符。在需要多个字节的UTF-8字符中，额外的前导位不是用于字符编码，而是用于防止损坏的检查位。在4字节字符中的32位(8 $\times$ 4)中，只有21位用于字符编码，总共有2,097,152个可能的字符，其中当前分配了1,114,112个。当然，所有通用语言编码标准的问题在于，任何用一种外语编写的文档都可能比必须的大得多。虽然您的语言可能只有100个左右的字符，但是每个字符需要16位，而不是像特定于英语的ASCII那样只需要8位。这使得UTF-8中的外语文本文档的大小大约是英语文本文档的两倍，至少对于不使用拉丁字符集的外语来说是这样。 ISO通过为每种语言创建特定的编码来解决这个问题。与Unicode一样，它具有与ASCII相同的编码，但是在每个字符的开头使用填充0位，以便为所有需要它们的语言创建128个特殊字符。这对于严重依赖拉丁字母(在编码中仍处于0 127的位置)，但需要额外特殊字符的欧洲语言最有效。这允许ISO-8859-1**** Encodings in action 在上一节中，您使用了urlopen的默认设置来读取可能在internet上遇到的文本文档。这对大多数英语文本都很有用。然而，当你遇到俄语、阿拉伯语，甚至像“resume”这样的单词时，你可能会遇到问题。 以下面的代码为例: 123from urllib.request import urlopentextPage = urlopen('http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt')print(textPage.read()) 这读在第一章的原始战争与和平(写在俄罗斯和并将其打印到屏幕上。这个屏幕文本的部分内容如下: 1b&quot;\xd0\xa7\xd0\x90\xd0\xa1\xd0\xa2\xd0\xac \xd0\x9f\xd0\x95\xd0\xa0\xd0\x92\xd0\x90\xd0\xaf\n\nI\n\n\xe2\x80\x94 Eh bien, mon prince. 此外，在大多数浏览器中访问这个页面会导致胡言乱语。 即使对母语为俄语的人来说，这也可能有点难以理解。问题是Python试图将文档读取为ASCII文档，而浏览器则试图将其读取为ISO-8859-1编码的文档。当然，双方都没有意识到这是一个UTF-8文档。您可以显式地将字符串定义为UTF-8，它可以正确地将输出格式化为Cyrillic字符: 12345from urllib.request import urlopentextPage = urlopen( 'http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt')print(str(textPage.read(), 'utf-8')) 在BeautifulSoup和Python 3.x中使用这个概念是这样的: 1234567from urllib.request import urlopenhtml = urlopen("http://en.wikipedia.org/wiki/Python_(programming_language)")bs = BeautifulSoup(html, "html.parser")content = bs.find("div", &#123;"id":"mw-content-text"&#125;).get_text()content = bytes(content, "UTF-8")content = content.decode("UTF-8")print(content) Python 3.默认情况下，将所有字符编码为UTF-8。您可能会忍不住不去管它，而是为您编写的每个web scraper使用UTF-8编码。毕竟，UTF-8还可以流畅地处理ASCII字符和外语。然而，重要的是要记住9%的网站使用的是ISO编码也一样，所以你永远无法完全避免这个问题。 幸运的是，对于HTML页面，编码通常包含在站点的部分中找到的标记中。大多数网站，尤其是英语网站，都有这样的标签： 1&lt;meta charset="utf-8" /&gt; 而ECMA国际的网站上有这个标签： 1&lt;META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1"&gt; 如果您计划进行大量的web抓取，特别是国际站点，那么在阅读页面内容时，最好查找这个元标记并使用它推荐的编码。 CSV当web抓取时，您可能会遇到CSV文件或喜欢这种格式的数据的同事。幸运的是，Python有一个非常棒的库，可以读取和写入CSV文件。尽管这个库能够处理CSV的许多变体，但本节主要关注标准格式。如果您有特殊情况需要处理，请参考文档! Reading CSV Files Python的csv库主要用于处理本地文件，假设您需要的csv数据存储在您的机器上。不幸的是，情况并非总是如此，尤其是在web抓取时。有几种方法可以解决这个问题: 手工下载文件并将Python指向本地文件位置 编写一个Python脚本来下载文件、读取文件，并(可选地)在检索后删除文件。 从web中以字符串的形式检索该文件，并将该字符串包装在StringIO对象中，这样它的行为就像一个文件。 虽然前两个选项是可行的，但是当您可以轻松地将文件保存在内存中时，占用硬盘空间是一种不好的做法。更好的方法是将文件读入字符串并将其封装在一个对象中，该对象允许Python将其视为文件，而无需保存文件。下面的脚本从internet检索CSV文件(在本例中，Monty Python相册列表位于http://pythonscraping.com/files/Monty‐PythonAlbums.csv然后一行一行地打印到终端: 1234567891011from urllib.request import urlopenfrom io import StringIOimport csvdata = urlopen('http://pythonscraping.com/files/MontyPythonAlbums.csv').read().decode('ascii', 'ignore')dataFile = StringIO(data)csvReader = csv.reader(dataFile)for row in csvReader: print(row) print("The album \""+row[0]+"\" was released in "+str(row[1])) 输出类似如下： 12345678[&apos;Name&apos;, &apos;Year&apos;]The album &quot;Name&quot; was released in Year[&quot;Monty Python&apos;s Flying Circus&quot;, &apos;1970&apos;]The album &quot;Monty Python&apos;s Flying Circus&quot; was released in 1970[&apos;Another Monty Python Record&apos;, &apos;1971&apos;]The album &quot;Another Monty Python Record&quot; was released in 1971[&quot;Monty Python&apos;s Previous Record&quot;, &apos;1972&apos;]... 注意第一行:专辑“Name”是在那年发行的。虽然在编写示例代码时，这可能是一个容易忽略的结果，但您不希望在现实世界中将其放入数据中。级别较低的程序员可能只是跳过csvReader对象中的第一行，或者用特殊的情况编写来处理它。幸运的是，csv的替代品。reader函数会自动为您处理所有这些。输入DictReader: 123456789101112from urllib.request import urlopenfrom io import StringIOimport csvdata = urlopen("http://pythonscraping.com/files/MontyPythonAlbums.csv").read().decode('ascii', 'ignore')dataFile = StringIO(data)dictReader = csv.DictReader(dataFile)print(dictReader.fieldnames)for row in dictReader: print(row) 输出类似如下： 1234567[&apos;Name&apos;, &apos;Year&apos;]OrderedDict([(&apos;Name&apos;, &quot;Monty Python&apos;s Flying Circus&quot;), (&apos;Year&apos;, &apos;1970&apos;)])OrderedDict([(&apos;Name&apos;, &apos;Another Monty Python Record&apos;), (&apos;Year&apos;, &apos;1971&apos;)])OrderedDict([(&apos;Name&apos;, &quot;Monty Python&apos;s Previous Record&quot;), (&apos;Year&apos;, &apos;1972&apos;)])OrderedDict([(&apos;Name&apos;, &apos;The Monty Python Matching Tie and Handkerchief&apos;), (&apos;Year&apos;, &apos;1973&apos;)])OrderedDict([(&apos;Name&apos;, &apos;Monty Python Live at Drury Lane&apos;), (&apos;Year&apos;, &apos;1974&apos;)])... csv.DictReader返回CSV文件中每一行的值作为dictionary对象而不是list对象，字段名存储在变量DictReader中。字段名和键在每个dictionary对象: 当然，与csvReader相比，它的缺点是创建、处理和打印这些DictReader对象所需的时间稍微长一些，但是其方便性和可用性通常值得额外的开销。也要记住,当涉及到web抓取请求所需的开销和检索网站数据从外部服务器几乎总是会不可避免的限制因素在任何程序编写,所以担心技术可能刮微秒你总运行时通常是一个有争议的问题！ PDF作为一名Linux用户，我知道被发送一个.docx文件的痛苦，我的非微软软件把这个文件弄得一团糟，而且我还在努力寻找代码编解码器来解释一些新的苹果媒体格式。在某种程度上，Adobe在1993年创建其可移植文档格式方面是革命性的。pdf允许不同平台上的用户以完全相同的方式查看图像和文本文档，而不管他们是在哪个平台上查看的。尽管在web上存储pdf有点过时(当您可以将其编写为HTML时，为什么要以静态、慢加载的格式存储内容?)，但是pdf仍然无处不在，尤其是在处理官方表单和归档时。2009年，一位名叫尼克·英尼斯(Nick Innes)的英国人因为要求白金汉郡市议会提供公开的学生考试成绩信息而上了新闻。根据英国版的《信息自由法》(Freedom of information Act)，白金汉郡市议会可以获得学生的考试成绩信息。在多次请求和拒绝之后，他终于收到了184份PDF文件，这是他要找的资料。尽管Innes坚持了下来，并最终获得了一个格式更合适的数据库，但如果他是一个专业的web scraper，他很可能会在法庭上节省很多时间，直接使用PDF文档，使用Python的许多PDF解析模块之一。 不幸的是，许多pdf解析库都是为python2构建的。没有随着Python 3.x的发布而升级。不过，因为PDF是相对而言的简单和开放源码的文档格式，许多体面的Python库，甚至在Python 3.x，可以读出来。 PDFMiner3K就是这样一个相对容易使用的库。它是灵活的，允许命令行使用或集成到现有代码中。它还可以处理各种语言编码——同样，这在web上经常派上用场。 这里是一个基本的实现，允许你读取任意的pdf到一个字符串，给定一个本地文件对象： 这个PDF阅读器的好处是，如果你在本地处理文件，你可以用一个普通的Python文件对象替换urlopen返回的文件对象，并使用下面这行代码: 1pdfFile = open('../pages/warandpeace/chapter1.pdf', 'rb') 输出可能并不完美，特别是对于带有图像、格式奇怪的文本或表或图表中的文本的pdf。但是，对于大多数纯文本PDF，输出应该与PDF是文本文件时的输出没有什么不同。 Microsoft Word and .docx冒着冒犯微软朋友的风险:我不喜欢微软Word。不是因为它一定是一个坏软件，而是因为它的用户滥用它的方式。它有一种特殊的天赋，可以将原本应该是简单的文本文档或pdf文件转换成大型、缓慢、难以打开的文件，这些文件在不同机器之间常常会失去所有格式，而且由于某种原因，当内容通常是静态的时候，这些文件是可以编辑的。Word文件是为内容创建而设计的，而不是为内容共享而设计的。然而，它们在某些网站上是无处不在的，包括重要的文件、信息，甚至图表和多媒体;简而言之，所有可以而且应该用HTML创建的东西。大约在2008年以前，Microsoft Office产品使用专有的.doc文件格式。这种二进制文件格式很难阅读，而且其他文字处理程序也不支持这种格式。为了顺应时代潮流，采用许多其他软件都使用的标准，微软决定使用基于xml的Open Office标准，该标准使文件与开源和其他软件兼容。不幸的是，Python对这种文件格式的支持(谷歌Docs、Open Office和Microsoft Office都使用这种格式)仍然不是很好。有python-docx库，但这只允许用户创建文档并只读取基本的文件数据，比如文件的大小和标题，而不是实际内容。要读取Microsoft Office文件的内容，您需要滚动自己的解决方案。 第一步是从文件中读取XML: 123456789101112131415from zipfile import ZipFilefrom urllib.request import urlopenfrom io import BytesIOfrom bs4 import BeautifulSoupwordFile = urlopen('http://pythonscraping.com/pages/AWordDocument.docx').read()wordFile = BytesIO(wordFile)document = ZipFile(wordFile)xml_content = document.read('word/document.xml')wordObj = BeautifulSoup(xml_content.decode('utf-8'), 'xml')textStrings = wordObj.find_all('w:t')for textElem in textStrings: print(textElem.text) 它将远程Word文档读取为二进制文件对象(BytesIO类似于使用Python的核心zipfile库解压缩它(所有.docx文件都被压缩以节省空间)，然后读取未压缩的文件XML。 读取我的简单Word文档的Python脚本输出如下: 1234567891011121314151617181920212223242526272829303132333435&lt;!--?xml version="1.0" encoding="UTF-8" standalone="yes"?--&gt;&lt;w:document mc:ignorable="w14 w15 wp14" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math" xmlns:mc="http://schemas.openxmlformats.org/markup-compatibility/2006" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml" xmlns:w15="http://schemas.microsoft.com/office/word/2012/wordml" xmlns:wne="http://schemas.microsoft.com/office/word/2006/wordml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:wp14="http://schemas.microsoft.com/office/word/2010/wordprocessingDrawing" xmlns:wpc="http://schemas.microsoft.com/office/word/2010/wordprocessingCanvas" xmlns:wpg="http://schemas.microsoft.com/office/word/2010/wordprocessingGroup" xmlns:wpi="http://schemas.microsoft.com/office/word/2010/wordprocessingInk" xmlns:wps="http://schemas.microsoft.com/office/word/2010/wordprocessingShape"&gt;&lt;w:body&gt;&lt;w:p w:rsidp="00764658" w:rsidr="00764658" w:rsidrdefault="00764658"&gt;&lt;w:ppr&gt;&lt;w:pstyle w:val="Title"&gt;&lt;/w:pstyle&gt;&lt;/w:ppr&gt;&lt;w:r&gt;&lt;w:t&gt;A Word Document on a Website&lt;/w:t&gt;&lt;/w:r&gt;&lt;w:bookmarkstart w:id="0" w:name="_GoBack"&gt;&lt;/w:bookmarkstart&gt;&lt;w:bookmarkend w:id="0"&gt;&lt;/w:bookmarkend&gt;&lt;/w:p&gt;&lt;w:p w:rsidp="00764658" w:rsidr="00764658" w:rsidrdefault="00764658"&gt;&lt;/w:p&gt;&lt;w:p w:rsidp="00764658" w:rsidr="00764658" w:rsidrdefault="00764658" w:rsidrpr="00764658"&gt;&lt;w: r&gt; &lt;w:t&gt;This is a Word document, full of content that you want very much. Unfortunately, it’s difficult to access because I’m puttingit on my website as a .&lt;/w:t&gt;&lt;/w:r&gt;&lt;w:prooferr w:type="spellStart"&gt;&lt;/w:prooferr&gt;&lt;w:r&gt;&lt;w:t&gt;docx&lt;/w:t&gt;&lt;/w:r&gt;&lt;w:prooferr w:type="spellEnd"&gt;&lt;/w:prooferr&gt; &lt;w:r&gt; &lt;w:t xml:space="preserve"&gt; file, rather than just publishing it as HTML&lt;/w:t&gt; &lt;/w:r&gt; &lt;/w:p&gt; &lt;w:sectpr w:rsidr="00764658"w:rsidrpr="00764658"&gt; &lt;w:pgszw:h="15840" w:w="12240"&gt;&lt;/w:pgsz&gt;&lt;w:pgmar w:bottom="1440" w:footer="720" w:gutter="0" w:header="720" w:left="1440" w:right="1440" w:top="1440"&gt;&lt;/w:pgmar&gt; &lt;w:cols w:space="720"&gt;&lt;/w:cols&amp;g; &lt;w:docgrid w:linepitch="360"&gt;&lt;/w:docgrid&gt; &lt;/w:sectpr&gt; &lt;/w:body&gt; &lt;/w:document&gt; 这里显然有很多元数据，但是您想要的实际文本内容被隐藏了。幸运的是，文档中的所有文本，包括顶部的标题，都包含在w:t标签中，便于抓取: 123456789101112131415from zipfile import ZipFilefrom urllib.request import urlopenfrom io import BytesIOfrom bs4 import BeautifulSoupwordFile = urlopen('http://pythonscraping.com/pages/AWordDocument.docx').read()wordFile = BytesIO(wordFile)document = ZipFile(wordFile)xml_content = document.read('word/document.xml')wordObj = BeautifulSoup(xml_content.decode('utf-8'), 'xml')textStrings = wordObj.find_all('w:t')for textElem in textStrings: print(textElem.text) 注意这里不是html。解析器解析器通常与BeautifulSoup一起使用，您将把xml解析器传递给它。这是因为冒号在HTML标记名中是不标准的，比如w:t和HTML。解析器不能识别它们。输出并不完美，但它已经达到了这个目标，将每个w:t标记打印在新行上可以很容易地看到单词是如何分割文本的 12345A Word Document on a WebsiteThis is a Word document, full of content that you want very much. Unfortunately,it’s difficult to access because I’m putting it on my website as a .docxfile, rather than just publishing it as HTML 请注意，单词“docx”位于它自己的行上。在原始XML中，它被标记&lt;w:proofErrw:type=&quot;spellStart&quot;/&gt;包围。这是Word的突出显示方式“docx”下划线为红色，表示它认为自己的文件格式的名称存在拼写错误。 文档的标题前面有样式描述符标签&lt;w:pstyle w:val=&quot; title &quot;&gt;。尽管这并不能让我们非常容易地识别标题(或其他样式的文本)，使用BeautifulSoup的导航功能可以很有用: 12345678textStrings = wordObj.find_all('w:t')for textElem in textStrings: style = textElem.parent.parent.find('w:pStyle') if style is not None and style['w:val'] == 'Title': print('Title is: &#123;&#125;'.format(textElem.text)) else: print(textElem.text) 可以很容易地扩展此函数，以便围绕各种文本样式打印标记，或者以其他方式标记它们。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-6]]></title>
    <url>%2F2019%2F07%2F23%2FWebScraping6%2F</url>
    <content type="text"><![CDATA[尽管将数据打印到终端非常有趣，但在数据聚合和分析方面，它并不是非常有用。要使大多数远程有用，您需要能够保存它们所抓取的信息。本章介绍了三种主要的数据管理方法，这些方法几乎适用于任何可以想象的应用程序。您是否需要为网站的后端供电或创建自己的API?您可能希望写入数据库。需要一个快速和简单的方法来收集文件从互联网上，并把他们放在您的硬盘驱动器?您可能想为此创建一个文件流。需要偶尔的提醒，或者每天一次聚合数据?给自己发一封电子邮件!除了web抓取之外，存储和与大量数据交互的能力对于任何现代编程应用程序都非常重要。事实上，本章中的信息对于实现本书后面章节中的许多示例是必要的。如果您不熟悉自动数据存储，我强烈建议您至少浏览一下这一章。 Media Files您可以通过两种主要方式存储媒体文件:引用和下载文件本身。通过存储文件所在的URL，可以通过引用存储文件。这有几个优点： 运行得更快，当他们不需要下载文件时，需要更少的带宽。 通过只存储url，您可以在自己的机器上节省空间。 编写只存储url且不需要处理额外文件下载的代码会更容易。 您可以通过避免大型文件下载来减轻主机服务器上的负载。 缺点如下: 将这些url嵌入到您自己的网站或应用程序中称为盗链，这样做是让您在internet上陷入困境的一种快速方法。 您不希望使用其他人的服务器周期为自己的应用程序托管媒体。 位于任何特定URL的文件都可能发生更改。这可能会导致尴尬的效果，例如，如果你在一个公共博客中嵌入一个热链接图像。如果您存储url的目的是为了稍后存储该文件，以便进行进一步的研究，那么它可能最终会丢失，或者在稍后被更改为完全不相关的内容。 真正的web浏览器不只是请求页面的HTML然后继续前进。它们还下载页面所需的所有资产。下载文件可以看起来像一个人在浏览网站，这可能是一个优势。 如果你辩论是否存储文件或URL到一个文件,你应该问问你自己你是否可能视图或读到文件不止一次或两次,或如果该数据库文件是围坐在收集电子尘埃的大部分生活。如果答案是后者，那么最好只存储URL。如果是前者，请继续阅读： 用于检索网页内容的urllib库还包含检索文件内容的函数。下面的程序使用urlib .request.urlretrieve下载图片从一个远程URL： 12345678from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com')bs = BeautifulSoup(html, 'html.parser')imageLocation = bs.find('a', &#123;'id': 'logo'&#125;).find('img')['src']urlretrieve (imageLocation, 'logo.jpg') 这将从http://pythonscraping.com下载徽标，并将其作为logo.jpg存储在运行脚本的同一目录中。如果您只需要下载一个文件，并且知道如何调用它，以及文件扩展名是什么，那么这种方法非常有效。但大多数抓取器不会下载一个文件，然后就挂掉。下面从http://pythonscraping.com的主页下载所有内部文件，这些文件由任何标记的src属性链接到: 12345678910111213141516171819202122232425262728293031323334353637383940414243import osfrom urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSoupdownloadDirectory = 'downloaded'baseUrl = 'http://pythonscraping.com'def getAbsoluteURL(baseUrl, source): if source.startswith('http://www.'): url = 'http://&#123;&#125;'.format(source[11:]) elif source.startswith('http://'): url = source elif source.startswith('www.'): url = source[4:] url = 'http://&#123;&#125;'.format(source) else: url = '&#123;&#125;/&#123;&#125;'.format(baseUrl, source) if baseUrl not in url: return None return urldef getDownloadPath(baseUrl, absoluteUrl, downloadDirectory): path = absoluteUrl.replace('www.', '') path = path.replace(baseUrl, '') path = downloadDirectory+path directory = os.path.dirname(path) if not os.path.exists(directory): os.makedirs(directory) return pathhtml = urlopen('http://www.pythonscraping.com')bs = BeautifulSoup(html, 'html.parser')downloadList = bs.findAll(src=True)for download in downloadList: fileUrl = getAbsoluteURL(baseUrl, download['src']) if fileUrl is not None: print(fileUrl)urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory)) 你知道所有那些关于从网上下载未知文件的警告吗?这个脚本将它遇到的所有内容下载到您的计算机硬盘上。这包括随机的bash脚本、.exe文件和其他潜在的恶意软件。认为你是安全的，因为你从来没有真正执行任何发送到你的下载文件夹?尤其是如果你以管理员的身份运行这个程序，你就是在自找麻烦。如果你在网站上偶然看到一个文件，它把自己发送到../../../../usr/bin/ python吗?下一次从命令行运行Python脚本时，您可能正在您的机器上部署恶意软件!本程序仅为说明目的而编写;它不应该在没有更广泛的文件名检查的情况下随机部署，而且它应该只在具有有限权限的帐户中运行。像往常一样，备份文件、不在硬盘上存储敏感信息，以及使用一点常识，这些都大有帮助。 这个脚本使用一个lambda函数(在第2章中介绍)来选择首页上所有具有src属性的标记，然后清理和规范url，以获得每个下载的绝对路径(确保丢弃外部链接)。然后，将每个文件下载到您自己机器上下载的本地文件夹中的自己的路径。注意，Python s os模块被简单地用于检索每次下载的目标目录，并在需要时沿着路径创建丢失的目录。os模块充当Python和操作系统之间的接口，允许它操作文件路径、创建目录、获取有关运行进程和环境变量的信息，以及许多其他有用的东西。 Storing Data to CSVCSV(comma-separated values逗号分隔值)是存储电子表格数据的最流行的文件格式之一。由于其简单性，它受到Microsoft Excel和许多其他应用程序的支持。下面是一个完全有效的CSV文件示例: 1234fruit,costapple,1.00banana,0.30pear,1.25 与Python一样，这里的空格也很重要:每一行由换行符分隔，而行中的列由逗号分隔(因此得名csv)。其他形式的CSV文件(有时称为字符分隔值文件)使用制表符或其他字符分隔行，但是这些文件格式不太常见，也不太受广泛支持。 如果您希望直接从web下载CSV文件并将其存储在本地，而不需要进行任何解析或修改，则不需要本节。像下载其他文件一样下载它们，并使用上一节描述的方法以CSV文件格式保存它们。 使用Python的CSV库，修改CSV文件，甚至完全从零开始创建CSV文件都非常容易: 12345678910import csvcsvFile = open('test.csv', 'w+')try: writer = csv.writer(csvFile) writer.writerow(('number', 'number plus 2', 'number times 2')) for i in range(10): writer.writerow( (i, i+2, i*2))finally: csvFile.close() 警告:用Python创建文件是非常可靠的。如果test.csv还不存在，Python将自动创建文件(而不是目录)。如果已经存在，Python将使用新数据覆盖test.csv。 运行后，你应该看到一个CSV文件: 12345number,number plus 2,number times 20,2,01,3,22,4,4... 个常见的web抓取任务是检索HTML表并将其编写为CSV文件。Wikipedia对文本编辑器的比较提供了一个相当复杂的HTML表，其中包含颜色编码、链接、排序和其他HTML垃圾，在将其写入CSV之前需要丢弃这些垃圾。使用BeautifulSoup和get_text()函数，你可以在20行之内完成: 1234567891011121314151617181920import csvfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')bs = BeautifulSoup(html, 'html.parser')# The main comparison table is currently the first table on the pagetable = bs.findAll('table',&#123;'class':'wikitable'&#125;)[0]rows = table.findAll('tr')csvFile = open('editors.csv', 'wt+',encoding='utf-8')writer = csv.writer(csvFile)try: for row in rows: csvRow = [] for cell in row.findAll(['td', 'th']): csvRow.append(cell.get_text()) writer.writerow(csvRow)finally: csvFile.close() 如果遇到许多HTML表需要转换为CSV文件，或者许多HTML表需要收集到一个CSV文件中，那么这个脚本非常适合集成到scraper中。然而，如果你只需要做一次，有一个更好的工具:复制和粘贴。选择并复制HTML表的所有内容，并将其粘贴到Excel或谷歌文档中，就可以得到您正在寻找的CSV文件，而无需运行脚本。 MySQL这个就偷懒了，觉得小项目不太会用到数据库，如果后期用到我再不上。 Email这个就偷懒了，觉得小项目不太会用到数据库，如果后期用到我再不上。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-5]]></title>
    <url>%2F2019%2F07%2F21%2FWebScraping5%2F</url>
    <content type="text"><![CDATA[前一章介绍了构建大型、可伸缩和(最重要的是)可维护的web爬虫程序的一些技术和模式。虽然这很容易手工完成，但是许多库、框架，甚至基于GUI的工具都可以为您完成这一任务，或者至少尝试让您的生活变得更简单一些。 本章介绍了开发爬行器的最佳框架之一：Scrapy。编写web爬虫程序的一个挑战是，您经常一次又一次地执行相同的任务：查找页面上的所有链接，评估内部和外部链接之间的差异，转到新页面。了解这些基本模式并能够从头开始编写非常有用，但是Scrapy库为您处理了其中的许多细节。 当然，Scrapy并不是一个读心者。您仍然需要定义页面模板，给它提供开始抓取的位置，并为您要查找的页面定义URL模式。但是在这些情况下，它提供了一个干净的框架来保持代码的组织性。 我是用anaconda来安装的，很简单，在库里找到，并且下载就好。可能是网络问题，我也是下载了好几次才成功。其中最新版的文档在此。 Initializing a New Spider安装了Scrapy框架之后，需要为每个爬行器进行少量的设置。蜘蛛是一个杂乱的项目，就像它的同名蛛形纲动物一样，它的设计目的是爬网。在本章中，我特别用“spider”来描述一个杂乱的项目，用“crawler”来表示“任何爬行web的通用程序，不管使用与否”。 要在当前目录中创建一个新的爬行器，可以从命令行运行以下命令： 1$ scrapy startproject wikiSpider 这将在创建项目的目录中创建一个新的子目录，标题为wikiSpider。在这个目录中是以下文件结构： scrapy.cfg wikiSpider spiders __init.py__ items.py middlewares.py settings.py __init.py__ Writing a Simple Scraper要创建爬行器，您将在spider目录中的wikiSpider/ wikiSpider/article.py中添加一个新文件。在您新创建的article.py文件中，编写以下内容： 1234567891011121314151617import scrapyclass ArticleSpider(scrapy.Spider): name='article' def start_requests(self): urls = [ "http://en.wikipedia.org/wiki/Python_%28programming_language%29", "https://en.wikipedia.org/wiki/Functional_programming", "https://en.wikipedia.org/wiki/Monty_Python"] return [scrapy.Request(url=url, callback=self.parse) for url in urls] def parse(self, response): url = response.url title = response.css('h1::text').extract_first() print('URL is: &#123;&#125;'.format(url)) print('Title is: &#123;&#125;'.format(title)) 这个类的名称(ArticleSpider)与目录的名称不同(wikiSpider)，表示这个类特别负责在wikiSpider这个更广泛的类别下搜索文章页面，您稍后可能希望使用它搜索其他页面类型。 对于包含多种类型内容的大型站点，您可能为每种类型(博客文章、新闻稿、文章等)都有单独的剪贴项目，每个项目都有不同的字段，但是都运行在相同的剪贴项目下。每个爬行器的名称在项目中必须是惟一的。 关于这个爬行器需要注意的另一个关键问题是两个函数start_requests和parse。 start_requests是用于生成程序的入口点请求Scrapy用于抓取网站的对象。 parse是由用户定义的回调函数，并通过callback=self.parse传递给请求对象。稍后，您将看到使用parse函数可以完成的更强大的功能，但是现在它打印页面的标题。 您可以通过导航到wikiSpider/wikiSpider目录运行本文spider，并运行： 1scrapy runspider article.py 默认的杂乱输出相当冗长。除了调试信息外，还应该打印如下行： 12345678910112019-07-21 20:47:55 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/robots.txt&gt; (referer: None)2019-07-21 20:47:56 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/wiki/Functional_programming&gt; (referer: None)URL is: https://en.wikipedia.org/wiki/Functional_programmingTitle is: Functional programming2019-07-21 20:47:56 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/wiki/Monty_Python&gt; (referer: None)URL is: https://en.wikipedia.org/wiki/Monty_PythonTitle is: Monty Python2019-07-21 20:47:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https://en.wikipedia.org/wiki/Python_%28programming_language%29&gt; from &lt;GET http://en.wikipedia.org/wiki/Python_%28programming_language%29&gt;2019-07-21 20:47:58 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/wiki/Python_%28programming_language%29&gt; (referer: None)URL is: https://en.wikipedia.org/wiki/Python_%28programming_language%29Title is: Python (programming language) scraper转到作为start_urls列出的三个页面，收集信息，然后终止。 Spidering with Rules前一节中的爬行器并不是爬行器，仅限于抓取它提供的url列表。它没有能力自己寻找新的页面。把它要成为一个成熟的爬虫程序，您需要使用由Scrapy。 不幸的是，这种杂乱的框架不能很容易地在jupyter notebook中运行，这使得代码的线性进展难以捕捉。为了在文本中显示所有代码示例，本文存储了上一节中的scraper.py文件，而下面的示例创建了一个遍历多个页面的杂乱爬行器，存储在article .py中(注意复数形式的使用)。 后面的示例也将存储在单独的文件中，每个部分都给出了新的文件名。运行这些示例时，请确保使用了正确的文件名。 123456789# 注意：以下两行已经不支持###############from scrapy.contrib.linkextractors import LinkExtractorfrom scrapy.contrib.spiders import CrawlSpider, Rule# 注意：以上两行已经不支持################改成:from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Rule 创建文件名articles.py，并写入： 12345678910111213141516171819from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Ruleclass ArticleSpider(CrawlSpider): name = 'articles' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [Rule(LxmlLinkExtractor(allow=r'.*'), callback='parse_items', follow=True)] def parse_items(self, response): url = response.url title = response.css('h1::text').extract_first() text = response.xpath('//div[@id="mw-content-text"]//text()').extract() lastUpdated = response.css('li#footer-info-lastmod::text').extract_first() lastUpdated = lastUpdated.replace('This page was last edited on ', '') print('URL is: &#123;&#125;'.format(url)) print('title is: &#123;&#125; '.format(title)) print('text is: &#123;&#125;'.format(text)) print('Last updated: &#123;&#125;'.format(lastUpdated)) 这个新的ArticleSpider扩展了CrawlSpider类。它没有提供start_requests函数，而是提供了start_urls和allowed_domains的列表。这将告诉爬虫从哪里开始爬行，以及它应该遵循还是忽略基于域的链接。 还提供了rules列表。这进一步说明了哪些链接可以遵循或忽略(在本例中，您允许使用正则表达式. *的所有URLs)。 除了提取每个页面上的标题和URL外，还添加了几个新项。使用XPath选择器提取每个页面的文本内容。XPath通常用于检索文本内容，包括子标记中的文本(例如，文本块中的&lt;a&gt;标记)。如果使用CSS选择器来实现这一点，子标记中的所有文本都将被忽略。 最后更新的日期字符串也从页脚解析并存储在lastUpdated变量中。 让我们使用Scrapy ‘s Rule和LinkExtractor仔细看看这行代码: 1rules = [Rule(LinkExtractor(allow=r'.*'), callback='parse_items',follow=True)] 这一行提供了一个杂乱Rule对象列表，这些对象定义了所有找到的链接都要经过过滤的规则。当有多个规则时，按顺序根据规则检查每个链接。第一个匹配的规则用于确定如何处理链接。如果链接不匹配任何规则，则忽略它。它有六个参数： link_extractor 唯一的强制参数是LxmlLinkExtractor对象 callback 用于解析页面内容的函数 cb_kwargs 要传递给callback函数的参数的字典。该字典的格式为{arg_name1: arg_value1, arg_name2: arg_value2}，可以作为一个方便的工具，重用相同的解析函数，用于稍微不同的任务。 follow 指示是否希望在将来的爬行中包含在该页面中找到的链接。如果没有提供回调函数，则默认值为True(毕竟，如果您没有对页面做任何操作，那么您至少应该使用它来继续在站点中爬行)。如果提供回调函数，则默认为False。 LxmlLinkExtractorr是一个简单的类，专门用于根据提供给它的规则识别和返回HTML内容页面中的链接。它有许多参数，可用于接受或拒绝基于CSS和XPath选择器的链接、标记(您可以在不仅仅是锚标记中寻找链接!)、域等等。 LxmlLinkExtractor类甚至可以扩展，并且可以创建自定义参数。有关更多信息，请参阅有关链接提取器的Scrapy文档。 allow 允许与提供的正则表达式匹配的所有链接 deny 拒绝与提供的正则表达式匹配的所有链接 使用两个单独的规则和一个解析函数的LinkExtractor类，您可以创建一个爬行Wikipedia的爬行器，识别所有文章页面并标记非文章页面(articlesmorerulls .py)： 12345678910111213141516171819202122232425from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Ruleclass ArticleSpider(CrawlSpider): name = 'articles' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [ Rule(LxmlLinkExtractor(allow='^(/wiki/)((?!:).)*$'), callback='parse_items', follow=True, cb_kwargs=&#123;'is_article': True&#125;), Rule(LxmlLinkExtractor(allow='.*'), callback='parse_items', cb_kwargs=&#123;'is_article': False&#125;) ] def parse_items(self, response, is_article): print(response.url) title = response.css('h1::text').extract_first() if is_article: url = response.url text = response.xpath('//div[@id="mw-content-text"]//text()').extract() lastUpdated = response.css('li#footer-info-lastmod::text').extract_first() lastUpdated = lastUpdated.replace('This page was last edited on ', '') print('Title is: &#123;&#125; '.format(title)) print('title is: &#123;&#125; '.format(title)) print('text is: &#123;&#125;'.format(text)) else: print('This is not an article: &#123;&#125;'.format(title)) 回想一下，这些规则按照它们在列表中显示的顺序应用于每个链接。所有文章页面(以/wiki/开头且不包含冒号的页面)首先传递给parse_items函数，默认参数为is_arti cle=True。然后，将所有其他的非article链接传递给parse_items函数，参数为is_article=False。 当然，如果您希望只收集文章类型的页面而忽略其他所有页面，那么这种方法是不切实际的。如果忽略与文章URL模式不匹配的页面，并完全忽略第二条规则(以及is_arti cle变量)，则会容易得多。然而，这种方法在URL信息或爬行过程中收集的信息影响页面解析方式的奇怪情况下可能很有用。 Creating Items到目前为止，您已经了解了许多查找、解析和爬行网站的方法。但是，Scrapy还提供了一些有用的工具，用于将收集到的项组织起来，并将其存储在具有定义良好字段的自定义对象中。 要帮助组织正在收集的所有信息，您需要创建一个Article对象。在items.py文件中定义一个名为Article的新项目。当您打开items.py文件时，它应该是这样的: 12345678910# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# http://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass WikispiderItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() pass 用一个扩展了Scrapy.Item的新Article类替换这个默认的Item存根。 123456import scrapyclass Article(scrapy.Item): url = scrapy.Field() title = scrapy.Field() text = scrapy.Field() lastUpdated = scrapy.Field() 您正在定义将从每个页面收集的三个字段：标题、URL和页面最后一次编辑的日期。 如果要为多个页面类型收集数据，应该在items.py中将每个单独的类型定义为它自己的类。如果您的项比较大，或者您开始将更多的解析功能转移到项对象中，您还可能希望将每个项提取到自己的文件中。然而，尽管这些项目很小，但我喜欢将它们保存在一个文件中。 在文件ArticleSpider .py中，注意为了创建新的文章项，对ArticleSpider类所做的更改: 1234567891011121314151617181920from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Rulefrom wikiSpider.items import Articleclass ArticleSpider(CrawlSpider): name = 'articleItems' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [ Rule(LxmlLinkExtractor(allow='(/wiki/)((?!:).)*$'), callback='parse_items', follow=True), ] def parse_items(self, response): article = Article() article['url'] = response.url article['title'] = response.css('h1::text').extract_first() article['text'] = response.xpath('//div[@id="mw-content-text"]//text()').extract() lastUpdated = response.css('li#footer-info-lastmod::text').extract_first() article['lastUpdated'] = lastUpdated.replace('This page was last edited on ', '') return article 它将输出通常杂乱的调试数据以及每个项目Python字典: 12 用杂乱的条目不仅仅是为了促进良好的代码组织，或者以一种可读的方式进行布局。项目提供了许多用于输出和处理数据的工具，这些工具将在下一节中介绍。 Outputting ItemsScrapy使用Item对象来确定应该从它访问的页面中保存哪些信息。这些信息可以通过多种方式保存，如CSV、JSON或XML文件，使用以下命令: 123$ scrapy runspider articleItems.py -o articles.csv -t csv$ scrapy runspider articleItems.py -o articles.json -t json$ scrapy runspider articleItems.py -o articles.xml -t xml 每一个都运行scraper articleItems项，并将指定格式的输出写入提供的文件。如果该文件不存在，将创建该文件。 您可能已经注意到，在前面的示例中创建的文章spider中，文本变量是字符串列表，而不是单个字符串。这个列表中的每个字符串表示单个HTML元素中的文本，而&lt;div id=&quot;mwcontent- text&quot;&gt;中的内容(您从其中收集文本数据)由许多子元素组成。Scrapy很好地管理了这些更复杂的值。例如，在CSV格式中，它将列表转换为字符串并转义所有逗号，以便在单个CSV单元格中显示文本列表。 在XML中，这个列表的每个元素都保存在子值标签中： 123456789101112&lt;items&gt;&lt;item&gt; &lt;url&gt;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life&lt;/url&gt; &lt;title&gt;Benevolent dictator for life&lt;/title&gt; &lt;text&gt; &lt;value&gt;For the political term, see &lt;/value&gt; &lt;value&gt;Benevolent dictatorship&lt;/value&gt; ... &lt;/text&gt; &lt;lastUpdated&gt; 13 December 2017, at 09:26.&lt;/lastUpdated&gt;&lt;/item&gt;.... 在JSON格式中，列表保存为列表。当然，您可以自己使用Item对象，并以您想要的任何方式将它们写入文件或数据库，只需在爬虫程序的解析函数中添加适当的代码即可。 The Item Pipeline虽然Scrapy是单线程的，但它能够异步地发出和处理许多请求。这使得它比本书目前所写的scraper更快，尽管我一直坚信，对于web抓取来说，速度并不总是越快越好。 使用item pipeline可以在等待请求返回的同时执行所有数据处理，而不是在发出另一个请求之前等待数据处理，从而进一步提高web scraper的速度。当数据处理需要大量时间或必须执行处理器密集型计算时，有时甚至需要这种类型的优化。要创建项目管道，请重新访问本章开头创建的settings.py文件。您应该看到以下注释行 12345# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html#ITEM_PIPELINES = &#123;# &apos;wikiSpider.pipelines.WikispiderPipeline&apos;: 300,#&#125; 取消最后三行注释，代之以： 123ITEM_PIPELINES = &#123; &apos;wikiSpider.pipelines.WikispiderPipeline&apos;: 300,&#125; 这提供了一个Python类 wikispider .pipeline。WikispiderPipeline将用于处理数据，如果有多个处理类，则使用一个整数表示运行管道的顺序。虽然这里可以使用任何整数，但通常使用0-1000，并且将按升序运行。 现在，您需要添加管道类并重写原始爬行器，以便爬行器收集数据，而管道将执行繁重的数据处理工作。在原始爬行器中编写parse_items方法来返回响应并让管道创建Article对象，这可能很有吸引力： 12def parse_items(self, response): return response 但是，Scrapy框架不允许这样做，并且不允许一个Item对象(例如必须返回Article，它扩展Item)。所以parse_items现在的目标是提取原始数据，尽可能少的处理，这样它就可以传递给管道: 12345678910111213141516171819from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Rulefrom wikiSpider.items import Articleclass ArticleSpider(CrawlSpider): name = 'articlePipelines' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [ Rule(LxmlLinkExtractor(allow='(/wiki/)((?!:).)*$'), callback='parse_items', follow=True), ] def parse_items(self, response): article = Article() article['url'] = response.url article['title'] = response.css('h1::text').extract_first() article['text'] = response.xpath('//div[@id="mw-content-text"]//text()').extract() article['lastUpdated'] = response.css('li#footer-info-lastmod::text').extract_first() return article 当然，现在需要通过添加管道将settings.py文件和更新后的爬行器绑定在一起。当Scrapy项目第一次初始化时，在wikiSpider/wikiSpider/pipelines.py文件中创建了一个文件: 12345678# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlclass WikispiderPipeline(object): def process_item(self, item, spider): return item 应该用新的管道代码替换这个存根类。在前面的部分中，您已经以原始格式收集了两个字段，这些字段可以使用额外的处理：lastUpdated(这是一个格式很差的字符串对象，表示一个日期)和text(一组杂乱的字符串片段)。 应该使用以下代码替换wikiSpider/wikiSpider/ pipelines.py中的存根代码: 123456789101112from datetime import datetimefrom wikiSpider.items import Articlefrom string import whitespaceclass WikispiderPipeline(object): def process_item(self, article, spider): article['lastUpdated'] = article['lastUpdated'].replace('This page was last edited on', '') article['lastUpdated'] = article['lastUpdated'].strip() article['lastUpdated'] = datetime.strptime(article['lastUpdated'], '%d %B %Y, at %H:%M.') article['text'] = [line for line in article['text'] if line not in whitespace] article['text'] = ''.join(article['text']) return article 类WikispiderPipeline有一个process_item方法，该方法接收Article对象，将最后更新的字符串解析为Python datetime对象，并将文本从字符串列表中清理并连接到单个字符串中。 process_item是每个管道类的强制方法。Scrapy使用此方法异步传递爬行器收集的Item。例如，如果您像上一节那样将条目输出到JSON或CSV，那么这里返回的已解析的Article对象将被Scrapy记录或打印。 现在，在决定在何处进行数据处理时，您有两种选择:爬行器中的parse_items方法，或者管道中的process_items方法。 可以在settings.py文件中声明具有不同任务的多个管道。然而，Scrapy按顺序将所有项目(无论项目类型如何)传递到每个管道。在数据到达管道之前，项目特定的解析可能在爬行器中得到更好的处理。但是，如果这种解析需要很长时间，您可能需要考虑将其移动到管道(在管道中可以异步处理它)，并添加对项类型的检查: 123def process_item(self, item, spider): if isinstance(item, Article): # Article-specific processing here 在编写杂乱的项目，尤其是大型项目时，要考虑哪些处理和在哪里处理。 Logging with Scrapy由Scrapy生成的调试信息可能很有用，但是，正如您可能已经注意到的，它通常过于冗长。你可以很容易地调整日志记录的水平，在你的Scrapy项目的settings.py文件中添加一行: 1LOG_LEVEL = 'ERROR' Scrapy使用了日志级别的标准层次结构，如下所示: CRITICAL ERROR WARNING DEBUG INFO 如果日志设置为ERROR，则只显示CRITICAL和ERROR日志。如果日志设置为INFO，那么将显示所有日志，依此类推。 除了通过settings.py文件控制日志记录之外，还可以从命令行控制日志的位置。若要将日志输出到单独的日志文件而不是终端，请在从命令行运行时定义一个日志文件: 1$ scrapy crawl articles -s LOG_FILE=wiki.log 这将在当前目录中创建一个新的日志文件(如果不存在)，并将所有日志输出到该文件中，从而使终端只显示手动添加的Python print语句。 More ResourcesScrapy是一个强大的工具，可以处理与web爬行相关的许多问题。它自动收集所有url并将它们与预定义的规则进行比较，确保所有url都是惟一的，在需要的地方对相对url进行规范化，并递归更深入地进入页面。 尽管本章几乎没有触及到Scrapy功能的表面，但我鼓励您查看Dimitrios编写的Scrapy文档，并学习一下它库兹-劳卡斯(O ‘Reilly)编写的Learning Scrapy，提供了关于该框架的全面论述。 Scrapy是一个非常大且不断扩展的库，具有许多特性。它的功能可以无缝地协同工作，但有许多重叠的领域，使用户可以轻松地在其中开发自己的特定样式。如果有什么你想做的这里没有提到，可能有一种(或几种)方法可以做到！]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-4]]></title>
    <url>%2F2019%2F07%2F21%2FWebScraping4%2F</url>
    <content type="text"><![CDATA[当您能够控制数据和输入时，编写干净且可伸缩的代码就已经非常困难了。为web爬虫程序编写代码通常会带来独特的组织挑战，因为web爬虫程序可能需要从程序员无法控制的不同站点集合中抓取和存储各种数据。 可能被要求从各种各样的网站收集新闻文章或博客文章，每个网站都有不同的模板和布局。一个网站的h1标签包含文章的标题，另一个网站的h1标签包含网站本身的标题，文章标题在&lt;span id=&quot;title&quot;&gt;中。 可能需要灵活地控制哪些网站被擦除，以及它们如何被擦除，以及一种快速添加新网站或修改现有网站的方法，尽可能快，而不需要编写多行代码。 可能会被要求从不同的网站上搜集产品的价格，最终目的是比较相同产品的价格。也许这些价格以不同的货币表示，也许还需要将其与来自其他非web源的外部数据相结合。 尽管web爬虫程序的应用程序几乎是无穷无尽的，但是大型可伸缩爬虫程序往往会陷入以下几种模式之一。通过学习这些模式并识别它们适用的情况，可以极大地提高web爬行器的可维护性和健壮性。 本章主要关注web爬虫程序，这些爬虫程序从各种网站收集有限数量的数据类型(如餐馆评论、新闻文章、公司简介)，并将这些数据类型存储为Python对象，从数据库中读写数据。 Planning and Defining Objectsweb抓取的一个常见陷阱是完全根据眼前可用的内容定义要收集的数据。例如，如果你想收集产品数据，你可以先去服装店看看，然后决定你刮下来的每一件产品都需要有以下几个字段： 产品名字 价格 描述 尺寸 颜色 材质 客户评价 查看另一个网站，您会发现页面上列出了sku(用于跟踪和订购商品的库存单位)。想必肯定也想收集这些数据，即使它没有出现在第一个站点上!添加这个字段： SKU 虽然服装可能是一个很好的开始，但您也希望确保可以将这个爬虫扩展到其他类型的产品。你开始浏览其他网站的产品部分，并决定你也需要收集这些信息： 显然，这是一种不可持续的做法。每次在网站上看到新信息时，只要向产品类型添加属性，就会导致太多字段无法跟踪。不仅如此，每次抓取一个新网站时，都将被迫对该网站的字段和到目前为止积累的字段进行详细分析，并可能添加新字段(修改的Python对象类型和数据库结构)。这将导致混乱和难以阅读的数据集，可能导致使用它的问题。 在决定收集哪些数据时，最好的方法之一就是完全忽略这些网站。你不会通过浏览一个网站，然后说:“存在什么?但是通过说，“我需要什么?”然后想办法从那里找到你需要的信息。 也许您真正想做的是比较多个商店之间的产品价格，并随着时间的推移跟踪这些产品的价格。在这种情况下，你需要足够的信息来唯一地识别产品，就是这样： 商品名称 制造商 产品编号(如适用/相关) 需要注意的是，这些信息都不是特定于特定存储的。例如，产品评论、评级、价格，甚至描述都是特定于特定商店中该产品的实例的。可以单独存储。 其他信息(产品的颜色、材质)是特定于产品的，但可能很少——它并不适用于所有产品。重要的是退后一步，为你考虑的每一项都列一个清单，然后问自己以下问题： 这些信息对项目目标有帮助吗？如果我没有它，它会成为一个障碍吗？或者它只是“拥有它很好”，但最终不会影响任何东西？ 如果这在将来可能有帮助，但我不确定，在以后的时间返回并收集数据会有多难？ 这些数据与我已经收集到的数据是否冗余？ 将数据存储在这个特定对象中合乎逻辑吗？(如前所述，如果同一产品的描述在不同站点之间发生变化，那么在产品中存储描述就没有意义了。) 如果你决定你需要收集数据，重要的是问几个问题，然后决定如何存储和处理它的代码： 这些数据是稀疏的还是密集的?它是相关的，并在每个列表中填充，还是只包含少数列表？ 数据有多大？ 特别是在大数据的情况下，我是否需要在每次运行分析时定期检索它，或者只是偶尔检索？ 这类数据的变量有多大?我是否需要定期添加新属性、修改类型(比如可能经常添加的织物图案)，或者设置为石头(鞋码)？ 假设计划围绕产品属性和价格进行一些元分析：例如，一本书的页数，或者一件衣服的面料类型，以及将来可能与价格相关的其他属性。您将遍历这些问题并意识到这些数据是稀疏的(很少有产品具有这些属性)，并且您可能决定频繁地添加或删除这些属性。在这种情况下，创建这样的产品类型可能是有意义的： 商品名字 制造商 产品编号(如适用/相关) 属性(可选列表或字典) 属性类型是这样的： 属性名字 属性值 这允许操作者随着时间的推移灵活地添加新产品属性，而不需要重新设计数据模式或重写代码。在决定如何在数据库中存储这些属性时，可以将JSON写入属性字段，或者将每个属性存储在一个单独的表中，并使用产品ID。有关实现这些类型的数据库模型的更多信息，请参见第6章。 您也可以将上述问题应用于您需要存储的其他信息。为了跟踪每种产品的价格，您可能需要以下工具： 产品编号 商店编号 价钱 日期/时间戳价格见 但是，如果产品的属性实际上改变了产品的价格，情况又会怎样呢?例如，商店可能会对一件大衬衫比一件小衬衫要价更高，因为大衬衫需要更多的劳动力或材料。在这种情况下，您可以考虑将单个衬衫产品拆分为每个尺寸的单独产品列表(以便每个衬衫产品可以独立定价)，或者创建一个新的项目类型来存储关于产品实例的信息，其中包含以下字段： 产品编号 实例类型(本例中为衬衫的大小) 每个价格是这样的： 产品实例ID 商店编号 价钱 日期/时间戳价格见 虽然“产品和价格”的主题似乎过于具体，但是您需要问自己的基本问题，以及在设计Python对象时使用的逻辑，几乎适用于所有情况。 如果您正在抓取新闻文章，您可能需要以下基本信息： 题目 作者 日期 内容 但是说一些文章包含修改日期，或者相关文章，或者一些社交媒体分享。你需要这些吗？它们与项目相关吗？如果不是所有的新闻网站都使用所有形式的社交媒体，而且社交媒体网站的受欢迎程度可能会随着时间的推移而上升或下降，你如何有效和灵活地存储社交媒体分享的数量？ 当面对一个新项目时，立即开始编写Python来抓取网站是很有诱惑力的。数据模型(放在后面考虑)常常会受到第一个站点上数据的可用性和格式的强烈影响。 然而，数据模型是使用它的所有代码的基础。模型中的错误决策很容易导致编写和维护代码的问题，或者难以提取和有效地使用结果数据。特别是在处理各种已知和未知的网站时——认真考虑和计划你到底需要收集什么，以及如何储存，变得至关重要。 Dealing with Different Websites Layouts像谷歌这样的搜索引擎最令人印象深刻的功绩之一是，它能够从各种各样的网站中提取相关和有用的数据，而不需要预先了解网站结构本身。尽管我们人类能够立即识别页面的标题和主要内容(除非web设计非常糟糕)，但是让机器人做同样的事情要困难得多。 幸运的是，在大多数web爬行的情况下，不是要从从未见过的站点收集数据，而是要从几个或几十个由人类预先选择的站点收集数据。这意味着不需要使用复杂的算法或机器学习来检测页面上哪些文本看起来最像标题，或者哪些可能是主要内容。 可以手动确定这些元素是什么。最明显的方法是为每个网站编写一个单独的web爬虫程序或页面解析器。每个对象都可以接受URL、字符串或BeautifulSoup对象，并为被抓取的对象返回一个Python对象。 下面是内容类(表示网站上的一段内容，比如一篇新闻文章)和两个scraper函数的示例，它们接收一个BeautifulSoup对象并返回一个内容实例 123456789101112131415161718192021222324252627282930313233343536373839import requestsclass Content: def __init__(self, url, title, body): self.url = url self.title = title self.body = bodydef getPage(url): req = requests.get(url) return BeautifulSoup(req.text, 'html.parser')def scrapeNYTimes(url): bs = getPage(url) title = bs.find('h1').text lines = bs.select('div.StoryBodyCompanionColumn div p') body = '\n'.join([line.text for line in lines]) return Content(url, title, body)def scrapeBrookings(url): bs = getPage(url) title = bs.find('h1').text body = bs.find('div', &#123;'class', 'post-body'&#125;).text return Content(url, title, body)url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'content = scrapeBrookings(url)print('Title: &#123;&#125;'.format(content.title))print('URL: &#123;&#125;\n'.format(content.url))print(content.body)url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'content = scrapeNYTimes(url)print('Title: &#123;&#125;'.format(content.title))print('URL: &#123;&#125;\n'.format(content.url))print(content.body) 当您开始为其他新闻站点添加scraper功能时，您可能会注意到模式正在形成。每个网站的解析功能本质上都是一样的： 选择title元素并提取标题的文本 选择文章的主要内容 根据需要选择其他内容项 返回使用前面找到的字符串实例化的Content对象 这里唯一真正的站点相关变量是用于获取每条信息的CSS选择器。BeautifulSoup的find和find_all函数接受两个参数—一个标记字符串和一个键/值属性字典—因此您可以将这些参数作为参数传递进来，这些参数定义站点本身的结构和目标数据的位置。 解释以下requests的用法。 1r=requests.get(url,params,**kwargs) url: 需要爬取的网站地址。 params: 翻译过来就是参数， url中的额外参数，字典或者字节流格式，可选。 **kwargs : 12个控制访问的参数 而对于输出r，它的用法如下： 属性 说明 r.status_code http请求的返回状态，若为200则表示请求成功。 r.text http响应内容的字符串形式，即返回的页面内容 r.encoding 从http header 中猜测的相应内容编码方式 r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式） r.content http响应内容的二进制形式 具体例子：(其中有的输出太多，已删除部分) 123456789101112131415161718import requestsr=requests.get("http://www.baidu.com")r.status_code200r.encoding'ISO-8859-1'r.apparent_encoding'utf-8'r.text'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;ipt&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;"&gt;æ\x9b´å¤\x9aäº§å\x93\x81&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a com/ class=cp-feedback&gt;æ\x84\x8fè§\x81å\x8f\x8dé¦\x88&lt;/a&gt;&amp;nbsp;äº¬ICPè¯\x81030173å\x8f·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n'r.encoding='utf-8'r.text'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta chref=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css="h读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京ICP证030173号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n' 还有，join(iterable)函数，用于以指定分隔符将可迭代对象【成员必须为str类型】连接为一个新的字符串,分隔符可以为空，返回值位字符串 123456789101112131415161718192021222324252627282930313233343536import osstring = "test"lis = ['w', 'e', 'q']tpl = ('w', 'e', 'q')dic = &#123;"55": 5, "44": 4, "22": 2, "33": 3, "11": 1&#125;print("11".join(string))t11e11s11tprint("".join(tpl))weqprint(" ".join(lis))w e qprint("key is : [%s] " % (",".join(dic)))key is : [55,44,22,33,11] # 字符串去重并按从大到小排列words = "wsasdeddcewtttwssa"words_set = set(words) # 集合特性实现去重 字符串集合化words_list = list(words_set) # 集合列表化words_list.sort(reverse=True) # 设置排序为从大到小new_words = "".join(words_list) # join方法以空位分隔符拼接列表元素位新字符串print(words_list)['w', 't', 's', 'e', 'd', 'c', 'a']print(new_words)wtsedcaprint(os.path.join("/home/", "test/", "python")) /home/test/pythonprint(os.path.join("/home", "/test", "/python"), end="")/python 为了让事情变得更方便，你可以使用BeautifulSoup select函数，为你想要收集的每条信息添加一个CSS选择器字符串，然后把所有这些选择器都放到dictionary对象中，而不是处理所有这些标签参数和键/值对： 123456789101112131415161718192021222324252627class Content: """ Common base class for all articles/pages """ def __init__(self, url, title, body): self.url = url self.title = title self.body = body def print(self): """ Flexible printing function controls output """ print('URL: &#123;&#125;'.format(self.url)) print('TITLE: &#123;&#125;'.format(self.title)) print('BODY:\n&#123;&#125;'.format(self.body))class Website: """ Contains information about website structure """ def __init__(self, name, url, titleTag, bodyTag): self.name = name self.url = url self.titleTag = titleTag self.bodyTag = bodyTag 请注意，网站类并不存储从各个页面本身收集的信息，而是存储关于如何收集数据的说明。它不存储标题“My Page title”。它只存储字符串标签h1，表示可以在哪里找到标题。这就是为什么这个类被称为网站(这里的信息属于整个网站)，而不是内容(仅包含来自单个页面的信息)。 1234567891011121314151617181920212223242526272829303132333435import requestsfrom bs4 import BeautifulSoupclass Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, 'html.parser') def safeGet(self, pageObj, selector): """ Utilty function used to get a content string from a Beautiful Soup object and a selector. Returns an empty string if no object is found for the given selector """ selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &gt; 0: return '\n'.join([elem.get_text() for elem in selectedElems]) return '' def parse(self, site, url): """ Extract content from a given page URL """ bs = self.getPage(url) if bs is not None: title = self.safeGet(bs, site.titleTag) body = self.safeGet(bs, site.bodyTag) if title != '' and body != '': content = Content(url, title, body) content.print() 下面的代码定义了网站对象，并开始了这个过程： 123456789101112131415161718192021crawler = Crawler()siteData = [ ['O\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'], ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'], ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'], ['New York Times', 'http://nytimes.com', 'h1', 'div.StoryBodyCompanionColumn div p']]websites = []for row in siteData: websites.append(Website(row[0], row[1], row[2], row[3]))crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')crawler.parse( websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')crawler.parse( websites[2], 'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')crawler.parse( websites[3], 'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html') 虽然这个新方法乍一看似乎并不比为每个新站点编写一个新的Python函数简单多少，但是想象一下，当从一个有4个站点源的系统切换到一个有20或200个源的系统时会发生什么。每个字符串列表相对容易编写。它不占太多空间。它可以从数据库或CSV文件加载。它可以从远程源代码导入，也可以交给有一些前端经验的非程序员填写和添加新网站，而且他们永远不必看一行代码。当然，缺点是正在放弃一定程度的灵活性。在第一个例子中，每个网站都有自己的自由形式函数来选择和解析HTML，以获得最终结果。在第二个例子中，每个网站都需要有一个特定的结构，保证字段的存在，字段中的数据必须是干净的，每个目标字段必须有一个唯一的和可靠的CSS选择器。 然而，我认为这种方法的威力和相对灵活性超过了弥补其真实的或察觉到的缺点。下一节将详细介绍这个基本模板的应用程序和扩展，例如，可以处理缺少的字段、收集不同类型的数据、仅遍历网站的特定部分以及存储关于页面的更复杂的信息。 Structuring Crawlers如果你仍然需要手工查找每个链接，那么创建灵活的、可修改的网站布局类型并没有多大用处。前一章展示了各种各样的方法，可以自动地在网站上爬行并找到新的页面。本节将展示如何将这些方法合并到一个结构良好且可扩展的网站爬虫程序中，该爬虫程序可以以自动化的方式收集链接和发现数据。我在这里只介绍了三种基本的web爬虫结构，尽管我相信它们适用于您在野外爬行站点时可能需要的大多数情况，只是在这里和那里做了一些修改。如果你遇到一个不寻常的情况与你自己的爬行问题，我也希望你将使用这些结构作为灵感，以创造一个优雅和强大的爬虫设计。 Crawling Sites Through Search抓取网站最简单的方法之一就是使用与人类相同的方法:使用搜索栏。虽然搜索一个网站的关键字或主题，并收集搜索结果列表的过程可能看起来像一项任务，在不同的网站之间有很大的可变性，但有几个关键点使这一点变得非常简单： 大多数站点检索特定主题的搜索结果列表，方法是通过URL中的参数将该主题作为字符串传递。例如:http://example.com?search=myTopic。URL的第一部分可以保存为网站对象的属性，主题可以简单地附加到它。 搜索之后，大多数站点将结果页面呈现为一个易于识别的链接列表，通常带有一个方便的周围标记，如&lt;span class=&quot;result&quot;&gt;，其确切格式也可以作为网站对象的属性存储。 每个结果链接要么是相对URL(例如/articles/page.html)，要么是绝对URLURL(例如,http://example.com/articles/page.html)。无论期望的是绝对URL还是相对URL，都可以存储为网站对象的属性。 让我们看看这个算法在代码中的实现。Content类与前面的示例非常相似。正在添加URL属性，以跟踪内容是在哪里找到的： 12345678910111213141516class Content: """Common base class for all articles/pages""" def __init__(self, topic, url, title, body): self.topic = topic self.title = title self.body = body self.url = url def print(self): """ Flexible printing function controls output """ print("New article found for topic: &#123;&#125;".format(self.topic)) print("TITLE: &#123;&#125;".format(self.title)) print("BODY:\n&#123;&#125;".format(self.body)) print("URL: &#123;&#125;".format(self.url)) 网站类添加了一些新属性。searchUrl定义了如果您附加了要查找的主题，应该到哪里获取搜索结果。resultListing定义了一个“框”，其中包含关于每个结果的信息，resultUrl定义了这个框中的标记，它将为您提供结果的确切URL。absoluteUrl属性是一个布尔值，它告诉您这些搜索结果是绝对链接还是相对链接。 123456789101112class Website: """Contains information about website structure""" def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag): self.name = name self.url = url self.searchUrl = searchUrl self.resultListing = resultListing self.resultUrl = resultUrl self.absoluteUrl=absoluteUrl self.titleTag = titleTag self.bodyTag = bodyTag 已经扩展了一些，包含了我们的网站数据、要搜索的主题列表和一个循环，该循环遍历所有主题和所有网站。它还包含一个搜索功能，可以导航到特定网站和主题的搜索页面，并提取该页面上列出的所有结果链接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import requestsfrom bs4 import BeautifulSoupclass Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, 'html.parser') def safeGet(self, pageObj, selector): childObj = pageObj.select(selector) if childObj is not None and len(childObj) &gt; 0: return childObj[0].get_text() return '' def search(self, topic, site): """ Searches a given website for a given topic and records all pages found """ bs = self.getPage(site.searchUrl + topic) searchResults = bs.select(site.resultListing) for result in searchResults: url = result.select(site.resultUrl)[0].attrs['href'] # Check to see whether it's a relative or an absolute URL if(site.absoluteUrl): bs = self.getPage(url) else: bs = self.getPage(site.url + url) if bs is None: print('Something was wrong with that page or URL. Skipping!') return title = self.safeGet(bs, site.titleTag) body = self.safeGet(bs, site.bodyTag) if title != '' and body != '': content = Content(topic, title, body, url) content.print()crawler = Crawler()siteData = [ ['O\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=', 'article.product-result', 'p.title a', True, 'h1', 'section#product-description'], ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content', 'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body_1gnLA'], ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=', 'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']]sites = []for row in siteData: sites.append(Website(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]))topics = ['python', 'data science']for topic in topics: print('GETTING INFO ABOUT: ' + topic) for targetSite in sites: crawler.search(topic, targetSite) 这个脚本循环遍历主题列表中的所有主题，并在开始抓取主题之前声明: 1GETTING INFO ABOUT python 然后它循环遍历站点列表中的所有站点，并为每个特定的主题抓取每个特定的站点。每当它成功地抓取到关于页面的信息时，就会将其打印到控制台: 1234New article found for topic: pythonURL: http://example.com/examplepage.htmlTITLE: Page Title HereBODY: Body content is here 注意，它循环遍历所有主题，然后在内部循环遍历所有网站。为什么不反过来，从一个网站收集所有的主题，然后从下一个网站收集所有的主题?首先遍历所有主题是一种更均匀地分配任何一台web服务器上的负载的方法。如果有一个包含数百个主题和数十个网站的列表，这一点尤其重要。你不会一次对一个网站发出成千上万的请求;你发出10个请求，等待几分钟，再发出10个请求，等待几分钟，等等。 尽管请求的数量最终是相同的，但通常情况下，在合理的时间内尽可能多地分发这些请求会更好。注意循环的结构是一种简单的方法。 其中函数解释如下： Crawling Sites Through Links前一章介绍了一些方法来识别web页面上的内部和外部链接，然后使用这些链接在站点上爬行。在本节中，您将把这些相同的基本方法组合成一个更灵活的网站爬虫程序，它可以遵循任何匹配特定URL模式的链接。 当希望从站点收集所有数据，而不仅仅是从特定的搜索结果或页面列表中收集数据时，这种类型的爬虫非常适合于项目。当站点的页面可能杂乱无章或广泛分散时，它也可以很好地工作。这些类型的爬虫程序不需要结构化的方法来定位链接，就像前一节在搜索页面中爬行一样，因此描述搜索页面的属性在Website对象中不需要。但是，因为爬虫没有为它正在寻找的链接的位置提供特定的指令，所以您确实需要一些规则来告诉它应该选择什么样的页面。您提供targetPattern(目标URLs的正则表达式)，并留下布尔值 absoluteUrl变量来完成此操作： 123456789101112131415161718class Website: def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag): self.name = name self.url = url self.targetPattern = targetPattern self.absoluteUrl=absoluteUrl self.titleTag = titleTag self.bodyTag = bodyTagclass Content: def __init__(self, url, title, body): self.url = url self.title = title self.body = body def print(self): print("URL: &#123;&#125;".format(self.url)) print("TITLE: &#123;&#125;".format(self.title)) print("BODY:\n&#123;&#125;".format(self.body)) Content类与第一个爬虫程序示例中使用的是同一个类。爬虫类从每个站点的主页开始，定位内部链接，解析找到的每个内部链接的内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import reclass Crawler: def __init__(self, site): self.site = site self.visited = [] def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, 'html.parser') def safeGet(self, pageObj, selector): selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &gt; 0: return '\n'.join([elem.get_text() for elem in selectedElems]) return '' def parse(self, url): bs = self.getPage(url) if bs is not None: title = self.safeGet(bs, self.site.titleTag) body = self.safeGet(bs, self.site.bodyTag) if title != '' and body != '': content = Content(url, title, body) content.print() def crawl(self): """ Get pages from website home page """ bs = self.getPage(self.site.url) targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern)) for targetPage in targetPages: targetPage = targetPage.attrs['href'] if targetPage not in self.visited: self.visited.append(targetPage) if not self.site.absoluteUrl: targetPage = '&#123;&#125;&#123;&#125;'.format(self.site.url, targetPage) self.parse(targetPage)reuters = Website('Reuters', 'https://www.reuters.com', '^(/article/)', False, 'h1', 'div.StandardArticleBody_body_1gnLA')crawler = Crawler(reuters)crawler.crawl() 这里还有一个以前示例中没有使用的更改：Website对象(在本例中是变量reuters)是Crawler程序对象本身的属性。这对于在爬虫程序中存储已访问页面(已访问visited)很有效，但是这意味着必须为每个网站实例化一个新的爬虫程序，而不是重用同一个爬虫程序来抓取一个网站列表。不管您是选择使爬行器网站不可知，还是选择使网站成为爬行器的一个属性，这都是一个设计决策，您必须根据自己的特定需求来权衡。两种方法都可以。 另一件需要注意的事情是，这个爬虫程序将从主页获取页面，但是在所有这些页面都被记录之后，它将不会继续爬行。您可能想编写一个包含第3章中的模式之一的爬行器，并让它在访问的每个页面上查找更多的目标。您甚至可以跟踪每个页面上的所有url(不仅仅是匹配目标模式的url)来查找包含目标模式的URLs。 Crawling Multiple Page Types不像爬行通过预定的一组页面，爬行通过一个网站的所有内部链接可以提出一个挑战，因为你永远不知道你到底得到了什么。幸运的是，有一些基本的方法可以识别页面类型： By the URL 网站上的所有博客文章都可能包含一个URL(例如http://example.com/blog/titl-of-post)。 By the presence or lack of certain fields on a site 如果一个页面有日期，但是没有作者的名字，您可以将其归类为新闻稿。如果它有标题、主图像、价格，但没有主内容，那么它可能是一个产品页面。 By the presence of certain tags on the page to identify the page 即使没有在标记中收集数据，也可以利用标记。您的爬虫程序可能会查找诸如&lt;div id=&quot;relatedproducts&quot;&gt;这样的元素来将页面标识为产品页面，即使爬虫程序对相关产品的内容不感兴趣。 要跟踪多个页面类型，您需要在Python中拥有多个类型的页面对象。这可以通过两种方式实现： 如果所有页面都是相似的(它们的内容类型基本相同)，您可能想要向现有的网站页面对象添加pageType属性： 123456789class Website: """Common base class for all articles/pages""" def __init__(self, type, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag): self.name = name self.url = url self.titleTag = titleTag self.bodyTag = bodyTag self.pageType = pageType 如果您将这些页面存储在类似sql的数据库中，这种类型的模式表明所有这些页面可能都存储在同一个表中，并且会添加一个额外的pageType列。 如果您正在抓取的页面/内容彼此之间有足够的差异(它们包含不同类型的字段)，这可能需要为每种页面类型创建新的对象。当然，有些东西对所有web页面都是通用的，它们都有一个URL，并且很可能还有一个名称或页面标题。这是使用子类的理想情况： 123456class Webpage: """Common base class for all articles/pages""" def __init__(self, name, url, titleTag): self.name = name self.url = url self.titleTag = titleTag 这不是一个对象，您的爬虫程序将直接使用，但对象将被您的页面类型引用： 12345678910111213class Product(Website): """Contains information for scraping a product page""" def __init__(self, name, url, titleTag, productNumber, price): Website.__init__(self, name, url, TitleTag) self.productNumberTag = productNumberTag self.priceTag = priceTag class Article(Website): """Contains information for scraping an article page""" def __init__(self, name, url, titleTag, bodyTag, dateTag): Website.__init__(self, name, url, titleTag) self.bodyTag = bodyTag self.dateTag = dateTag 这个产品页面扩展了网站基类，并添加了只适用于产品的属性productNumber和price，而Article类添加了不适用于产品的属性body和date。您可以使用这两个类来抓取，例如，商店网站除了产品外，可能还包含博客文章或新闻稿。 Thinking About Web Crawler Models从互联网上收集信息就像从消防水管里喝水一样。外面有很多东西，你需要什么或者如何需要并不总是很清楚。任何大型web抓取项目(甚至一些小型web抓取项目)的第一步都应该是回答这些问题。 当跨多个域或从多个源收集类似数据时，您的目标几乎总是要将其标准化。处理具有相同和可比字段的数据要比处理完全依赖于原始源格式的数据容易得多。 在许多情况下，您应该构建scraper，并假定将来会向其中添加更多的数据源，并以最小化添加这些新数据源所需的编程开销为目标。即使一个网站乍一看不符合你的模型，它也可能以更微妙的方式符合你的模型。能够看到这些潜在的模式可以节省您的时间、金钱和很多头痛。 数据块之间的连接也不应该被忽略。你希望对于具有“类型”、“大小”或“主题”等跨属性的信息数据来源？如何存储、检索和概念化这些属性软件架构是一个广泛而重要的主题，可能需要整个职业生涯的主人。幸运的是，用于web抓取的软件架构要有限得多一组相对容易掌握的技能。当你继续刮除数据后，您可能会发现相同的基本模式反复出现。创建一个结构良好的web刮刀不需要很多神秘的知识，但是它确实需要花点时间退后一步想想你的项目。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-3]]></title>
    <url>%2F2019%2F07%2F14%2FWebScraping3%2F</url>
    <content type="text"><![CDATA[到目前为止，已经看到了带有一些静态页面。在本章中，将开始研究实际的问题，抓取会遍历多个页面，甚至多个站点。 Web爬虫之所以这样命名，是因为它们在Web上爬行。它们的核心是递归的一个元素。它们必须为URL检索页面内容，为另一个URL检查该页面，并检索该页面，一直到无穷。 但是要注意：仅仅因为你能在网上爬行并不意味着你总是应该这么做。在前面的示例中使用的爬虫在所有需要的数据都在一个页面上的情况下工作得非常好。使用爬虫程序时，必须非常注意所使用的带宽，并尽一切努力确定是否有一种方法可以使目标服务器的加载更容易。 Traversing a Single Domain应该已经知道如何编写检索任意值的Python脚本Wikipedia页面，并在该页面上生成链接列表： 1234567from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')bs = BeautifulSoup(html, 'html.parser')for link in bs.find_all('a'): if 'href' in link.attrs: print(link.attrs['href']) 其中，会出现一些你并不像出现的输出，例如： 12//wikimediafoundation.org/wiki/Privacy_policy//en.wikipedia.org/wiki/Wikipedia:Contact_us 事实上，Wikipedia充满了出现在每个页面上的边栏、页脚和页眉链接，以及指向类别页面、谈话页面和其他不包含不同文章的页面的链接： 12/wiki/Category:Articles_with_unsourced_statements_from_April_2014/wiki/Talk:Kevin_Bacon 最近，我的一个朋友在做一个类似的维基百科抓取项目时，提到他写了一个很大的过滤函数，有100多行代码，用来判断一个内部的维基百科链接是否是一个文章页面。不幸的是，他并没有花太多的时间去寻找两者之间的规律“文章链接”和“其他链接”，或者他可能已经发现了这个技巧。如果你查看指向文章页面的链接(相对于其他内部页面)，你会发现它们都有三个共同点: 它们驻留在div中，id设置为bodyContent 链接中不包含冒号 链接以/wiki/开头 可以使用正则表达式^(/wiki/)((?!:).)*$&quot;):使用这些规则稍微修改代码，只检索所需的文章链接： 12345678from urllib.request import urlopenfrom bs4 import BeautifulSoupimport rehtml = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')bs = BeautifulSoup(html, 'html.parser')for link in bs.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')): if 'href' in link.attrs: print(link.attrs['href']) 当然，一个脚本可以在一篇硬编码的Wikipedia文章中找到所有的文章链接，虽然很有趣，但在实践中却毫无用处。需要能够采取这段代码，并将其转换成类似于以下内容： 12345678910111213141516from urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(articleUrl)) bs = BeautifulSoup(html, 'html.parser') return bs.find('div', &#123;'id':'bodyContent'&#125;).find_all('a',href=re.compile('^(/wiki/)((?!:).)*$'))links = getLinks('/wiki/Kevin_Bacon')while len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs['href'] print(newArticle) links = getLinks(newArticle) 在导入所需的库之后，程序要做的第一件事是用当前系统时间设置随机数生成器种子。这实际上确保了每次运行程序时，都能在Wikipedia文章中找到一个新的、有趣的随机路径。 接下来，程序定义getLinks函数，该函数接受表单/wiki/…，然后加上Wikipedia域名http://en.wikipedia.org，并在该域中检索HTML的BeautifulSoup对象。然后根据前面讨论的参数提取文章链接标记列表，并返回它们。 程序的主体从设置文章链接标签列表开始，然后进入一个循环，在页面中找到一个随机的文章链接标记，从中提取href属性，打印页面，并从提取的URL中获得一个新的链接列表。 Crawling an Entire Site在上一节中，随意浏览了一个网站，从一个链接到另一个链接。但是，如果需要系统地编目或搜索站点上的每个页面，该怎么办呢？爬行整个站点，特别是大型站点，是一个内存密集型的过程，最适合存储爬行结果的数据库随时可用的应用程序。但是，可以在不全面运行这些类型的应用程序的情况下研究它们的行为。 什么时候爬行整个网站是有用的，什么时候是有害的？遍历整个站点的抓取有很多好处，包括以下内容： 生成站点地图 几年前，我遇到了一个问题:一个重要的客户希望对网站的重新设计进行评估，但他不想让我的公司访问他们当前内容管理系统的内部结构，也没有公开可用的网站地图。我能够使用爬行器覆盖整个站点，收集所有内部链接，并将页面组织到站点上使用的实际文件夹结构中。这使我能够快速找到我甚至不知道存在的站点部分，并准确地计算需要多少页面设计和需要迁移多少内容。(不是我，是这个书的作者。。) 数据采集 我的另一个客户想收集文章(故事、博客文章、新闻文章等)，以便创建一个专门搜索平台的工作原型。虽然这些网站抓取不需要是详尽的，但它们确实需要相当广泛的(我们只对从几个站点获取数据感兴趣)。我能够创建爬虫程序，递归遍历每个站点，只收集在文章页面上找到的数据。 彻底搜索站点的一般方法是从顶级页面(如主页)开始，搜索该页面上所有内部链接的列表。然后爬行其中的每个链接，并在每个链接上找到附加的链接列表，从而触发另一轮爬行。但是这便是一个指数级的工作量！ 为了避免在同一个页面上爬行两次，非常重要的一点是，发现的所有内部链接都要保持一致的格式，并在程序运行时保存在一个运行集中，以便于查找。集合类似于列表，但是元素没有特定的顺序，只存储惟一的元素，这非常适合我们的需要。只有新的链接才应该被抓取和搜索额外的链接： 1234567891011121314151617from urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(pageUrl)) bs = BeautifulSoup(html, 'html.parser') for link in bs.find_all('a', href=re.compile('^(/wiki/)')): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print(newPage) pages.add(newPage) getLinks(newPage)getLinks('') 为了展示爬虫业务如何工作的全部效果，我放松了构成内部链接的标准(来自前面的示例)。它不是将scraper限制为文章页面，而是查找以/wiki/开头的所有链接，而不管它们在页面的什么位置，也不管它们是否包含冒号。记住：文章页面不包含冒号，但是文件上传页面、谈话页面等在URL中包含冒号。 最初，使用空URL调用getLinks。只要空URL前面加上http://en.wikipedia，这就被翻译为“Wikipedia的首页。然后，遍历第一个页面上的每个链接，并检查它是否在全局页面集中(脚本已经遇到的一组页面)。如果没有，则将其添加到列表中，并将其打印到屏幕上，然后在其上递归调用getLinks函数。 Collecting Data Across an Entire Site如果Web爬行器所做的只是从一个页面跳转到另一个页面，那么它们将非常无聊。为了使它们有用，需要能够在页面上做一些事情。让我们看看如何构建一个scraper，它收集标题、内容的第一段和编辑页面的链接(如果有的话)。 像往常一样，确定如何最好地做到这一点的第一步是查看站点的几个页面并确定一个模式。看看维基百科上的一些页面(包括文章和非文章页面，如隐私政策页面)，以下内容应该清楚: 所有标题(在所有页面上，无论它们作为文章页面、编辑历史页面或任何其他页面的状态如何)都有标题位于h1-&gt;span标记之下，这些是页面上惟一的h1标记。 如前所述，所有正文都位于div#bodyContent标记之下。但是，如果希望获得更具体的信息，并且只访问文本的第一段，那么最好使用div#mw-content-text→p(只选择第一段标记)。对于除文件页面(例如，https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg)之外的所有内容页面都是如此，这些页面没有内容文本的部分。 编辑链接只出现在文章页面上。如果它们发生了，它们将在li#ca-edit标签下的li#ca-edit→span→a中找到。 修改后，如下： 123456789101112131415161718192021222324from urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(pageUrl)) bs = BeautifulSoup(html, 'html.parser') try: print(bs.h1.get_text()) print(bs.find(id ='mw-content-text').find_all('p')[0]) print(bs.find(id='ca-edit').find('span').find('a').attrs['href']) except AttributeError: print('This page is missing something! Continuing.') for link in bs.find_all('a', href=re.compile('^(/wiki/)')): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print('-'*20) print(newPage) pages.add(newPage) getLinks(newPage)getLinks('') 这个程序中的for循环本质上与原始爬行程序中的for循环相同(为了清晰起见，添加了打印破折号，将打印的内容分隔开)。 因为您永远无法完全确定所有的数据都在每个页面上，所以每个print语句都按照最有可能出现在站点上的顺序排列。也就是说，h1标题标签出现在每个页面上(至少就我所知)，所以您首先尝试获取数据。文本内容出现在大多数页面上(文件页面除外)，因此这是检索到的第二段数据。Edit按钮只出现在标题和文本内容都已经存在的页面上，但不会出现在所有这些页面上。 显然，在异常处理程序中封装多行涉及一些危险。首先，您不知道哪一行抛出了异常。此外，如果由于某种原因，一个页面包含一个Edit按钮，但是没有标题，那么Edit按钮将永远不会被记录。然而，在许多情况下，如果站点上出现了按顺序排列的条目，并且无意中丢失了一些数据点或保留了详细的日志，那么它就足够了。 重定向允许web服务器将一个域名或URL指向位于不同位置的内容块。有两种类型的重定向： server-side 服务器端重定向，即在加载页面之前更改URL client-side 客户端重定向，有时会出现“您将在10秒内重定向”类型的消息，在重定向到新消息之前，页面将加载到该消息 使用服务器端重定向，您通常不必担心。如果您在python3.x中使用urllib库，它自动处理重定向!如果您正在使用请求库，请确保将allow- redirects标志设置为True: 1r = requests.get('http://github.com', allow_redirects=True) 只是要注意，有时候，您正在爬行的页面的URL可能不是您输入页面的URL。 Crawling across the Internet在你开始编写一个爬虫程序，跟踪所有出站链接，不管你是否愿意，你应该问自己几个问题： 我要收集什么数据？这可以通过抓取几个预定义的网站来实现吗(几乎总是更容易的选项)，或者我的爬虫程序需要能够发现我可能不知道的新网站吗？ 当我的爬虫到达一个特定的网站，它会立即跟随下一个出站链接到一个新的网站，还是会停留一段时间，并深入到当前的网站？ 在什么情况下，我不想刮一个特定的网站?我对非英语内容感兴趣吗？ 如果我的网络爬虫在某个网站上引起了站长的注意，我该如何保护自己免受法律诉讼？ 因此，代码更新如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from urllib.request import urlopenfrom urllib.parse import urlparsefrom bs4 import BeautifulSoupimport reimport datetimeimport randompages = set()random.seed(datetime.datetime.now())#Retrieves a list of all Internal links found on a pagedef getInternalLinks(bs, includeUrl): includeUrl = '&#123;&#125;://&#123;&#125;'.format(urlparse(includeUrl).scheme,urlparse(includeUrl).netloc) internalLinks = [] #Finds all links that begin with a "/" for link in bs.find_all('a', href=re.compile('^(/|.*'+includeUrl+')')): if link.attrs['href'] is not None: if link.attrs['href'] not in internalLinks: if(link.attrs['href'].startswith('/')): internalLinks.append(includeUrl+link.attrs['href']) else: internalLinks.append(link.attrs['href']) return internalLinks#Retrieves a list of all external links found on a pagedef getExternalLinks(bs, excludeUrl): externalLinks = [] #Finds all links that start with "http" that do #not contain the current URL for link in bs.find_all('a', href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')): if link.attrs['href'] is not None: if link.attrs['href'] not in externalLinks: externalLinks.append(link.attrs['href']) return externalLinksdef getRandomExternalLink(startingPage): html = urlopen(startingPage) bs = BeautifulSoup(html, 'html.parser') externalLinks = getExternalLinks(bs,urlparse(startingPage).netloc) if len(externalLinks) == 0: print('No external links, looking around the site for one') domain = '&#123;&#125;://&#123;&#125;'.format(urlparse(startingPage).scheme,urlparse(startingPage).netloc) internalLinks = getInternalLinks(bs, domain) return getRandomExternalLink(internalLinks[random.randint(0,len(internalLinks)-1)]) else: return externalLinks[random.randint(0, len(externalLinks)-1)]def followExternalOnly(startingSite): externalLink = getRandomExternalLink(startingSite) print('Random external link is: &#123;&#125;'.format(externalLink))followExternalOnly(externalLink) followExternalOnly('http://oreilly.com') followExternalOnly('http://oreilly.com') 前面的程序从http://oreilly.com开始，从外部链接随机跳转到外部链接。下面是它产生的输出示例: 1234http://igniteshow.com/http://feeds.feedburner.com/oreilly/newshttp://hire.jobvite.com/CompanyJobs/Careers.aspx?c=q319http://makerfaire.com/ 外部链接并不总是保证能在网站的首页找到。在本例中，为了找到外部链接，使用了一种类似于前一个爬行示例中使用的方法来递归地深入到一个网站，直到找到一个外部链接。 具体的流程图，如下： 其中，有几个函数的意义需要解释一下： urlparse(link).scheme与 (urlparse(link).netloc两个解析网址的函数： 1234567link = "https://www.baidu.com/baidu?isource=infinity&amp;iname=baidu&amp;itype=web&amp;tn=02003390_42_hao_pg&amp;ie=utf-8&amp;wd=%E7%88%AC%E8%99%AB"print(urlparse(link).scheme)print(urlparse(link).netloc)output:httpswww.baidu.com 我一直提到这一点，但是为了空间和可读性，本书中的示例程序并不总是包含代码所需的必要检查和异常处理。例如，如果在爬行器遇到的站点的任何地方都没有找到外部链接(不太可能，但是如果运行足够长的时间，它一定会在某个时刻发生)，这个程序将继续运行，直到达到Python的递归限制。增加这个爬虫程序健壮性的一个简单方法是将它与第1章中的连接异常处理代码结合起来。这将允许代码在检索页面时遇到HTTP错误或服务器异常时选择不同的URL。 增加这个爬虫程序健壮性的一个简单方法是将它与章节中的连接异常处理代码结合起来这将允许代码在检索页面时遇到HTTP错误或服务器异常时选择不同的URL。 在为任何重要目的运行这段代码之前，请确保您进行了检查，以处理潜在的陷阱。 将任务分解为简单的函数(如“查找此页面上的所有外部链接”)的好处是，稍后可以轻松重构代码来执行不同的爬行任务。例如，如果你的目标是抓取整个网站的外部链接，并记录下每一个链接，你可以添加以下功能： 1234567891011121314151617# Collects a list of all external URLs found on the siteallExtLinks = set()allIntLinks = set()def getAllExternalLinks(siteUrl): html = urlopen(siteUrl) domain = '&#123;&#125;://&#123;&#125;'.format(urlparse(siteUrl).scheme,urlparse(siteUrl).netloc) bs = BeautifulSoup(html, 'html.parser') internalLinks = getInternalLinks(bs, domain) externalLinks = getExternalLinks(bs, domain) for link in externalLinks: if link not in allExtLinks:allExtLinks.add(link) print(link) for link in internalLinks: if link not in allIntLinks:allIntLinks.add(link) getAllExternalLinks(link)allIntLinks.add('http://oreilly.com')getAllExternalLinks('http://oreilly.com') 这段代码可以被看作是两个循环—一个收集内部链接，一个收集外部链接—彼此协同工作。流程图如下： 在编写代码之前，将代码应该做的事情记录下来或绘制图表，这是一种非常好的习惯，可以在爬行器变得越来越复杂时为您节省大量时间和挫折。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-2]]></title>
    <url>%2F2019%2F07%2F13%2FWebScraping2%2F</url>
    <content type="text"><![CDATA[没什么好说的，觉得学的太基础，想快快学，但是在家就很懈怠，一直看剧看剧。。。 对了，忘记声明：此系列参照了《Web Scraping with Python, 2nd Edition》———Ryan Mitchell 假设你有一些目标内容。可能是一个名称、统计数据或文本块。也许它将20层标签在HTML代码中，没有任何有用的标签或HTML属性。假设您决定将警告抛到九霄云外，并编写类似下面这样的代码来尝试提取: 1bs.find_all('table')[4].find_all('tr')[2].find('td').find_all('div')[1].find('a') 看起来不太好。除了线条的美观之外，站点管理员对网站的任何微小更改都可能彻底破坏web scraper。 对于已下这种文本，可以用find_all()来提取绿色的文本： 12&lt;span class="red"&gt;Heavens! what a virulent attack!&lt;/span&gt; replied&lt;span class="green"&gt;the prince&lt;/span&gt;, not in the least disconcerted by this reception. 具体代码： 1234567from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page1.html')bs = BeautifulSoup(html.read(), 'html.parser')nameList = bs.findAll('span', &#123;'class':'green'&#125;)for name in nameList: print(name.get_text()) 其中，.get_text()从正在处理的文档中删除所有标记，并返回只包含文本的Unicode字符串。例如，如果您正在处理一个包含许多超链接、段落和其他标记的大文本块，那么所有这些都将被删除，只剩下一个无标记的文本块。请记住，在一个BeautifulSoup对象中查找要比在一个文本块中查找容易得多。在打印、存储或操作最终数据之前，调用.get_text()应该是您做的最后一件事。一般来说，应该尽可能长地保存文档的标记结构。 Another serving of BeautifulSoupfind/find_all() with BeautifulSoupfind()与find_all()非常相似。它们的细节如下： 12find_all(tag, attributes, recursive, text, limit, keywords)find(tag, attributes, recursive, text, keywords) tag是之前见过的，可以传递标记的字符串名称，甚至是字符串标记名称的Python列表。例如 1.find_all(['h1','h2','h3','h4','h5','h6']) attributes参数接受一个Python属性字典，并匹配包含其中任何一个属性的标记。例如，下面的函数将返回HTML文档中的绿色和红色span标记: 1.find_all('span', &#123;'class':&#123;'green', 'red'&#125;&#125;) recursive是一个 布尔值。您想要深入到文档的哪个部分?如果将recursive设置为True, find_all函数将查看children、children‘s children…用于匹配参数的标记。如果为False，它将只查看文档中的顶级标记。默认情况下，find_all是递归工作的(recursive设置为True)；一般来说，保持现状是一个好主意，除非真正知道需要做什么，并且性能是个问题。 text参数的不同寻常之处在于，它是基于标记的文本内容而不是标记本身的属性进行匹配的。例如，如果想在示例页面上找到“prince”被标记包围的次数，可以用以下行替换前面示例中的.find_all()函数： 12nameList = bs.find_all(text='the prince')print(len(nameList)) limit参数只在find_all方法中使用；find等价于相同的find_all调用，限制为1。如果只对从页面中检索前x项感兴趣，可以设置此选项。但是，请注意，这将按出现的顺序显示页面上的第一项，而不一定是您想要的第一项。 keyword参数允许您选择包含特定属性或一组属性的标记。例如： 1title = bs.find_all(id='title', class_='text') 这将返回class_属性中带有单词“text”和id属性中带有单词“title”的第一个标记。注意，按照惯例，id的每个值在页面上只能使用一次。因此，在实际中，这样一行字可能不是特别有用，应相当于下列案文: 1title = bs.find(id='title') keyword参数在某些情况下是有用的。然而，作为一个BeautifulSoup特性，它在技术上是多余的。请记住，任何可以使用关键字完成的事情也可以使用本章后面介绍的技术来完成(参见regular_express和lambda_express)。例如，以下是相同的： 12bs.find_all(id='text')bs.find_all('', &#123;'id':'text'&#125;) 此外，您可能偶尔会遇到使用keyword的问题，最明显的是当根据类属性搜索元素时，因为class在Python中是受保护的关键字。也就是说，class是Python中不能使用的保留字作为变量或参数名(与前面讨论的beautiful .find_all()关键字参数没有关系)。例如，如果您尝试以下调用，由于类的非标准使用，您将得到一个语法错误: 123456bs.find_all(class='green') File "&lt;ipython-input-1-6de1a859337f&gt;", line 7 bs.find_all(class = 'green') ^SyntaxError: invalid syntax 而以下便可以，加入一个下划线： 1bs.find_all(class_='green') 当然，以下也是可以的： 1bs.find_all('', &#123;'class':'green'&#125;) 回想一下，通过属性列表将标记列表传递给.find_all()就像一个“或”过滤器(它选择包含tag1、tag2或tag3…的所有标记的列表)。如果你有一个很长的标签列表，你可能会得到很多你不想要的东西。keyword参数允许您为此添加一个额外的“和”过滤器。 other BeautifulSoup objects到目前为止，已经在BeautifulSoup库中看到了两种类型的对象: BeautifulSoup objects 在前面的代码示例中看到的实例作为变量bs 在列表中检索，或通过在BeautifulSoup上调用find和find_all分别检索或向下搜索，如下: 1bs.div.h1 然而，库中还有另外两个对象，虽然不太常用，但仍然需要了解: NavigableString objects 用于表示标记内的文本，而不是标记本身(一些函数操作并生成navigablestring，而不是标记对象)。 Comment object 用于在注释标签中查找HTML注释，如： 1&lt;!----like this one----&gt; 这四个对象是在BeautifulSoup库中(此版本时)将遇到的惟一对象。 navigating treesfind_all函数负责根据标签的名称和属性查找标签。但是，如果需要根据文档中的位置查找标记，该怎么办？这就是树导航(navigating trees)派上用场的地方。之前我们的老师方法如下： 1bs.tag.subTag.anotherSubTag 现在，让我们看看如何向上、跨界和对角地导航HTML树。您将使用我们非常可疑的在线购物网站http://www.pythonscraping.com/pages/page3.html ，作为一个用于抓取的示例页面，如图下图所示： 具体的html树是这样的： children and descendants此时要区分children与 descendants。 children：是一个标签的孩子，例如，在上图中，tr是table的孩子。 descendants：是一个标签的后代，例如，在上图中，tr，th，td，img和span都是table的后代。 因此，对于一个标签，它的所有的children都是decendants，但反之未必。 通常，BeautifulSoup函数总是处理所选当前标记的descendants。例如，bs.body.h1选择主体标记的后代的第一个h1标记。它不会找到位于主体之外的标记。 同样的，bs.div.find_all(&#39;img&#39;)也是查找文档中的第一个div，并且返回此div的 descendants中的的所有img的标签。 如果你只想输出只是children的标签： 123456from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')for child in bs.find('table',&#123;'id':'giftList'&#125;).children: print(child) 如果是children，那么输出的，如下： 同样的，如果输出后代，只要把children换成descendants即可。 如果，是descendants，输入，类似如下 1234567891011121314151617&lt;tr class="gift" id="gift1"&gt; &lt;td&gt; Vegetable Basket &lt;/td&gt; &lt;td&gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! &lt;span class="excitingNote"&gt;Now with super-colorful bell peppers! &lt;/span&gt; &lt;/td&gt; &lt;td&gt; $15.00 &lt;/td&gt; &lt;td&gt; &lt;img src="../img/gifts/img1.jpg"/&gt; &lt;/td&gt;&lt;/tr&gt; 那么输出如下： 123456789101112131415161718192021222324252627&lt;td&gt; Vegetable Basket&lt;/td&gt;Vegetable Basket&lt;td&gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! &lt;span class="excitingNote"&gt;Now with super-colorful bell peppers!&lt;/span&gt;&lt;/td&gt;This vegetable basket is the perfect gift for your health conscious (or overweight) friends!&lt;span class="excitingNote"&gt;Now with super-colorful bell peppers!&lt;/span&gt;Now with super-colorful bell peppers!&lt;td&gt;$15.00&lt;/td&gt;$15.00&lt;td&gt;&lt;img src="../img/gifts/img1.jpg"/&gt;&lt;/td&gt;&lt;img src="../img/gifts/img1.jpg"/&gt; next_siblings()有了子孙后代，当然还有同辈之间函数：next_siblings()，具体实例如下： 123456from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')for sibling in bs.find('table', &#123;'id':'giftList'&#125;).tr.next_siblings: print(sibling) 这段代码的输出是打印商品表中的所有产品行，除了第一行标题行。为什么标题行被跳过？对象不能是它们自己的兄弟姐妹。任何时候，只要对象有兄弟姐妹，该对象本身就不会包含在列表中。正如函数的名称所示，它只调用next sibling()。例如，如果您要选择列表中间的一行，并在其上调用next_sibling()，那么只会返回后面的兄弟姐妹。因此，通过选择标题行并调用next_sibling()，可以选择表中的所有行，而不需要选择标题行本身。 作为next_sibling()的一个补充，如果希望获得的同级标记列表的末尾有一个易于选择的标记，那么previous_sibling()函数通常会很有帮助。 parents在抓取页面时，您可能会发现，与查找标记的孩子或兄弟姐妹相比，查找标记的父母的频率要低一些。通常，当您以爬行为目标查看HTML页面时，首先要查看标签的顶层，然后找出如何深入到所需的确切数据块。然而，偶尔会发现自己处于一些奇怪的情况，需要BeautifulSoup的父类查找功能.parent和.parents。例如： 12345from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')print(bs.find('img',&#123;'src':'../img/gifts/img1.jpg'&#125;).parent.previous_sibling.get_text()) 这段代码将打印由该位置的图像表示的对象的价格../img/gifts/img1.jpg(本例中价格为15.00美元)。 具体过程如下： 1234567&lt;tr&gt; - td - td - td 《3》 - "$15.00" 《4》 - td 《2》 - &lt;img src="../img/gifts/img1.jpg"&gt; 《1》 《1》图像 ../img/gifts/img1.jpg首先被选择 《2》选择它的父母，此例中为td 《3》选择td的previous_sibling，此例中是价钱的那个td 《4》选择这个标签的文本”$15.00” Regular expressions这是一个很重要的模块！ 我用了正则字符串这个短语。什么是常规字符串?它是任何可以由一系列线性规则生成的字符串，例如: 至少出现一个字母a 再加上准确的5次字母b 再加上任何偶数个的字母c 以d或者e为结尾 用正则表达式写出：aa*bbbbb(cc)*(d|e) 符号 含义 举例 实例 * 匹配前面的字符，子表达式或括起来的字符，0或者更多次 a*b* aaaaa, aabbb, bbbb + 匹配前面的字符，子表达式或括起来的字符，1或者更多次 a+b+ aaaab, aaabb, abbb [] 匹配括号里的任何一个字符， [A-Z]* APPLE, HELLO () 变成一个子表达式(优先按照正则表达式的“操作顺序”计算这些值) (a*b)* aaabaab, abaaab, ababaaab {m,n} 匹配前面的字符，(优先按照正则表达式的“操作顺序”计算这些值) a{2,3}b{2,3} aabbb, aaabbb, aabb [^] 匹配任何一个不在括号的字符 [^A-Z]* apple, nihao I 匹配由I分隔的任何字符、字符串或子表达式 b(aIiIe)d bad, bid, bed . 匹配任何一个字符(包括符号，数字，空格等) b.d bad, b d, b$d ^ 指示字符或子表达式出现在字符串的开头 ^a apple, asdf, a \ 转义字符(这允许您使用特殊字符作为其字面含义) \.\I\\ .I\ $ 通常在正则表达式的末尾使用，它的意思是“将这个匹配到字符串的末尾”。没有它，每个正则表达式都有一个事实，”.*“在它的末尾，接受只有字符串的第一部分匹配的字符串。这可以看作类似于^符号。 [A-Z]*[a-z]*$ ABCabc, zzzyx, Bob ?! ”不包含。这是个奇怪的符号，紧接在一个字符(或正则表达式)前面，表示这个字符不应该在字符串的特定位置找到。这可能很难使用；毕竟，字符可能在字符串的不同部分找到。如果试图完全消除一个字符，请与连用^和$在两端。 ^((?![A-Z]).)*$ no-caps-here, $ymb0ls a4e f!ne 当然，这是简略版，还有很多的符号，在此就先不解释了。 Regular expressions and BeautifulSoup因此，如果想应用在BeautifulSoup函数中，已上例中，想要输出所有的 1&lt;img src="../img/gifts/img3.jpg"&gt; 在使用find_all(&quot;img&quot;)的时候，也会把其他的多余项加进来，代码如下： 123456789101112131415from urllib.request import urlopenfrom bs4 import BeautifulSoupimport rehtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')images = bs.find_all('img',&#123;'src':re.compile('\.\.\/img\/gifts/img.*\.jpg')&#125;)for image in images: print(image['src']) output:../img/gifts/img1.jpg../img/gifts/img2.jpg../img/gifts/img3.jpg../img/gifts/img4.jpg../img/gifts/img6.jpg Accessing attributes标题翻译为：访问属性 到目前为止，已经了解了如何访问和过滤标签以及访问其中的内容。然而，通常在web抓取中，并不寻找标记的内容；你在寻找它的属性。这对于a之类的标记尤其有用，其中它所指向的URL包含在href属性中；或者img标记，其中目标图像包含在src属性中。 使用标签对象，可以通过调用以下命令自动访问Python属性列表: 1myTag.attrs 请记住，这实际上返回了一个Python dictionary对象，这使得检索和操作这些属性变得非常简单。例如，可以使用以下行找到图像的源位置： 1myImgTag.attrs['src'] Lambda expressionslambda表达式是作为变量传递给另一个函数的函数;您可以将函数定义为f(g(x) y)，甚至f(g(x),h(x))，而不是f(x, y)。 BeautifulSoup允许您将某些类型的函数作为参数传递给find_all函数。 唯一的限制是这些函数必须接受一个标记对象作为参数并返回一个布尔值。在这个函数中，BeautifulSoup遇到的每个标记对象都被求值，求值为True的标记被返回，其余的则被丢弃。 例如，下面的代码检索所有恰好具有两个属性的标签： 1bs.find_all(lambda tag: len(tag.attrs) == 2) 这里，作为参数传递的函数是len(tag.attrs) == 2。如果为真，find_all函数将返回标记。也就是说，它会找到具有两个属性的标签，例如： 12&lt;div class="body" id="content"&gt;&lt;/div&gt;&lt;span style="color:red" class="title"&gt;&lt;/span&gt; Lambda函数非常有用，你甚至可以用它们来替换现有的BeautifulSoup函数: 1bs.find_all(lambda tag: tag.get_text() =='Or maybe he\'s only resting?') 这也可以在没有lambda函数的情况下完成: 1bs.find_all('', text='Or maybe he\'s only resting?') 但是，如果您记住lambda函数的语法，以及如何访问标记属性，那么您可能再也不需要记住任何其他BeautifulSoup语法了! 因为所提供的lambda函数可以是返回True或的任何函数False值，您甚至可以将它们与正则表达式组合起来，以查找具有匹配特定字符串模式的属性的标记。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Webscraping-1]]></title>
    <url>%2F2019%2F07%2F12%2Fwebscraping1%2F</url>
    <content type="text"><![CDATA[创建爬虫专题，完全出于自己的爱好兴趣，可能与学业无关。因此，所记录的会尽量精简，只作为我的学习笔记。 建议环境：python3，jupyter notebook 初入最初的样例如下，函数urlopen(url)：打开网站 123from urllib.request import urlopenhtml = urlopen('http://pythonscraping.com/pages/page1.html')print(html.read()) 输出是这样的： 1b'&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;A Useful Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;An Interesting Title&lt;/h1&gt;\n&lt;div&gt;\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n 可以再通过BeautifulSoup对网站进行解析 12345678from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page1.html')bs = BeautifulSoup(html.read(), 'html.parser')print(bs.h1)output:&lt;h1&gt;An Interesting Title&lt;/h1&gt; 其中，bs为： 1234567891011&lt;html&gt;&lt;head&gt;&lt;title&gt;A Useful Page&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;An Interesting Title&lt;/h1&gt;&lt;div&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 此网页的结构如下： 123456- html -&gt; &lt;html&gt;&lt;head&gt;...&lt;/head&gt;&lt;body&gt;...&lt;/body&gt;&lt;/html&gt; - head -&gt; &lt;head&gt;&lt;title&gt;A Useful Page&lt;title&gt;&lt;/head&gt; - title -&gt; &lt;title&gt;A Useful Page&lt;/title&gt;- body -&gt; &lt;body&gt;&lt;h1&gt;An Int...&lt;/h1&gt;&lt;div&gt;Lorem ip...&lt;/div&gt;&lt;/body&gt; - h1 -&gt; &lt;h1&gt;An Interesting Title&lt;/h1&gt; - div -&gt; &lt;div&gt;Lorem Ipsum dolor...&lt;/div&gt; 因此对于这个实例，bs.h1、bs.html.body.h1、bs.body.h1、bs.html.h1，这四个的结果是相同的。 对于BeautifulSoup函数，第一个参数是指定的HTML文本，第二个参数是为了选择不同的解析器。其他的还有lxml、html5lib等，可以自行查询其利弊。 连接的可靠性与处理异常网络是混乱的。数据格式很差，网站崩溃，关闭标签丢失。最令人沮丧的经历之一在web抓取运行，在你睡觉的时候来抓取数据，在第二天却发现爬虫因为一些意想不到的错误数据格式而停止执行。在这种情况下，你可能会想咒骂创建网站的开发人员的名字(以及格式奇怪的数据)，但是你真正应该责备的是您自己，因为你一开始就没有预料到异常。 爬虫的第一步，就可能会有异常出现： 1html = urlopen('http://www.pythonscraping.com/pages/page1.html' 在服务器上找不到该页面(或者检索时出错)。 找不到服务器 HTTPError第一种情况，将返回一个HTTPerror，可能是404 Page Not Found，也可能是500 Internal Server Error等等。在所有这些情况下，urlopen函数都会抛出通用异常HTTPError，具体解决办法如下： 12345678910from urllib.request import urlopenfrom urllib.error import HTTPErrortry: html = urlopen('http://www.pythonscraping.com/pages/page1.html')except HTTPError as e: print(e) # return null, break, or do some other "Plan B"else: # program continues. Note: If you return or break in the # exception catch, you do not need to use the "else" statement 如果返回HTTP错误代码，程序现在打印错误，并且不执行else语句下程序的其余部分。 URLErrorno server如果根本找不到服务器(例如，”http://www.pythonscraping.com“ 宕机了，或者URL输入错误)，urlopen将抛出一个URLError。这表明根本无法访问任何服务器，而且由于远程服务器负责返回HTTP状态码，不能抛出HTTPError，必须捕获更严重的URLError。你可以添加一个检查，看看是不是这样: 123456789101112131415161718192021222324252627from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom urllib.error import URLErrortry: html = urlopen('https://pythonscrapingthisurldoesnotexist.com')except HTTPError as e: print(e)except URLError as e: print('The server could not be found!')else: print('It Worked!') output:The server could not be found!# be compared with HTTPError# ”http://www.pythonscraping.com“ works well but no page1000try: html = urlopen('http://www.pythonscraping.com/pages/page1000.html')except HTTPError as e: print(e) # this line runsexcept URLError as e: print('The server could not be found!')else: print('It Worked!')output:HTTP Error 404: Not Found no tag当然，如果从服务器成功检索到页面，仍然存在页面上的内容不完全符合你的预期的问题。每次访问BeautifulSoup对象中的标记时，添加一个检查以确保标记确实存在是明智的。如果你试图访问一个不存在的标签，BeautifulSoup将返回一个没有对象。问题是：试图访问None对象本身上的标记将导致抛出AttributeError。 print(bs.nonExistentTag)，其中，nonExistentTag是一个虚构的标记，而不是BeautifulSoup函数的真实名称。返回一个None对象。这个对象是完全合理的处理和检查。如果不检查它，而是继续尝试在None对象上调用另一个函数，print(bs.nonExistentTag.someTag)就会出现问题，如下所示: 1AttributeError: 'NoneType' object has no attribute 'someTag' 对于no tag的这两种情况，最简单的处理方式： 123456789try: badContent = bs.nonExistingTag.anotherTagexcept AttributeError as e: print('nonExistingTag was not found')else: if badContent == None: print ('anotherTag was not found') else: print(badContent) 对每个错误的检查和处理一开始看起来确实很费力，但是很容易在代码中添加一些重组，从而降低编写的难度(更重要的是，降低阅读的难度)。例如，这段代码是我们用稍微不同的方式编写的相同的scraper: 12345678910111213141516171819from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except HTTPError as e: return None try: bs = BeautifulSoup(html.read(), 'html.parser') title = bs.body.h1 except AttributeError as e: return None return titletitle = getTitle('http://www.pythonscraping.com/pages/page1.html')if title == None: print('Title could not be found')else: print(title) 在本例中，将创建一个getTitle函数，该函数将返回页面的标题，如果检索有问题，则返回一个None对象。在getTitle中，检查HTTPError，就像前面的示例一样，并将两个BeautifulSoup封装在一个try语句中。可以从这两行抛出AttributeError(如果服务器不存在、html将是一个None对象、html.read()将抛出AttributeError)。实际上，可以在一个try语句中包含任意多行，或者完全调用另一个函数，这是可以做到的。 以下为测试 12345678910111213141516171819202122232425262728293031from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom urllib.error import URLErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except URLError as e: return e except HTTPError as e: return e try: bs = BeautifulSoup(html.read(), 'html.parser') title = bs.body.h1 except AttributeError as e: return e return titletitle = getTitle('http://www.pythonscraping.com/pages/page1.html')print(title)output:&lt;h1&gt;An Interesting Title&lt;/h1&gt;title = getTitle('http://www.pythonscraping11111.com/pages/page1.html')print(title)output:&lt;urlopen error [Errno 11001] getaddrinfo failed&gt;title = getTitle('http://www.pythonscraping.com/pages/page10000.html')print(title)output:HTTP Error 404: Not Found]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MaOEA/IGD]]></title>
    <url>%2F2019%2F03%2F29%2Fmoeaigd%2F</url>
    <content type="text"><![CDATA[这个还是关于IGD-indicator的多目标函数优化问题，多学学，长长见识。 IGD Indicator-based Evolutionary Algorithm for Many-objective Optimization Problems Yanan Sun, Member, IEEE, Gary G. Yen, Fellow, IEEE, and Zhang Yi, Fellow, IEEE Nadir Point可以分为如下三种：usurface-to-nadir, edge-to-nadir, and extremepoint-to-nadir。时间有限，没有细看引用的论文。 论文中定义了nadir point、worst point、ideal point。 对于m维目标函数 $y^{ext}_1,..,y^{ext}_m$ ， 使 $y^{ext}_i = f(x^{ext}_i) \ and \ x^{ext}_i = argmax_xf_i(x)$. nadir point as $z^{nad}_i = f_i(x_i^{ext})$ . worst point as $z^{w}_i = \max f_i(x)$ . ideal point as $z^*_i = \min f_i(x)​$. 这三个点的区别可以用下图形象的表示出来： 并且在本文中，所定义的极端点如下： z^{nad}_i = \min|f_i(x)| + \lambda \sum_{j=1,j \ne i}^m (f_j(x))^2其中，$\lambda​$ 大于 1。具体样子如下： A与B点为极端点extreme points，C与D点为坏点worse ponits。其中当最小化 $|f_1(x)|+\lambda(f_2(x))^2$ 时， 因为 $\lambda$ 大于 1$，所以 (f_2(x))^2$ 具有更高的优先级，解会在线段BC上，又因为要使 $|f_1(x)|$ 最小，因此B点为极端点。另一个维度同理获得。 具体算法： $IGD^+-EMOA$先介绍一下IGD： IGD =\frac{ \sum_{p \in p^*} dist(p,PF)}{|p^*|}其中，$p^*​$为参考点，$dist(p,PF)​$ 为 p 到离 PF 最近的个体 $y​$ 的欧式距离，$d(p,y)=\sqrt{\sum_{j=1}^m(p_j-y_j)^2}​$。 而 $IGD^+$ 则是在 $d(p,y)$做出改变： d(p,y)=\sqrt{\sum_{j=1}^m\max(y_j-p_j,0)^2}翻译一下：在求平方和时，仅考虑个体比参考点数字大(性能差)的维度。 产生一致性点和NSGA-III不同，NSGA-III是归一化了种群的每一个个体，使极端点形成的平面在各个轴上的截距均为1，总结来说，就是一致性点不变，而改变个体。 此算法相反，修改了每一个一致性点的尺度，种群的每一个个体不变，以此来适应个体。 分配等级与近似距离the rank values are used to distinguish the proximity of the solutions to the Utopian PF from the view of reference points the proximity distances are utilized to indicate which individuals are with better convergence and diversity in the sub-population in which the solutions are with the same rank values. assign rank等级分为三种：$r_1,r_2,r_3$.并且都是个体与参考点 $p^*$ 之间的比较关系。 $r_1​$ ：个体 $s​$ 点至少支配 $p^*​$ 中的一个解。 $r_2$ ：个体 $s$ 点与所有的 $p^*$ 都是非支配关系。 $r_3​$ ：个体 $s​$ 点受 $p^*​$ 部分支配，另一部分的 $p^*​$ 与 $s​$ 非支配。 其中，$p^*$ 类似于下图这种，但要经过各个维度的变换来适应的当前种群。那么在这个空间中，任意取出一点，一定可以满足上面三种中的一个，并且可以直观的看到，优先级：$r_1 &gt; r_2 &gt; r_3$。 具体的说，如果一个最小问题的PF是一个超平面，那么Utopian PF显然等于PF。结果，位于PF的pareto-optimal解全都非支配于从Utopian PF抽样取出的参考点。又由上面所定义的那样，可知pareto-optimal都是 $r_2$。同理，对于convex PF 都是 $r_1$。对于concave PF 都是 $r_3$。 assign proximity distance就像上面所说那样，proximity distance是在 rank 相同的前提下来区分个体的好坏(收敛性)。并定义： $d^i_j$ 是第 $i$ 个个体，对第 $j$ 个参考点的距离。因此 $d$ 矩阵的尺寸为: q x k，(q个种群，k个参考点) 对于第 $i$ 个个体，对第 $j$ 个参考点: r_1 \rightarrow d^i_j = -\sqrt{\sum_{l=1}^m(f_l(x^i)-(p^*)^j_l)^2}\\ r_2 \rightarrow d^i_j = \sqrt{\sum_{l=1}^m\max (f_l(x^i)-(p^*)^j_l,0)^2}\\ r_3 \rightarrow d^i_j = \sqrt{\sum_{l=1}^m(f_l(x^i)-(p^*)^j_l)^2}等级1、3很好理解，解释一下为何2是这样的，如下图： $x^1,x^2,x^3$ 都是 $r_2$(非支配于那三个参考点)，如果单纯的计算欧式距离，那么可以看出 $x^1$ 是最好的解，但是直观上来看，$x^2$ 更有前景收敛于PF。但如果 $r_2$ 那种计算方式，便可得到 $x^2$ 更优。 总结来说，越小的proximity distance表现了更好的估计当待优化的PF未知时。具体算法流程如下： 后代产生Step 1：从当前种群上填充基因池直至满 Step 2：从基因池中选择两个父代，并且从基因池中把他们删除 Step 3：用所选择的父代通过 SBX 操作去产生后代 Step 4：在所产生的后代中变异 Step 5：重复Steps 2-4直至基因池满 具体算法如下： 注意到，其中SBX与多项式变异也有一定的要求。 Generally, two ways can be employed to solve this problem. One is the mating restrictionmethod to limit the offspring to be generated by the neighbor solutions. The other one is to use SBX with a large distribution index. 环境选择算法流程如下： 其中11行：找到A个个体，这些个体满足整体对于参考点 $r$ 有最小的proximity distance，需要一个 linear assignment problem(LAP)。 讨论选择压力的损失是传统的MOEAs解决MaOPs的主要问题，因为传统的个体间的支配比较会给出很大部分的非支配解集。在所提出的算法中，所有个体间的支配关系与通过IGD来计算的所需的参考点相比较。可是，在PF中均匀分布的参考点很难获得。基于此，就在Utopian PF中均与采样出一些点来。而已，为了解决因为参考点不准确的问题，基于它们对所估计出的参考点的支配关系而设计了三种计算距离的方法。我们希望，越小的proximity distance意味着对应的个体有更好的估计。特别地，如果等级为 $r_2$ 的个体，仍然与 $r_1$ 和 $r_3$ 就算距离的方式相同，那么收敛性就会损失。 代码代码最先不断的循环找到nadir point 12345678910DNPE = Global.ParameterSet(100*Global.N);while Global.NotTermination(Population) &amp;&amp; Global.evaluated &lt; DNPE Offspring = GA(Population(randi(end,1,Global.N)),&#123;0.9,20,1,20&#125;); % Off = GA(P,&#123;proC,disC,proM,disM&#125;) dis = distribution index Population = [Population,Offspring]; [~,rank] = sort(Fitness(Population.objs),1); Population = Population(unique(rank(1:ceil(Global.N/Global.M),:)));end% --------------------------------------------% 可以看到：交叉的概率0.9，交叉的参数20，变异的概率1，变异的参数20，非常大的数值了 种群依次为：]]></content>
      <categories>
        <category>indicator-based</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NSGA-III]]></title>
    <url>%2F2019%2F03%2F28%2Fnsgaiii%2F</url>
    <content type="text"><![CDATA[这个算法是慕名而来，之前学习的两个算法都用到了一致性点作为参考点，而这个算法也是比较经典的算法，就来学习学习。 An Evolutionary Many-Objective Optimization Algorithm Using Reference-point Based Non-dominated Sorting Approach, Part I: Solving Problems with Box Constraints Kalyanmoy Deb, Fellow, IEEE and Himanshu Jain 可以看到还是先一层一层的留下(收敛性)，如果数量有多余的，需要从加入一些特殊的规则，保持住多样性。多样性的话，就要有对应的参考点，然后利用个体与参考点的关系，选择出更有前景的个体，直至个体数到规定个数。 参考点选择 参考点就是在以ideal point为原点，在m维空间上，确定一个m-1的平面，均匀分布的点，个数可参考如下公式，用之前学过的概率论的格式，就是那个 $C$ 的样子，一般的问题是已知 $m$ (目标函数个数)、种群数，我要找到与种群数与相接近的 $H$。此时一下公式的 $p$ 便为未知量， $p$ 在几何中的意义就是，把坐标平均分成 $p$ 份，上图 $p=4$， $m=3$，$H=15$。 H =\left( \begin{matrix} M + p-1 \\ p \end{matrix} \right)=\left( \begin{matrix} M + p-1 \\ M - 1 \end{matrix} \right) = C^{p}_{M+p-1}解释：这个用到了高中学过的小球问题，有 $p$ 个不作区分的小球放到 $m$ 个作区分的篮子里，篮子可以可以有多少种情况，数学公式为： f_1 + f_2+... + f_m = p \\ f_i \geq 0 \ and \ f_i \in Z,\ \forall i \in [1,m]从而也可以转化为： f_1 + f_2+... + f_m = p + m \\ f_i \geq 1 \ and \ f_i \in Z,\ \forall i \in [1,m]可以想象有 $p+m$ 个小球排成一排，因为每个篮子里均要有小球，因此这些小球形成的间隔共有 $p +m -1$ 个，共放 $m-1$个隔板，因此会得出结果。 平面确定这样如果确定了平面，我们可以求得一致性点，那么如何求出这个平面。 以 $m=3$ 为例，也就是说，通过某种规则，找到这三个点，$z^{1,max},z^{2,max},z^{2,max}$ 组成的平面来找一致性点。再求出与坐标轴形成的截距：$a_1,a_2,a_3$ 。 下图为实例：黑色的为 $z^{1,max},z^{2,max},z^{2,max}$ 。 那 $z^{1,max},z^{2,max},z^{2,max}$ ,怎么求呢，先介绍一个函数，在MOMBI-II也用到了，但是策略不同： ASF(x,W) = \max_{i=1}^{M}f'_i(x)/w_i \quad for \ x \in S_t理解：所谓极端点就是找到在一个维度上很大，在另外两个维度上的值很小的个体。 假设我要求出 $z^{1,max}$ ，我就在每一个维度上除以1、1e6、1e6。这样我就可以抽出另两个维度的目标值，并取出最大的那个目标值，为什么要取出最大的呢？因为我要找到除了第一维度另两个维度都很小的值，第一次要取出最大的，再对每一个个体的最大的那个取出最小的，那么这个个体的另一个目标值(除了第一维度的那个目标值)肯定会更小，这样才能保证另外两个维度上的值很小。具体操作如下line-4： 翻译一下就是： 对于每一个维度操作 找到此维度最小的值 此维度上都减掉它 求出$z^{j,max}$ 计算截距 每一个点都要$\frac{f_i(x)-z_i^{min}}{a_i - z_i^{min}}$ 这步相当于把这个超平面的截距都变成成(1,…,1) 那么我知道了这三个极端点，如何求出截距呢？如下： 以三维为例： 平面方程式为：$ax + by + cz + d = 0$ 。 我想求的截距便为： \left[ \begin{matrix} -d/a \\ -d/b \\ -d/c \end{matrix} \right]此时我们已知三个点坐标，也就可以知道，下面的方阵： \left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right] \left[ \begin{matrix} a \\ b \\ c \end{matrix} \right] =\left[ \begin{matrix} -d \\ -d \\ -d \end{matrix} \right]继续化简： \left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right] \left[ \begin{matrix} -a/d \\ -b/d \\ -c/d \end{matrix} \right] =\left[ \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right]因此： \left[ \begin{matrix} -a/d \\ -b/d \\ -c/d \end{matrix} \right] =inv(\left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right])\left[ \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right] =\left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right] \backslash \left[ \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right]然后每一位均取倒数即可。高维度($m \geq 3$)的依此类推。 注意，如果矩阵E的秩小于m，那么这m个极限点就不能构成一个m维的超平面。甚至即使超平面能够建立，也可能在某些方向上得不到截距或某些截距 $a_i$ ，不满足 $a_i &gt; z^*_i$。在所有上述情形下，对于每一个 $i \in \{ i,…,m\}$ ，$z^{nad}_i$设置维 $S_t$ 中的非支配解在目标 $f_i$ 上的最大值。 在MATLAB代码中，如下： 12345678910Extreme = zeros(1,M);w = zeros(M)+1e-6+eye(M);for i = 1 : M [~,Extreme(i)] = min(max(PopObj./repmat(w(i,:),N,1),[],2));endHyperplane = PopObj(Extreme,:)\ones(M,1);a = 1./Hyperplane;if any(isnan(a)) a = max(PopObj,[],1)';end 个体对参考点链接效果如下： 算法流程： 此时这个超平面的各各截距均为1，每一个个体也都适应性拉伸，此时原点与一致性点连接所形成的射线，对于每一个 $S_t$ 个体，找到离它最近的射线，测量距离，如上上图，并对这些个体记录那个参考点，距离是多少。 并且记录 $P_{t+1} = S_t/F_l$ 在每个参考点周围的个数。这很重要！！ j \in Z^r:\rho_j = \sum_{S \in S_t/F_l}((\pi(s)=j)?1:0)选择机制流程如下： 为看着直观，下图为，$S_t,P_{t+1},F_l$ 之间的关系，但仅仅是为了理解，作图很不严谨！ 记录 $P_{t+1} = S_t/F_l$ 在每个参考点周围的个数。这很重要！！ j \in Z^r:\rho_j = \sum_{S \in S_t/F_l}((\pi(s)=j)?1:0) 找到计数最少的参考点集，并随机选择一个 —— 可能因为少的地方更需要开拓空间 找到离此参考点最近的那些 $F_l$ 中个体 —— 更能保证最后个体的一致性 如果有 $F_l$ 这些个体 —— 存在添加的可能性，不然去哪加新个体 这个参考点周围没有 $P_{t+1}$ 中的个体 —— 原来的个体中没有在这参考点附近的 选择 $F_l$ 中离此参考点最近的个体 —— 当然要找离它最近的 这个参考点周围有 $P_{t+1}$ 中的个体 —— 原来的个体中有在这参考点附近的 随便选一个$F_l$ 中在此参考点周围的个体 —— 那就随便找好啦 更新参数 没有 $F_l$ 这些个体 —— 不存在添加的可能性 以后不会考虑这个参考点了 —— 当然不会管这个参考点 重复以上操作直至选择的数量够了。注意，添加进去的点，就会默认在 $P_{t+1}$ 中了。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOMBI-II]]></title>
    <url>%2F2019%2F03%2F21%2Fmombiii%2F</url>
    <content type="text"><![CDATA[本想在复现的上一个算法AR-MOEA中加上一点小修改，我思来想去觉得每一个步骤都无懈可击。。。于是我意识到可能是论文读的太少，于是又选了一个indicator-based的多目标优化算法学习一下——MOMBI-II，这个算法过程较AR-MOEA比较简单，此算法选择的indicator是R2但效果也不错，学习学习。 Improved Metaheuristic Based on the R2 Indicator for Many-Objective Optimization Raquel Hernandez Gomez, Carlos A. Coello Coello 规定the ideal objective vector：$z^*_i = \min_{\vec{x}}f_i(\vec{x})​$ the nadir objective vector: $z^{nad}_i = \max_{P^*}f_i(\vec{x})$ indicator: R2 R2(A,U) = \frac{1}{U}\sum_{u \in U}u^*(A)其中，$u^*(A) = \min_{\vec{a} \in A} \{ u(\vec{a}) \}$，是在 $A$ 中最尤的效用值(utility value)。 一个测度叫做 achievement scalarizing function (ASF)： u_{asf}(\vec{v}:\vec{r},\vec{w}) = \max_{i \in \{1,...,m\} } \{\frac{|v_i - r_i|}{w_i} \}归一化： f'_i(\vec{x}) = \frac{f_i(\vec{x}) - z_i^{min}}{z_i^{max}-z_i^{min}} \\ \forall i \in \{1,...,m\}算法流程 代码借鉴了PlatEMO 1234567891011121314151617181920212223242526[alpha,epsilon,recordSize] = deal(0.5,0.001,5);%% Generate random population[W,N] = UniformPoint(N,M);Population = Initialization();% Ideal and nadir pointszmin = min(Population_objs,[],1);zmax = max(Population_objs,[],1);% For storing the nadir vectors of a few generationsRecord = repmat(zmax,recordSize,1);Archive= Population_objs;% For storing whether each objective has been marked for a few% generationsMark = false(recordSize,M);% R2 ranking procedure[Rank,Norm] = R2Ranking(Population_objs,W,zmin,zmax);%% Optimizationwhile Global.NotTermination(Population) MatingPool = TournamentSelection(2,N,Rank,Norm); Offspring = GA(Population(MatingPool)); Population = [Population,Offspring]; [Rank,Norm] = R2Ranking(Population_objs,W,zmin,zmax); [~,rank] = sortrows([Rank,Norm]); Population = Population(rank(1:Global.N)); Rank = Rank(rank(1:Global.N)); Norm = Norm(rank(1:Global.N)); [zmin,zmax,Record,Mark] = UpdateReferencePoints(Population_objs,zmin,zmax,Record,Mark,alpha,epsilon); R2Ranking.m 12345678910111213141516function [Rank,Norm] = R2Ranking(PopObj,W,zmin,zmax) N = size(PopObj,1); NW = size(W,1); %% Normalize the population PopObj = (PopObj-repmat(zmin,N,1))./repmat(zmax-zmin,N,1); %% Calculate the L2-norm of each solution Norm = sqrt(sum(PopObj.^2,2)); %% Rank the population Rank = zeros(N,NW); for i = 1 : NW ASF = max(PopObj./repmat(W(i,:),N,1),[],2); [~,rank] = sortrows([ASF,Norm]); [~,Rank(:,i)] = sort(rank); end Rank = min(Rank,[],2);end UpdateReferencePoints.m 12345678910111213141516171819202122232425function [zmin,zmax,Record,Mark] = UpdateReferencePoints(PopObj,zmin,zmax,Record,Mark,alpha,epsilon) z = min(PopObj,[],1); % z* znad = max(PopObj,[],1); zmin = min(zmin,z); Record = [Record(2:end,:);znad]; v = Record(end-1,:) - znad; % 前一轮的zmax减去当前的zmax mark = false(1,length(zmax)); if max(v) &gt; alpha % 如果差值大到一定程度，直接赋值 zmax = znad; else for i = 1 : length(zmax) % 对每一个目标函数上的维度操作 if abs(zmax(i)-zmin(i)) &lt; epsilon zmax(i) = max(zmax); % 如果在i-th维度上 zmax与新的zmin很接近，max(zmax)直接赋值到此维度上 mark(i) = true; elseif znad(i) &gt; zmax(i) zmax(i) = 2*znad(i) - zmax(i);% 如果在i-th维度上，当前种群最大的大于zmax(i) mark(i) = true; elseif v(i)==0 &amp;&amp; ~any(Mark(:,i)) % 如果在i-th维度上,与之前没有变：差值为0，并且 第i列Mark全为0 zmax(i) = (zmax(i)+max(Record(:,i)))/2; % 此维度上Record的最大值与原来的zmax取平均值 mark(i) = true; end end end Mark = [Mark(2:end,:);mark];end 过程理解Ranking其中主要的代码就是这个： 1234567Rank = zeros(N,NW);for i = 1 : NW ASF = max(PopObj./repmat(W(i,:),N,1),[],2); [~,rank] = sortrows([ASF,Norm]); [~,Rank(:,i)] = sort(rank);endRank = min(Rank,[],2); 可以看到max中便是开头介绍的 $u_{asf}$ 算法，公式： u_{asf}(\vec{v}:\vec{r},\vec{w}) = \max_{i \in \{1,...,m\} } \{\frac{|v_i - r_i|}{w_i} \} 首先遍历每一个一致性点W(i,:)，此时因为已经归一化，因此 $\vec{r} = \vec{0}$，用每一个个体点除一致性点向量($\vec{v}$)，因为在一个循环中，W(i,:)是不变的，因此相当于对每一维度的轴进行线性拉伸(点除)。 对得到的矩阵每一行取最大，翻译一下就是找到可以包含住此点的最小边长的立方体(假设三维，高维同理)，此立方体的边长就是 $\max_{i \in \{1,…,m\} } \{\frac{|v_i |}{w_i} \}$，也可以说拉伸后据原点的最大棋盘距离。 紧接着就是sortrows，翻译一下：先对ASF这个向量排序，如果相同的话，再按向量Norm排序。 再对上面的输出索引sort，翻译一下：对[ASF,Norm]进行离散化(ACM中术语-小弘说)，赋值成1~size(ASF,1)的整数值。如果假设数值越小越好，那于对第 i 列的向量，每一个个体都赋予一个等级 最后min(Rank,[],2)翻译一下：查看每一个个体的历史纪录，取它曾经最好(小)的一次。 有必要解释一下主函数里的这两步： 12[Rank,Norm] = R2Ranking(Population_objs,W,zmin,zmax);[~,rank] = sortrows([Rank,Norm]); 根据进化进程，Rank中会有大量的[1 1 1...2 2 2...]这种重复的元素，也就是说，对于[1...1]对应的个体来说，他们都再某个一致性点向量W(i,:)的结果中当过最最小，都“优秀”过。因此他们都是1，对于Rank中都是2的个体，同理。从这个角度来说，同为一个等级的个体都很好，但由于尺寸限制，可能不能都保留，这个时候便可以进一步比较他们到原点的(欧式)距离，对于minimize问题，当然越小越好，因此sortrows([Rank,Norm])。 update reference point这个我理解不了，只能复述一下代码过程，复述内容见上方的代码注释。]]></content>
      <categories>
        <category>indicator-based</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator-based</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AR-MOEA]]></title>
    <url>%2F2019%2F03%2F21%2Farmoea%2F</url>
    <content type="text"><![CDATA[最近想要复现一个关于indicator-based的多目标优化算法，因此，选了一个容易入手的AR-MOEA算法，此算法，是由安徽大学的田野老师在2018年提出，选用的indicator是IGD。 An Indicator-Based Multiobjective Evolutionary Algorithm With Reference Point Adaptation for Better Versatility Ye Tian, Ran Cheng, Xingyi Zhang, Fan Cheng, and Yaochu Jin, Fellow, IEEE 算法规定定义：无贡献解 \nexists y \in Y \ satisfying \ dis(y,x') = \min_{x \in X} dis(y,x)其中，距离为欧式距离。例子如下图： 适应度为： IGD-NS(X,Y) = \sum_{y \in Y} \min_{x \in X} dis(y,x) + \sum_{x \in X^*} \min_{y \in Y} dis(y,x')符号规定： the population P contains the candidate solutions as final output the initial reference point set R is used to guarantee uniform distribution of the candidate solutions in P the archive A reflects the Pareto front and guides the reference point adaptation the adapted reference point set R‘ is used in the IGD-NS-based selection for truncating the population P 具体关系如下图： 算法伪代码 MatingSelection(P,R’)： 个体 p 的适应度，定义为： fitness_p = IGD-NS(P \backslash \{p\},R' ) 细节图示以下为我的个人想法与心得：声明此代码借鉴PlatEMO中的算法 主函数以下为例 1234567891011121314151617N = 100; % 种群个数D = 10; % 变量个数M = 3; % 目标个数name = 'DTLZ3'; % 测试函数选择，目前只有：DTLZ1、DTLZ2、DTLZ3[res,Population,PF] = funfun(); % 生成初始种群与目标值REF = UniformPoint(N,M); % 生成一致性参考解[Archive,RefPoint,Range] = UpdateRefPoint(res,REF,[]);PD_v = [];for i = 1:400 MatingPool = MatingSelection(Population,RefPoint,Range); %已修改 Offspring = GA(Population(MatingPool,:)); %已修改 Offspring_objs = CalObj(Offspring); [Archive,RefPoint,Range] = UpdateRefPoint([Archive;Offspring_objs([all(PopCon&lt;=0,2)],:)],REF,Range); [Population,Range] = EnvironmentalSelection([Population;Offspring],RefPoint,Range,N);endhold onplot3(PF(:,1),PF(:,2),PF(:,3),'g*') UniformPoint.m 此函数是产生一致性点，当参数如上图所示，那么，这些一致性点在目标空间中，分布如下： UpdateRefPoint.m，此算法就是根据目前的Archive，Poupulation和Reference points生成新的Archive与Reference points。 先要介绍一下AdjustLocation.m 图为此时刻下的调整之前的Reference Points：两个图是同一状态，只是视角不同 再加上蓝色的点，即为当前种群： 红色的点即为调整之后的Reference Points： 这么看可能会发现不了什么规律，其实根据公式也可以知道，如果开始时对所有点进行归一化，那么由原点连接每一个黑色的点，对应的调整后的点必定在这条射线上。就像算盘上的珠子一样，滑到与所有蓝色的点(当前种群)中与到射线最短的距离所对应的投影点上，也就是原论文中的$p \leftarrow argmin_{p \in P}||F(P)||sin(\vec{z^*r},F(p))$，如下图： (黄线穿过：原点-黑点-红点 ) 只画出部分射线： AdjustLocation解释完了，就容易理解RefPointAdaption.m了 注意一点，在RefPointAdaption.m中的Reference Points是一致性点，所以，应该是这样的： 先对当前解、Reference点进行归一化，简单地说就是为了让射线从原点出发。 把ReferencePoint根据当前种群Archive进行调整，也就是上图的黑点变红点。 更新 Archive： 删除重复和受支配的解(因为又添加了交叉变异的个体) 滑动每一个Reference point到Archive中离它最近的的个体周围(映射点)，并且把这些新的个体称为 $A^{con}$，这样可以使得大多数个体多多少少周围都有几个Reference point，当然有的个体可能周围一个都没有。 如果个数不够就要从剩下的Archive中来凑，找在离newAchieve夹角最小中所有最大的那个$argmax_{p \in A \backslash A’} \min_{q \in A’} arccos(F(p),F(q))$。我猜测是为了增大多样性。 更新Reference Points： 根据之前找到的$A^{con}$ ，把离这些$A^{con}$ 中最近的Reference Point(调整后的)，作为newReferencePoint。 如果个数不够就要从新产生的的newArchive中来凑。 把newReferencePoint根据当前种群Population进行调整。 结果如下： 蓝色为一致性点 绿色为真实PF 红色为AR-MOEA算法结果 算法分析 首先，它是用了IGD-indicator来计算每一个个体的适应度。其次，选用了Archive与ReferencePoint来一起维护收敛与一致性。期间只要有靠近新的ReferencePoint，就有很大的概率被留下来，R’也会不断的接近R，其中在那个平面的点，说明此参考点周围已经子代存在，而呈拱形的点便是从A’\A中选择出来的，随着种群进化过程，平面上的点会逐渐变多，拱形上的点逐渐减少，最后便会得出均匀且收敛性较好的解集。]]></content>
      <categories>
        <category>indicator-based</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码学习]]></title>
    <url>%2F2019%2F03%2F15%2Flearningofcode%2F</url>
    <content type="text"><![CDATA[最近在复现AR-MOEA算法，虽说是复现，但目前的想法是从PlatEMO中把代码抠出来，我大致的看了一下代码，里面的代码很简洁，其中通过一些函数调用矩阵操作，从而大大的减少了for循环的使用。因此，想要通过这次解构AR-MOEA算法，储备一些常用的Matlab函数。另外，PlatEMO真的是神器！复现的很多很多算法，和常用的一些测试函数等等，很巧的是，AR-MOEA算法和PlatEMO是同一作者，哈哈哈哈。 说明一点：源代码用了很多结构体的东西，我都拆解了一下。。。这个和之前发过的Matlab常用函数可能有所重复，暂时没有想好究竟属于哪一部分，但重要的不是先记录下来吗？ 初始化种群binary：12345678910111213141516171819randi ：生成**均匀分布**的伪随机整数。randi(imax)：产生一个位于(0,imax]之间的整数。randi(imax,n,m)：产生一个位于（0,imax]之间的n*m的矩阵，所有元素都是整数。randi([imin,imax],n,m)：产生一个位于[imin,imax]之间的n*m的矩阵，所有的矩阵元素都是整数。&gt;&gt; randi(10)ans = 8 &gt;&gt; randi(5,2,3)ans = 1 4 3 4 5 1 &gt;&gt; randi([0,1],3,4)ans = 1 1 0 0 1 1 0 0 0 0 1 1 permutation12345678910111213rand：生成均匀分布的伪随机数。分布在（0~1）之间rand(m,n)生成m行n列的均匀分布的伪随机数&gt;&gt; rand(3,4)ans = 0.8328 0.7083 0.7038 0.5311 0.4554 0.3543 0.6873 0.4308 0.8578 0.1437 0.0276 0.8191 &gt;&gt; sort(ans,2)ans = 0.5311 0.7038 0.7083 0.8328 0.3543 0.4308 0.4554 0.6873 0.0276 0.1437 0.8191 0.8578 others1234567891011unifrnd(A,B)：A，B，ans均为同纬度矩阵，以A(i,j)为下界与B(i,j)为上界，均匀分布产生ans(i,j)。&gt;&gt; N = 5;&gt;&gt; lower = zeros(1,4);&gt;&gt; upper = ones(1,4);&gt;&gt; unifrnd(repmat(lower,N,1),repmat(upper,N,1))ans = 0.0145 0.4117 0.6161 0.6673 0.0026 0.5274 0.1487 0.3502 0.5629 0.2023 0.9411 0.8544 0.4124 0.6985 0.0984 0.1186 0.0801 0.0631 0.5249 0.7846 计算函数值此样例为：DTZL1，形式如下： f_1(x)=(1+g(x))x_1x_2 \\f_2(x)=(1+g(x))x_1(1-x_2) \\f_3(x)=(1+g(x))(1-x_1) \\where \ g(x)=100(n-2) +100\sum_{i=3}^{n}{\{(x_i-0.5)^2-cos[20\pi (x_i-0.5)]\}} \\x=(x_1,...,x_n)^T \in [0,1]^n,n=10比较简单的写法： 1234567891011121314ha = [];for ii = 1:N x = Population(ii,:); f=[]; sum=0; for i=3:D sum = sum+((x(i)-0.5)^2-cos(20*pi*(x(i)-0.5))); end g=100*(D-2)+100*sum; f(1)=(1+g)*x(1)*x(2); f(2)=(1+g)*x(1)*(1-x(2)); f(3)=(1+g)*(1-x(1)); ha(ii,:) = f / 2;end 高阶一点的写法： 123456M 为目标函数个数D 为变量维度g = 100*(D-M+1+sum((PopDec(:,M:end)-0.5).^2-cos(20.*pi.*(PopDec(:,M:end)-0.5)),2));PopObj = 0.5*repmat(1+g,1,M).*fliplr(cumprod([ones(N,1),PopDec(:,1:M-1)],2)).*[ones(N,1),1-PopDec(:,M-1:-1:1)];PopDec(:,M:end)-0.5).^2 = (x_i - 0.5)^2 ,i = 3:n 其中，cumprod 这个是第一次见： 123456789101112131415161718192021222324&gt;&gt; A = [1 2 3 4 5];&gt;&gt; B = cumprod(A) % A为向量连乘的形式B = 1 2 6 24 120 &gt;&gt; A = [1 4 7; 2 5 8; 3 6 9]A = 1 4 7 2 5 8 3 6 9&gt;&gt; B = cumprod(A) % 对矩阵A做列累积相乘B = 1 4 7 2 20 56 6 120 504 &gt;&gt; A = [1 3 5; 2 4 6] % 对矩阵A做行累积相乘A = 1 3 5 2 4 6&gt;&gt; B = cumprod(A,2)B = 1 3 15 2 8 48 fliplr 将矩阵A的列绕垂直轴进行左右翻转 matabc如果A是一个行向量，fliplr(A)将A中元素的顺序进行翻转。如果A是一个列向量，fliplr(A)还等于A。 1234567891011&gt;&gt; A =[1 4;2 5;3 6]A = 1 4 2 5 3 6&gt;&gt; fliplr(A)ans = 4 1 5 2 6 3 生成一致性参考点如下图： b = nchoosek(n,k) 返回二项式系数，定义为 n!/((n–k)! k!)。这就是从 n 项中一次取 k 项的组合的数目。说白了就是概率论里的 $C_n^k$ 。 C = nchoosek(v,k) 返回一个矩阵，其中包含了从向量 v 的元素中一次取 k 个元素的所有组合。矩阵 C 有 k 列和 n!/((n–k)! k!) 行，其中 n 为 length(v)。 1234567891011&gt;&gt; nchoosek(5,3)ans = 10&gt;&gt; v = 2:2:10;C = nchoosek(v,4) % 在[2 4 6 8 10] 这5个元素中取出4个的具体例子，列举出来C = 2 4 6 8 2 4 6 10 2 4 8 10 2 6 8 10 4 6 8 10 并且通过几行代码，即可生成均匀的参考点： 123456H1 = 1;while nchoosek(H1+M,M-1) &lt;= N H1 = H1 + 1;endW = nchoosek(1:H1+M-1,M-1) - repmat(0:M-2,nchoosek(H1+M-1,M-1),1) - 1;W = ([W,zeros(size(W,1),1)+H1]-[zeros(size(W,1),1),W])/H1; 交配池pdist2函数是求两个点集的欧式距离(默认) 1Cosine = 1 - pdist2(PopObj,RefPoint,'cosine'); % 此距离为夹角余弦距离,因为函数自带了 1-Cosine，因此要再减回去。 cosine夹角余弦距离： d_{s,t} = 1 - \frac{x_sx_t'}{||x_s||_2 . ||x_t||_2}true 生成logical的数组 12345K&gt;&gt; true(1,5)ans = 1×5 logical 数组 1 1 1 1 1 锦标赛选择varargin 表示用在一个函数中，输入参数不确定的情况，这增强了程序的灵活性。 例如：function g=fun(f,varargin) 然后在程序中使用时，假如在调用函数时，intrans(f,a,b,c)，那么：varargin{1}=a,varargin{2}=b,varargin{3}=c cellfun 是对cell进行操作的函数 12345678910&gt;&gt; C = &#123;1:10, [2; 4; 6], []&#125;&gt;&gt; cellfun(@mean, C)ans = 5.5000 4.0000 NaN &gt;&gt; days = &#123;'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'&#125;;&gt;&gt; cellfun(@(x) x(1:3), days, 'UniformOutput', false)ans = 1×5 cell 数组 'Mon' 'Tue' 'Wed' 'Thu' 'Fri' sortrows 默认依据第一列的数值按升序移动每一行，如果第一列的数值有相同的，依次往右比较。例： 就是说，把每一行看作整体，先从第一列升序排列，遇到相同的，就比较第二列按升序，如果还有相同的就比较第三轮升序,…….依次 123456789101112131415A = 95 45 92 41 13 1 84 95 7 73 89 20 74 52 95 7 73 5 19 44 20 95 7 40 35 60 93 67 76 61 93 81 27 46 83 76 79 91 0 19 41 1&gt;&gt; sortrows(A)ans = 76 61 93 81 27 46 83 76 79 91 0 19 41 1 95 7 40 35 60 93 67 95 7 73 5 19 44 20 95 7 73 89 20 74 52 95 45 92 41 13 1 84 连续用两个sort ，第一次见到这种操作，很有意思，把一组数进行替换，替换成他在这组数中的升序排名。如果原向量是适应度，那么就可以替换成1~N的序号 12345678910&gt;&gt; a = [1.1 3.1 5.1 7.1 4.1 2.1 6.1]&gt;&gt; [~,i] = sort(a)i = 1 6 2 5 3 7 4&gt;&gt; [~,ii] = sort(i)ii = 1 3 5 7 4 2 6% 原代码&gt;&gt; [~,rank] = sortrows([varargin&#123;:&#125;]);&gt;&gt; [~,rank] = sort(rank); 使用randi来进行随机选择 K 元锦标赛的序号 1Parents = randi(length(varargin&#123;1&#125;),K,N) % N 是种群个数 先通过rank(Parents)来通过序号找到对应的适应度值，用min 来找到每一列的最小值(函数输入时对适应度去负了) 1[~,best] = min(rank(Parents),[],1); 最后，best是一系列1、2组合的矩阵，再通过一下变换可求出结果 1index = Parents(best+(0:N-1)*K); 更新参考点unique 对向量去重，并且从小到大排序输出。 ismember 1234567891011121314&gt;&gt; a=[1 2 3 4 5];&gt;&gt; b=[3 4 5 6 7];&gt;&gt; c=[2 4 6 8 10];&gt;&gt; ismember(a,b) % a中的每一个元素是否在b中ans = 1×5 logical 数组 0 0 1 1 1 &gt;&gt; [lia,lib]=ismember(a,c) % a在lib中对应位置在c上的索引，如有多个，取第一个lia = 1×5 logical 数组 0 1 0 1 0lib = 0 1 0 2 0 交叉操作binary： 12345678% One point crossoverk = repmat(1:D,N,1) &gt; repmat(randi(D,N,1),1,D);k(repmat(rand(N,1)&gt;proC,1,D)) = false;Offspring1 = Parent1;Offspring2 = Parent2;Offspring1(k) = Parent2(k);Offspring2(k) = Parent1(k);Offspring = [Offspring1;Offspring2]; permutation： 1234567% Order crossoverOffspring = [Parent1;Parent2];k = randi(D,1,2*N);for i = 1 : N Offspring(i,k(i)+1:end) = setdiff(Parent2(i,:),Parent1(i,1:k(i)),'stable'); Offspring(i+N,k(i)+1:end) = setdiff(Parent1(i,:),Parent2(i,1:k(i)),'stable');end 其他： 12345678910% Simulated binary crossoverbeta = zeros(N,D);mu = rand(N,D);beta(mu&lt;=0.5) = (2*mu(mu&lt;=0.5)).^(1/(disC+1));beta(mu&gt;0.5) = (2-2*mu(mu&gt;0.5)).^(-1/(disC+1));beta = beta.*(-1).^randi([0,1],N,D);beta(rand(N,D)&lt;0.5) = 1;beta(repmat(rand(N,1)&gt;proC,1,D)) = 1;Offspring = [(Parent1+Parent2)/2+beta.*(Parent1-Parent2)/2 (Parent1+Parent2)/2-beta.*(Parent1-Parent2)/2]; 变异操作binary： 123% Bitwise mutationSite = rand(2*N,D) &lt; proM/D;Offspring(Site) = ~Offspring(Site); permutation： 12345678910% Slight mutationk = randi(D,1,2*N);s = randi(D,1,2*N);for i = 1 : 2*N if s(i) &lt; k(i) Offspring(i,:) = Offspring(i,[1:s(i)-1,k(i),s(i):k(i)-1,k(i)+1:end]); elseif s(i) &gt; k(i) Offspring(i,:) = Offspring(i,[1:k(i)-1,k(i)+1:s(i)-1,k(i),s(i):end]); endend 其他： 1234567891011121314 % Polynomial mutationLower = repmat(lower,2*N,1);Upper = repmat(upper,2*N,1);Site = rand(2*N,D) &lt; proM/D;mu = rand(2*N,D);temp = Site &amp; mu&lt;=0.5;Offspring = min(max(Offspring,Lower),Upper);Offspring(temp) = Offspring(temp)+(Upper(temp)-Lower(temp)).*((2.*mu(temp)+(1-... 2.*mu(temp)).*(1-(Offspring(temp)-Lower(temp))./... (Upper(temp)-Lower(temp))).^(disM+1)).^(1/(disM+1))-1);temp = Site &amp; mu&gt;0.5; Offspring(temp) = Offspring(temp)+(Upper(temp)-Lower(temp)).*(1-(2.*(1... -mu(temp))+2.*(mu(temp)-0.5).*(1-(Upper(temp)-... Offspring(temp))./(Upper(temp)-Lower(temp))).^(disM+1)).^(1/(disM+1))); hist函数： hist(X,Y)：X是一个事先给定的区间划分，统计Y在X这个区间划分下的个数，划分规则如下： 12345678910111213141516171819&gt;&gt; a = randi(5,1,10)a = 2 1 4 2 1 2 4 2 3 1&gt;&gt; k = hist(a,1:6)k = 3 4 1 2 0 0 b = [2.6 1 4 2 1 2 4 2 3 1];&gt;&gt; k = hist(b,1:6)k = 3 3 2 2 0 0 &gt;&gt; a = [2.5 -10 4 2 1 2 4 2 3 1];&gt;&gt; k = hist(a,1:6)k = 3 4 1 2 0 0 % ---------------------------------------------------------------- % 可以看到随机产生了1x10的[1~5]的整数向量a，那么函数结果为分别在:% (-inf 1.5],(1.5 2.5],(2.5 3.5],(3.5 4.5],(4.5 5.5],(5.5 inf) 用处： 1234567PopObj = [PopObj1;PopObj2];Distance = pdist2(PopObj,Ref); [d,pi] = min(Distance,[],2);rho = hist(pi(1:N_PopObj1),1:N_Ref);% -----------------------------------------------------------------% pi为PopObj到哪个Ref最近的index，rho即为离PopObj1种群最近的哪个Ref的对于所有Ref的计数量，% 可用于NSGA-III中]]></content>
      <categories>
        <category>matlab</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for All Quality Aspects]]></title>
    <url>%2F2019%2F03%2F06%2Fallaspects%2F</url>
    <content type="text"><![CDATA[一方面一拖再拖，一方面后面的 indicators 很多都是单独的一整篇论文，并非后面的一小部分，阅读并理解起来比较吃力，也有很多地方只看了算法步骤，而没有细看具体推导与证明，其中一篇，记忆很深刻，姚老师的 DoM-indicator 行文很严谨，并且断断续续看了四天才大概理解。 这类中的质量指标在文献中最常用，因为它们涵盖了解决方案集质量的所有四个方面。表2中的75-100项列出了这些QIs。一般可分为两类:基于距离的QIs(项目75-91)和基于容量的QIs(项目92-100)。 distance-based QIs： volume-based QIs： Distance-based QIs基于距离的QIs的基本思想是测量PF到所考虑解集的距离。因此，需要一个能很好地表示PF的参考集(reference set)。只有接近参考集的每个成员的解集才能有一个好的评估值，从而反映所有质量方面的收敛性、扩散性、一致性和基数性。这个想法可以通过平均(或累加)引用集的成员到解集中它们最近的解的距离，或者从这些距离中找到最大值来实现。对于前者，反向代际距离(IGD)是一个典型的例子，它考虑了平均值欧氏距离。其他的例子包括Dist1(D1)和一些IGD的变异。他们使用差异距离度量(如Tchebycheff distance和Hausdorff distance)，或者在评价中引入支配关系或附加点。 测量帕累托前沿到解集的最大差分(距离)可以很容易地识别出它们之间的差距，从而判断解集在前沿是否具有良好的覆盖性。Dist2(D2)和$\epsilon$-indicator就是这样的QIs。Dist2指标考虑Tchebycheff距离，而$\epsilon$-indicator考虑的是在目标上的最大区别：参考集的点优越于所考虑的解。与averaging difference-based QIs不同，maximum difference-based QIs可能具有明确的物理意义；例如，$\epsilon$-indicator是测量最小值添加到任何的解将使它被至少一个参考点集所weakly dominated。然而，他们的结果通常只涉及到一个特定目标的一个特解，因此自然会有大量的信息丢失。 最近提出了一种质量指标，称为优势移动(DoM)，它可以看作是上述两种QIs的组合。具体地说，给定两个解集A和B，A到B的DoM是移动A中一些点以便B中的任意一点都至少由A中的一点所支配。这种直观的指标具有许多可取的性质，如两种解决方案比较的自然延伸，符合帕累托优势，不需要问题知识和参数。然而，它的计算并不简单。虽然提出了一种双目标情况下的高效计算方法，但如何在有三个或三个以上目标的情况下高效计算仍有待探索。值得一提的是，早期的质量指标[126]可以看作是DoM的简化版本。它划分了引用集(即，$A\cup B$)为许多集群,将A对每个簇的最大差分累加。这使得计算变得高效，但是自然地失去了它的物理意义。 Inverted Generational Distance(IGD)它是最常用的指标之一，尽管之前提出了一些类似的观点。顾名思义，IGD是GD指标的反演，即测量从帕累托前沿到解集的距离。 形式上，给定解集A和参考集 $R=\{ r_1,r_2,…,r_M \}$ IGD(A,R) = \frac{1}{M} \sum_{i=1}^M \min_{a \in A} d_2(r_i,a)$d_2(r_i,a)$ 表示 $r_i$ 与 $a$ 的欧式距离。IGD值越低越好，说明该集合具有较好的收敛性、扩散性、均匀性和基数性的组合特性。 然而，IGD评估的准确性在很大程度上取决于参考集对帕累托前沿的逼近质量。不同的参考集可以使指标偏好不同的解集。通常建议对帕累托前沿采用高分辨率的大参考集。如[89,92]所示，集合中点数不足很容易导致反直觉的评价。此外，由优化器生成的所有非支配解决方案组成的引用集也可能导致误导结果，尽管这种做法在实际问题中得到了广泛采用。 Front to set distance看了好几遍，没有发现和IGD有什么不同，，，笨死了 为了度量MOEA的性能，我们只考虑运行MOEA所产生的最终总体中包含的所有非支配解决方案的子集。我们称这样的子集为近似集并用 $S$ 表示。近似集的大小取决于用于运行MOEA的设置。此指标计算离散帕累托最优集中每个解到近似集S中最近解的距离，并取平均值作为指标值： D_{P_F \rightarrow S}(S) = \frac{1}{|P_S|} \sum_{z^1 \in P_S}\min_{z^0 \in S}\{ d(z^0,z^1) \}由于我们感兴趣的是在目标空间中测量性能，两个多目标解 $z^0$ 和 $z^1$ 之间的距离就是它们的目标值 $f(z^0)$ 和 $f(z^1)$ 之间的欧氏距离。$D_{P_F \rightarrow S}(S) $ 指标既表示接近帕累托最优前沿的目标，也表示得到一个diverse、wide-spread 解决方案前沿的目标。这个性能指标的值越小越好。一个与此indicator密切相关的性能指标的hypervolume指标。在hypervolume指示器中，选择目标空间中的一个点，使其由需要测量的近似集中的所有点支配。然后，指示值等于由逼近集和所选参考点包围的多维区域的超体积。这个值是由近似集支配的目标空间中区域的指标。hypervolume 指示器和 $D_{P_F \rightarrow S} $ 指示器之间的主要区别在于，对于 hypervolume 指示器，必须选择一个参考点。不同的参考点导致不同的指示值。此外，不同的参考点可能导致指示值表明对不同的近似集的偏好。由于在 $D_{P_F \rightarrow S} $ 指标中使用了真正的帕累托最优前沿，因此 $D_{P_F \rightarrow S} $ 指标并不适用于此缺点。当然， $D_{P_F \rightarrow S} $ 指标的一个主要缺点是，在实际应用中，真正的帕累托最优前沿是未知的。在这种情况下，所有逼近集的帕累托前缘可以用来代替实际的帕累托最优前缘。 Delineation of Pareto Optimal Front介绍了描述度量 $\Phi$ 评价收敛性和多样性的程度一个已知的帕累托最优。本研究的目标是确定一组能够很好地表示帕累托最优。这个度量背后的思想是在帕累托最优前沿上的每个解能被已得到的非支配解有多好的表示出来。计算描述度量 $\Phi$ ，大量的H等间隔的解必须知道以反映真实帕累托最优前沿的帕累托最优集。用于计算的距离度量 $\gamma$ 同一组 $H$ 解决方案使用。从每个帕累托最优解到以获得的解 $l_i$ 的欧氏距离,这距离的平均值作为描述度量 $\Phi$,也就是说: \Phi(P_T) = \frac{1}{H} \sum_{i=1}^H l_i需要注意的是，在计算这个度量时，要考虑算法获得的所有解，包括那些受支配的解。 Dist1(D1)我们假设 $M$ 是一个好的 $R$ 的近似解，如果它可以给 $R$ 中的所有地区的重要信息，换句话说，如果对于每一个解 $y \in R$ ，这里都有一个比较接近的解 $x \in M$ 。我们建议用以下基于成就尺度函数的度量方法来度量两个解的亲密性： c(x,y) = \max_{j=0,...,J}\{0,w_j(f_j(y)-f(j(x) )\}$J$ 为目标函数的个数。因此，如果在所有目标上x达到解y的值，则测量值为0。否则，它取特定目标相对于y的最大加权偏差值。上述表达式中使用的权重设置为： w_j = 1 / \Delta_j其中，$\Delta_j$ 是在reference set的 $f_j$ 的范围。 Dist1=\frac{1}{card\{R\}} \sum_{y \in R} \{ min_{x \in M} \{c(x,y)\} \}Dist2(D2) Dist2= \max_{y \in R} \{ min_{x \in M} \{c(x,y)\} \}第一个度量给出关于 $y \in R$ 到 $M$ 中最接近解的平均距离的信息，而第二个度量给出关于最坏情况的信息。值越低，集合 $M$ 越接近集合 $R$。而且，$Dist2/Dist1$ 比值越低，从集合 $M$ 到集合 $R$ 的解分布越均匀。 $\epsilon$-indicator$\epsilon$-indicator 考虑sets之间的最大差异，它是受$\epsilon$-approximation所感，著名的测量设计和比较近似优化算法,运筹学和理论计算机科学。给定两个解集，$\epsilon$-indicator是一个集合在目标中被转换(以加法或乘法的方式)以弱支配另一个集合的最小因子。这就产生了两个版本:加法$\epsilon$-indicator和乘法$\epsilon$-indicator。数学上，解集A对于解集B的加法$\epsilon$-indicator定义如下： \epsilon_+(A,B) = \max_{b \in B} \min_{a \in A} \max_{j \in \{1...m\}} a_j - b_j$a_j$ 是 $a$ 的第 j 个目标，$m$ 是目标函数的个数。解集A对于解集B的乘法$\epsilon$-indicator定义如下： \epsilon_\times(A,B) = \max_{b \in B} \min_{a \in A} \max_{j \in \{1...m\}} \frac{a_j}{ b_j}这两个indicators都是越小越好。$\epsilon_+(A,B) \leq 0$ 或者 $\epsilon_\times(A,B) \leq 1$ 意味着 A weakly dominate B。当用代表PF的参考集R替换B时，$\epsilon$-indicator可以用作一元指标。它衡量的是被考虑的集合到帕累托前沿的距离。但是，由于返回的值只涉及两个集合中一个特解的一个特定目标(其中最大的差异)，指示器可能会忽略大量集合的差异。这可能导致不同执行的解决方案集具有相同/类似的评估结果。 (前提每个目标都是越小越好)对于加法拆解理解：设 $k = \min_{a \in A} \max_{j \in \{1…m\}} a_j - b_j$ 就是说对于B中指定一个解 $B_i$ ,把A中所有的解都减 $k$，那么A中至少(意味着min)存在一个解可以 weakly dominate $B_i$ ；如果遍历所有的$B$，那么需要取最大的那个 $k$，才能满足把 $A$ 中所有解都减掉 $k$ ，对于B任意一个解，A中都存在解可以 weakly dominate。对于乘法同理。 具体例子如图： 可知：$\epsilon_+(A_1,A_2) = 1$,$\epsilon_+(A_1,A_3) = 9/10$,$\epsilon_+(A_1,P) = 4$ $A_1$=(4,7),(5,6),(7,5),(8,4),(9,2) $A_3$=(6,8),(7,7)(8,6)(9,5)(10,4) 因为，想求 $\epsilon_+(A_1,A_3) $ 因此，先遍历 $A_3$ 中的元素，定性上说，在 $A_1$ 中里此元素越远，$k = \min_{a \in A_1} \max_{j \in \{1…m\}} a_j / b_j$越难被选上。 最后可以看到，(9,2) 与 (10,4) 的距离为标准，求得的结果，(10,4) 刚好在边缘上，且也可以看到，横轴间的距离差会比纵轴间的间隔会更大，因为横轴的数值就大。 ObjIGDObjective-wise Inverse Generational Distance(ObjIGD) ObjIGD度量评估MaOOA在每个目标上的收敛性和分布性能。ObjIGD的主要思想类似于IGD度量，然而ObjIGD测量的是PF与最接近的解决方案之间基于一个目标的距离。第i个目标的对象定义如下： ObjIGD_i(S,P)=\frac{ \sum_{j=1}^{|P|} \min_{s \in S}|F_i(p_j)-F_i(s)| }{|P|}$P$ 是 reference($PF_{true}$)。$S$ 是 $PF$ 近似集。$F_i(p_j)$ 是第$ i$ 个目标的第 $j$ 个解，$F_i(s)$ 是近似解的第 $i$ 个目标，因此，整体$ObjIGD$为： ObjIGD(S,P)=\frac{\sum_{i=1}^M ObjIGD_i(S,P)}{M}其中，$ObjIGD_i(S,P)$ 是第 $i$ 个目标的 $ObjIGD$ 的值，$M$ 是目标函数的个数。测度值越低，表明目标的收敛性和分布性越好。 IGD-NS在 IGD 计算中，我们经常发现，一些非支配解往往被忽略，因为它们不是均匀地从Pareto optimal front选取的计算 IGD 的任意参考点的最近邻。这意味着这些非支配解集中的解对集合的IGD值没有任何贡献，因此在逼近帕累托最优前沿方面，它们的重要性低于集合中其他非支配解。因此，我们将这些解称为非支配解集中的无贡献解(noncontributing)。具体地说，无贡献解的定义如下。 解 $y’$ 被认为在解集 $P$ 中，对于解 $P^*$是无贡献解，满足： \nexists x \in P^*:dist(x,y')=\min_{y \in P}dist(x,y)其中，$P^*$ 是一组参考点均匀采样的帕累托最优。从上面的方程,它可以学到无贡献解不是 $P^*$ 中任意点的最近邻点。 在考虑无贡献解的情况下，将提出的性能度量，即带无贡献解检测的IGD的度量(IGD-NS)，定义如下： IGD-NS(P,P^*)=\sum_{x \in P^*}\min_{y \in P} dis(x,y) + \sum_{y' \in P'}\min_{x \in P^*} dis(x,y')$P’$ 是population中无贡献解，上式的第一部分与IGD类似，控制了 $P$ 的多样性和收敛性；然而第二部分是对于每一个无贡献解到 $P^*$ 中点的最小距离的总和。因此，当且仅当满足以下两个条件时，可以得到一个较小(良好)的IGD-NS度规值:首先，种群具有良好的收敛性和多样性;第二，总体包含尽可能少的无贡献解。 个人理解：就是遍历 $P^*$ 中的每一个点，找到与此点距离最近的 $P$ 中的点都删掉，$P$ 中剩下的就是 $P’$ $IGD_p$ IGD_p(X,Y) = \left( \frac{1}{M}\sum_{i=1}^M dist(y_i,X)^p \right)^{1/p}这个就没有什么好说的了。。。 $\Delta_p$ \Delta_p(X,Y)=\max(GD_p(X,Y),IGD_p(X,Y))\\ =\max \left( \left( \frac{1}{N}\sum_{i=1}^N dist(x_i,Y)^p \right)^{1/p}, \left( \frac{1}{M}\sum_{i=1}^M dist(y_i,X)^p \right)^{1/p} \right)这个也是。。。。 $\epsilon$-performanceε-dominance是一个概念,用户可以指定他们想要的精度得到帕累托最优解决方案的多目标问题,在本质上给他们的能力来分配每个目标的相对重要性。这是通过应用一个网格(由用户指定大小的值)问题的搜索空间。$\epsilon$ 值较大导致巨大网格(和最终减少解决方案),而较小的 $\epsilon$ 值产生一个更精细的网格。每种解决方案的健身然后映射到一个盒子健身根据指定的 $\epsilon$ 值。 $\epsilon$-dominance适用于一套参考解根据用户指定的ε值 在每一代,匹配算法生成的每个解决方案,其相应的 $\epsilon$-nondominated参考集解。每个参考解只能有一个与之相关的算法解。如果存在多个解在 $\epsilon$-nondominated 参考集解，然后用欧氏距离最小的解来选择。这考虑了在参考解中的 $\epsilon$ 重叠地区，并且腾出额外的解与其他 $\epsilon$-nondominated参考解。 Each $\epsilon$-nondominated reference solution that has a corresponding algorithm solution receives a score of one, while each reference solution that has no corresponding algorithm solution receives a score of zero. ： \epsilon(P) = \sum_{i=1}^n h_i/n$h_i$ 是 对于 $\epsilon$-nondominated reference set 的第 $i$ 个解，并且 n 是reference set的个数。 这个指标测量了收敛性通过考虑,聚集在 $\epsilon$ 的引用集的解。多样性是占每个$\epsilon$-nondominated引用包括只有一个解决方案的解决方案,不管$\epsilon$-block额外的解的存在性。这可以确保集群解决方案不会对度量的计算产生影响。 说实话没太看懂，，，怎么个对应(corresponding algorithm)法子。 $I_{SDE}$ I_{SDE}(x,y) = \sqrt{\sum_{1 \leq i \leq m }sd(f_i(x),f_i(y))^2}其中： sd(f_i(x),f_i(y))=\begin{cases} f_i(y)-f_i(x) & if \ f_i(x) < f_i(y)\\ 0 & otherwise \end{cases}m 为目标函数个数。需要计算所有的 x 与 y 对。 PCI定义1：p 为一个点，Q为一组点$\{ q_1,q_2,..,q_k \}$ 。p 对 Q 的支配距离定义为p在目标空间中满足 p weakly dominate 所有的 Q 的最小距离： D(p,Q) = \sqrt{\sum_{i=1}^m(p^{(i)}-d(p^{(i)},Q))^2}其中： d(p^{(i)},Q) = \begin{cases} min\{ q_1^{(i)},q_2^{(i)},...,q_k^{(i)} \},&if \ p^{(i)} > min\{ q_1^{(i)},q_2^{(i)},...,q_k^{(i)} \}\\ p^{(i)},&otherwise \end{cases}$p^{(i)}$ 是 解 p 的第 i 个目标，m 为目标函数的个数。 $D(p,Q)$ 只考虑了 Q 中优于 p 的解，无关差于 p 的解。这可以使指标不受收敛性差的参考点的影响，如优势抵抗解。$D(p,Q)$ 的范围是0到无穷，越小越好。如果 p 在少数的目标函数中，轻微差于 Q ，$D(p,Q)$ 会很小，只有 p weakly dominate Q ：$D(p,Q)=0$ 易证： \max\{D(p,q_1),...,D(p,q_k)\} \leq D(p,Q) < D(p,q_1)+...+D(p,q_k)定义2： P，Q为两个解集。P 对 Q 的支配解集 $D(P,Q)$ 。定义如下：对于任意点 $q \in Q$ 中 P 的最小的总距离，使得至少有一个点 $p \in P$ weakly dominate q。 在该指标中，由于参考集由所有的近似集组成，因此一个聚类可以包含来自不同近似集的点。让一个聚类 C 包含 P 和 Q。$P = \{ p_1,…,p_i \}$ 与 $Q = \{ q_1,…,q_j \}$，显然 $D(P,C) = D(P,Q)$ 。当 $i=1$ 时，$D(P,C)$ 是 $p_1$ 对 C 的理想点的支配距离。当 $i \geq 2$ 时，$D(P,C)$ 可以小于 $\min\{D(p_1,C),…,D(p_i,C)\}$。 如上图：ideal point 代表了 每个cluster中每个函数的最小值 cluster $C_1$ ：$P_1$ 只有一个点，因此 $D(P_1,C_1) = (0.5^2 + 0.5^2)^{0.5} = 0.707$ cluster $C_2$ ：$P_2$ 有两个点，为 $D(P_2,C_2)= 0.559 &lt; \min\{1.031,1.25\}$ ,其中1.031与1.25是 $P_2$中的点分别到ideal point的点的距离。 cluster $C_3$： $P_3$ 是一个极端情况， $D(P_2,C_2)=0$，但是如果单个计算的话均为 1。 由此可以知道：当 $i \geq 2$ 时，$D(P,C)$ 可以小于 $\min\{D(p_1,C),…,D(p_i,C)\}$。这是因为共有 $i^j$ 种可能性，对于$p_1,p_2,…,p_i$ 去分开 $q_1,q_2,…,q_j$ ，就是对于每一个 $q$ 都有 $i$ 个可能性被 $p$ weakly dominate。因此，粗略计算如下： D'(P,C) = \max\{ \min\{ D(p_1,q_1),...,D(p_i,q_1) \},\\ ...\min\{ D(p_1,q_j),...,D(p_i,q_j) \} \}当 $i \geq 2$ 时，这仅仅有 $i \times j$ 个比较，尽管 $D’(P,C) \leq D(P,C)$ ，但是当 $C$ 的尺寸小时差距是很小的，例如：$D’(P_2,C_2) = 0.5 &lt; D(P_2,C_2)=0.559$ 与 $D’(P_3,C_3) = D(P_3,C_3) = 0$ 。 首先。所有的解集都要归一化， 如果评估的近似解小于两个，PCI考虑 the minimum move of one solution in the approximation set to weakly dominate the cluster (Step 8 否则计算the minimum move of the set’s solutions in the cluster to weakly dominate the cluster (Step 10 在 cluster 算法中：使用贪心的方法来逐步合并点根据他们的优势距离。设为归一化超平面上具有N个点理想分布的两个相邻点的区间(优势距离的意义)，其中N为参考集的大小。在这种情况下，$\sigma = 1/h$ 与 $N=C_{m-1+h}^{m-1}$ 其中，$h$ 为每个目标的分支，$m$ 为目标函数的个数，因为： (h+m-1)\times (h+m-2)\times ...\times (h+1) \approx (h + m/2)^{m-1}因此： \sigma \approx \frac{1}{\sqrt[m-1]{N(m-1)!}-(m/2)}G-Metric规定：$A_1,A_2,…,A_m$ 是 m 个NSs(non-dominated sets)： Scale the values of the vectors in the NSs Group the NSs by levels of complete outperformance For each level of complete outperformance and for every $A_i$ in the level, calculate the zone of infuence $I_{A_i}$ For every $A_i$, combine its convergence and DE to create a number that represents its relative performance respect to the other NSs Scale and normalization Take the union of the m sets, $C = \cup_{i=1}^m A_i$ From C take its non-dominated elements.$C^*=ND(C)$ Find $max_j$ and $min_j$ as the max and min value respectively, for the component j for all points $p \in C$ 暗指在known pareto front 中挑选。 Using $max_j$ and $min_j$ make a linear normalization of all points in all $A_i$ . Convergence Component已知 $D={A_1,A_2,…,A_m}$ ，其中 $A_i$ 是一个NS 令 $j=1$ 令 $L_j=\{\}$ 从 D 中提取出，并放入 $L_j$ 中，这些 $A_i$ 满足 $ \urcorner ((\bigcup_{A_k \in D }A_k) \ O_C \ A_i)$ 如果 D 不空，那么 j = j + 1，返回到第二步 结束 注意：这是以每一个NS作为整体的。 $L_1$ 是不能被 D 中除了 $L_1$ 的解所completely outperform。如果$A\in L_j$ ,$B \in L_k$ 并且 $j &lt; k$ ，我们可以知道 A 要好于 B，如下图，这有5个NSs ：A，B，C，D 和 E。我们分三个层次： $L_1 = \{A\}$，$L_2 = \{B,C\}$，$L_3 = \{D,E\}$ Dispersion–Extension Component定义1：$I_{p_i}$ 是一些据点 $p_i$ 的距离小于或等于一个正实数 $U$ 的一些点集，U 可以当作为半径。 定义2：$I_S$ 是 $I_{p_i}$ 的并集，$for \ all \ p_i \in S $ 一般来说(对于点或集合)，我们将影响区域称为I。 $I$ 的测量 $\mu(I)$ ：它是对一个点或NS的注入带的测量。它是对一个点或NS的测量。对于 2d 它意味着是面积，在 3d中意味着体积，依次类推。 如果 S 有一个较差的 DE(Dispersion–Extension)，那么他们中的许多都挨着很近，并且相互交叉，结果 $\mu(I_S)$ 就会变小。现在假设我们重新定位S的元素以改进DE。我们通过增加元素之间的扩展和距离，以及/或使它们的距离更均匀来实现这一点(如下图)。随着我们提高了 DE，$I_{p_i}$ 会下降，与此同时，DE 与 $\mu(I_S)$ 都会增加，因此，$\mu(I_S)$ 正比于DE，并且 $\mu(I_S)$ 是一个好的 DE indicator。 $I_S$ 也正比于 $I_{p_i}$ 之间的交叠。具有良好DE的NSs比具有不良DE的NSs重叠更少。 Computing the G–Metric已知 m 个 非支配解集，$A_1,A_2,…,A_m$ 归一化所有的解集 把所有的解集分类成$A_k$ for k = 1~Q ，其中，Q 是等级的数量。 对于每一个 $A_i \in L_k$ ，消除所有的点 $p \in A_i$，满足 $p$ 被另一个点 $q$ 所支配，$q \in A_j$ 对于任意 $A_j \in L_k$ 翻译一下就是：在 $L_k$ 中留下非支配解，其他的都删去。 计算基于所有的 $A_i \in L_k$ 的 U (下面会详细说) 计算 $\mu(I_{A_i})$ 对每一个 $A_i \in L_k$ (下面会详细说) 对于每一个 k = 1~Q-1 对于所有的 $A_i \in L_k$ : G(A_i) = \mu(I_{A_i}) + \sum_{j = k + 1} ^Q \mu_{max}(L_j)其中，$\mu_{max}(L_j)$ 是 对于 $A_i \in L_j$ 最大的 $\mu(I_{A_i})$ . 例如下图： 这段有点不会了，索性直接贴图了。。。。 Dominance move(DoM)我看了四天！！！ 好多证明和推导，在这里就不解释了…. 定义：$n_R(q)$ ，为在 $R$ 中最接近点 $q$ 的点。 距离测量： D(P,Q) = \min_{P' \preceq Q} \sum_{i = 1}^n d(p_i,p_i')\\ d(p_i,p_i') = \sum_{j=1}^m |p^j_i - p_i'^j|$p_i^j$ 是 在 $P$ 中第 $i$ 个解的第 $j$ 个目标函数。$p’$ 是 $p$ 转移到 $p$ 试支配 $Q$，使得：$Q$ 中的任意一点都可以被 $P^‘$ 支配。$m$ 是目标函数的个数。 d(p,Q_s) = \sum_{j=1}^m(p^j - \min\{ p^j,q^j_{s1},q^j_{s2},...,q^j_{sk} \})m$ 是目标函数的个数。 假设计算 $D(P,Q)$ 删除 $Q$,$P$ 个子中的被支配的解，再删除 $Q$ 中被 $P$ 支配(存在一个就行)的点。 设 $R = P \bigcup Q$ ，首先把 $Q$ 中的每一个点当作一组，然后对于 $Q$ 中的每一个点，在 $R$ 中寻找它的最近邻点；对 $Q$ 中的每一个点，寻找到一个 $ r \in R$ ，使 $r = n_R(q)$ ，如果 $r \in P$ ，那么把 $r$ 归为此 $q$ 一组；如果 $ r \in Q$ ，如果 $q$ 和 $r$ 已经在一组，什么都不需要做，否则，把这两组归为一组。 如果在任何组中不存在 $q \in Q$ ，满足：$q = n_R(n_R(q))$ ，即这两个点互为最近邻，那么结束； 对于有环(互为最近邻)的组，用这两个点的理想点取代这两个点，产生新的集合名为 $Q’$ ，寻找在 $P \bigcup Q’$ 中此理想点的最近邻点，并归类为一组，转向，Step.3 举例： 以下为 在收敛性，一致性，延展性，基数性，四个方面做出比较，效果都不错，但有一个很大的问题就是，只能比较二元问题，对于二元以上的存在一些漏洞，证明上不是充要条件。 Volume-based QIsHypervolume(HV)HV首次作为空间的大小所展示，然后被用作几个专业术语hyperarea metric，S-metric，Lebesgue measure。由于HV指标具有理想的实际可用性和理论特性，因此可以说是最常用的QIs。计算HV不需要表示帕累托前沿的参考集，这使得它适合于许多实际的优化场景。HV结果对帕累托优势集的任何改进都是敏感的。当一个集合A优于另一个集合B时(即,一个A◁B)，然后HV返回A的质量值高于B。因此，对于给定的问题，达到最大HV值的集合将包含所有帕累托最优解。 HV indicator 的定义如下。已知解集A和参考点r, HV可计算为： HV(A) = \lambda(\cup\{ x | a \prec x \prec r \})$\lambda$ 表示勒贝格测度，简而言之，一个集合的HV值可以看作是由每个解和参考点(分别为左底顶点和右顶顶点)确定的超立方体的并集的体积。 HV的局限性是它的数量关于目标数而指数增加的运行时间(除非P=NP)。HV的另一个问题是其参考点的设置。对于如何为给定的问题选择合适的参考点仍然没有共识，尽管有一些常见的实践，例如帕累托前沿的最低点或比较解集集合的最低点的1.1倍。不同的参考点会导致HV评价结果不一致[110]。除了少数特殊情况外，关于高压参考点的选择缺乏系统的研究/理论指导。Recently,have demonstrated a clear difference of specifying the proper reference point for problems with a simplex-like Pareto front and an inverted simplex-like Pareto front. 他们还通过实验表明，一个比最低点稍差的参考点并不总是合适的，特别是在多目标优化和/或小群体规模的情况下。此外，HV指标偏向膝关节区域，偏向凸区域多于凹区域。证明，一组达到最大HV值的解的分布很大程度上取决于帕累托前缘的斜率。例如，HV可能倾向于高度非线性帕累托前缘上非常不均匀的解集。这已经得到证明。 hyperarea ratiohyperarea 定义为 $PF_{known}$ 值所包含的空间，例如，在二维目标优化中，就是原点和函数值所覆盖的矩形面积： H = \{ \bigcup_i a_i | v_i \in PF_{known} \}其中，$v_i$ 是 $PF_{known}$ 中的非支配解向量，$a_i$ 是由 $v_i$ 分量和原点确定的超面积。 以下图为例： 被(0,0) 与 (4,4) 所围成的矩形的面积是 16。被 (0,0) 与 (3,6) 所围成的为 (3 x (6-4)) = 6个，依次…结果： $P_{true}$ ‘s H = 16 + 6 + 4 + 3 = 29. $PF_{true}$ ‘s H = 20 + 6 + 7.5 = 33.5. 同时，也注意到：如果 $PF_{true}$ 是 non-convex ，这种测量方法会有错误。它们还隐式地假设MOP的目标空间原点坐标为(0..，0)，但情况并非总是如此。$PF_{known}$ 中的向量可以转换为以零为中心的原点，但是由于MOPs之间每个目标的范围可能完全不同，因此最佳 $H$ 值可能相差很大。也定义了 $hyperarea \ ratio$ 定义如下： HR = \frac{H_1}{H_2}$H_1$ 为 $PF_{known}$ 的超面积，$H_2$ 为 $PF_{true}$ 的超面积。在极小化问题里：ratio 值为 1，当 $PF_{known} = PF_{true}$ ；如果大于 1，即为 $PF_{known}$ 的超面积大于 $PF_{true}$ ，上例中，$HR = \frac{33.5}{29} = 1.155$ 。 Hyperarea Difference (HD)使 $A,B \subseteq X $ 是两个decision vectors，那么，函数 $D$ 定义如下： D(A,B):= \xi (A+B) - \xi (B)所给的是被 $A$ weakly dominate 但是不被 B weakly dominate 的空间的大小(objective space)。 如上右图，A 为 前沿1，B 为前沿2。一方面，$\alpha $ 是被前沿1但不被前沿2所占的大小。另一方面，$\beta$ 是被前沿2但是不被前沿1所占的大小，黑色的区域是被两个都占的大小，因此，$D(A,B) = \alpha$ ，$D(B,A) = \beta$ 。因为： \alpha + \beta + \gamma = l(A+B)\\ \alpha + \beta = l(A)\\ \alpha + \gamma = l(B)\\在这个例子中，$D(B,A) &gt; D(A,B)$ 意味着与C度量相比，这两个方面的差异体现。另外，它给出了集合是否完全支配另一个集合的信息，例如 $D(A,B) = 0$，$D(B,A) &gt;0$ 意味着 $A$ 支配 $B$。 理想下，D 测量常用于被 $V$ 归一化的 $l$ 指标，对于应该最大化问题： V = \prod_{i = 1}^k (f_i^{max} - f_i^{min})$f_i^{max},f_i^{min}$ 是 目标 $f_i$ 的最大值，最小值。可是，也有其他的情况，$V = l(X_p)$ ，表现得也不错。结果，四个值被考虑，当考虑两个解集时，$A,B \in X_f$: $l(A) / V$ ，它给出了目标空间中被 $A$ 弱支配的区域的相对大小。 $l(B) / V$ ，它给出了目标空间中被 $B$ 弱支配的区域的相对大小。 $D(A,B) / V$ ，她给了被 $A$ 弱支配但不被 $B$ 弱支配的区域的相对大小。 $D(B,A) / V$ ，她给了被 $B$ 弱支配但不被 $A$ 弱支配的区域的相对大小。 由于 $D$ 度量是在 $l$ 度量的基础上定义的，因此不需要额外的实现工作。 Volume measure粗略的说$ \mathcal{V}(A,B) $ 是包含严格由 $A$ 的元素支配但不受 $B$ 的元素支配的两条边的最小超立方体的体积的分数(并且在[0,1]区间内)。如下图所示，两个连续的前沿 $A$ 和 $B$ 在目标空间的不同区域存在不同程度的差异，并且相互支配。(目标函数最小化) $\mathcal{V}(A,B)$ 定义如下，对任何D维的向量 $Y$ ，$H_Y$ 为包括 $Y$ 的最小的轴平行超立方体。 H_Y=\{ z \in R^D:a_i \leq z_i \leq b_i \ for \ some \ a,b \in Y \ i=1,...,D \}现在用映射到单位超立方体上的规格化缩放和平移来表示$h_Y(y):H_Y \rightarrow [0,1]^D$。此转换用于消除目标伸缩的影响。相当于 $k = h_Y(y)$ 把原来的点 $y$ 通过缩放与平移到单位超立方体中的点 $k$ 。 D_Y(A)=\{ z \in [1,0]^D:z \prec h_Y(a) \ for \ some \ a \in A \}上式为超立方体中被归一化控制的点的集合，那么$\mathcal{V}(A,B)$定义如下： \mathcal{V}(A,B)=\lambda(D_{A \cup B}(A) \backslash D_{A \cup B}(B) )其中，$\lambda(A)$ 是 $A$ 的勒贝格测量。个人认为： ​ 绿色的部分为$\mathcal{V}(B,A)$ 。 ​ 红色的部分为$\mathcal{V}(A,B)$ 。 尽管这个描述相当繁琐，但是 $\mathcal{V}(A,B)$ 和 $\mathcal{V}(B,A)$ 很容易通过对 $H_{A \cup B}$ 的蒙特卡罗抽样来计算，并计算A或b占绝对优势的样本的比例。本研究选取5万个样本进行蒙特卡罗估计。体积测量 $\mathcal{V}$ 的好处是,它将奖励设置更大的区段当这些区段是前面的比较,而不是当他们在后面,不受点分布方面,而且它也给信息多远一组(平均)面前的另一个地方。 不幸的是，这个测度 $\mathcal{V}$ ，像原来的度规$\mathcal{C}$ 一样，具有这样的性质，如果$\mathcal{W}$ 是一个非支配集，并且 $A \subseteq W$，$B \subseteq W$ ，$\mathcal{V}(A,B)$ 与 $\mathcal{V}(B,A)$ 两者都是积极的。 Integrated Preference Functional (IPF)作为一组在运筹学上已建立完善的QIs，IPF测量由集合中的每个非支配解和给定的效用函数在相应的最优权值上确定的polytopes的体积。它可以被理解为表示解决方案集为DM[16]所携带的预期实用程序。IPF指标的计算分为两个步骤:1)找出每个非优解的最优权重区间;2)对这些最优权重区间上的效用函数进行积分。 形式上，$A \subset \mathcal{R}^m$ 是非支配解集，其中 m 是目标函数个数。考虑一个参数化的效用函数族 $u (a,w)$，其中给定的权重 $w$ 产生一个要优化的值函数，其中 $a \subset A$ 和 $w \in W \subset \mathcal{R}^m$ ,对于给定的 $w$，让 $𝑢^*(A,𝑤)$ 为在A中最好的效用函数值的解决方案。给定权重密度函数$h:W \rightarrow \mathcal{R}^+$ ，表示未知权重w的概率分布，并且$\int_{w \in W}h(w)dw=1$，那么集合A的IPF值为： IPF(A)=\int_{w\in W} h(w)u^*(A,w)dw效用函数可以表示作为目标的凸组合(即加权线性和函数)或加权Tchebycheff函数。前者只考虑受支持的解决方案，而后者则涵盖所有非支配的解决方案。IPF indicator可以在/不需要 DM 的输入的情况下使用。当DM的偏好可以按照某个部分权重空间来表达时，IPF衡量的是该集合在部分帕累托前沿所代表的偏好的好坏。当没有可用的偏好信息时，可以假设所有的权重都是相等的(即$h(W)=1,\forall w \in W$)，IPF 衡量的是这组数据如何很好地代表整体帕累托。较低的IPF值为佳。然而，使用IPF指标的一个限制是，随着目标数量的增加，其计算复杂度呈指数级增长，因为它需要在(连续的)权重空间上进行积分。 以下为论文原文： 对于多目标优化问题，经常使用值(utility)函数法将各种目标函数组合成输入的一个标量函数。这个组合目标可以表示为参数化函数族 $g(x;α)$，一个给定的值参数向量 $\alpha$ 在它的领域 A 中代表一个特定的标量目标，并且目标是最小化。在二元目标的情况下0和1之间的 $\alpha$ 是一个标量,凸组合的情况下的目标。 对于给定的集合 $X$ (多目标函数的非支配解)，对于任意给定 $\alpha$ ，通过 $g(x;\alpha)$ 至少存在一个最优解。对于给定的 $g$ ，定义一个函数 $x_g:A \rightarrow X$，它把参数值( $\alpha$ )映射到 $X$ 中的相应解上。这个函数 $x_g(\alpha)$ 很清晰的把 $A$ 分成了几个区域，反函数 $x_g^{-1}(x)$ ，随着 $x \in X$，定义参数空间 A 在解上的分区，其中 $x_g$ 是常量： A = \bigcup_{x \in X}x^{-1}_g(x) = \bigcup_{x \in X} A_x对于，$x_1 \neq x_2$ ，其中，$A_{x_1} \ and \ A_{x_2}$ ，在二元目标例子中，至多有一个值相同。通常情况下，$A_{x_1} \bigcap A_{x_2}$ 是 对于 $x_1 \ and \ x_2$ 最优解时，两个区域间的边界。在所有实际情况下，这将是一组测度零，不会影响指规数的计算。给定: $h:A \rightarrow R_+$ ,$\int_{\alpha \in A} h(\alpha) d \alpha = 1$, IPF(X) = \int h(\alpha) g(x_g(\alpha);\alpha)d\alpha将解集映射到实数的积分偏好函数。因为 $x_g$ 是(piecewise)分开的常量，上式的积分可以分解成与 $x \in X$ 相对应的 $x_g^{-1}(x)$ 的不同区域。 IPF(X) = \int_{\alpha \in A} h(\alpha) g(x_g(\alpha);\alpha)d\alpha = \sum_{x \in X}\left[ \int_{\alpha \in x^{-1}_g(x)} h(\alpha) g(x;\alpha)d\alpha \right]因此,给定一个 $\alpha$ 的值产生一个特定的目标函数，因为至少有一个最优解在集 $x_g(\alpha)$ 。密度函数 $h(\alpha)$ 分配不同的值给权重向量 $\alpha$ 值,然后 $IPF$ 提供了一个通用的“最优”的解决方案,在已选择权重密度函数。 最后形式的方程表明,我们只需要能够评估积分 $h(\alpha)$ ,因此目标函数的形式是无关紧要的。此外，这里提出的 $IPF$ 测度没有考虑到决策者的任何个人偏好结构，因此可以认为是一般性的。当然，所有这些的主要困难是计算出 $x_g$ 为分段常数的 $A$ 的适当区域，以及计算式(2)中的积分。这些困难取决于函数g、函数h的类型和考虑的目标的数量。 $h(\alpha)$ 如下图： 在实际问题中应用IPF测量，需要对每个目标进行适当的标度。当所考虑的目标是不可比较的(例如，延迟作业的数量和总完成时间)，则无法解释混合的目标值。此外，当每个目标值的范围之间的差异非常大，以至于一个目标值可以被另一个目标值抵消时，将多个目标混合到一个合理的标量值需要适当的缩放。Schenkerman(1990)提出在缩放目标值时，合适的最小值和最大值分别是最大化问题中近似集的非支配最小值和理想点。他还坚持认为，其他的最低要求可能会阻止决策者做出自己喜欢的决定。De et al.(1992)在比较最小化问题的近似集与面积和长度度量时采用了相同的比例方法。正如Gershon(1984)所指出的，规模可以衡量目标的重要性，这影响所考虑的权重。 另外下面是在另一个论文里对 $IPF$ 的介绍，觉得更通俗，就摘过来了。 IPF(Tchebycheff)如Carlyle et al.(2003)所述，决策者的价值函数可以表示为目标的凸组合这一假设意味着只有支持的点才有助于IPF度量。一般来说，当决策者的隐式值函数是非线性的时，这是一个严重的限制，因此，在非支配解集中某些不受支持的点实际上可能是更可取的。在极端情况下，可能会出现这样的问题:受支持的有效解决方案非常少，而绝大多数有效解决方案可能不受支持。在评估非支配点集时，考虑不支持点的影响的一个好方法是使用加权的Tchebycheff函数来表示决策者的价值函数。Tchebycheff函数对应于权重规度$L_p​$ ,在下式中，当 $p = \infty​$ : minimize_{i \in I} \left( \sum_{j \in J} \alpha^p_j (z_j^i - z_j ^{**})^p \right)^{1/p}其中，$I=\{ 1,2,…,n \}$ 是解集的索引，$J=\{ 1,2,…,m \}$ 是目标的索引，$\alpha_j$ 是第 j 个目标的权重，并且 $\alpha_j \geq 0$，同时，$\sum_{j \in J} \alpha_j = 1$ ，$z^i_j$ 是第 i 个解的第 j 个目标函数值，$z^{**}_j$ 是第 j 个目标的理想值。注意到这一点可以通过最小化第 j 个目标本身来找到。当 $p = 1$ 时，测度对应于目标函数的凸组合当 $z^{**}_j = 0$ 时。当 $p = \infty$ 时，权重测度 $L_{\infty}$ 测度为： minimize_{i \in I} [ \max_{j \in J}\{ \alpha_j(z^i_j - z^{**}_j) \} ]加权Tchebycheff函数可用于生成所有非支配点，支持点和不支持点，因此在多目标优化问题中得到了广泛的应用。当使用加权Tchebycheff函数时，集合中的每个非支配点都有一个最优权区间。本文给出加权契比雪夫函数的IPF测度的计算方法。如果我们假设所有的权值都是等可能发生的($h(\alpha ) = 1 ,for \ all \ \alpha \in A​$)。当考虑二元函数时： minimize_{i \in I} [ \max \{ \alpha z^i_1, (1-\alpha) z^i_2 \} ]其中，$0 \leq \alpha \leq 1$ ， 并且考虑两个问题，1). 找出每个非支配点的最优权区间。2).在分解后的最优权值区间上对标量值函数积分。 以下的 Step.1~Srep.3可解释第一个问题，Step.4解释第二个问题。 Step.1：按照解的第一维度升序排好解如下： z_1^1 < z_1 ^2...>z_2^n Step.2：获得 break-even 权重，$\alpha ^i_b$ 对于每一个点 $i \in I$ ，其中，关系如下： \alpha_b^iz_1^i = (1-\alpha_b^i)z_2^i下图a，有在目标空间中的一个点，虚线表示一个break-even点 $\alpha ^i_b$ 。满足上式，并从图b，看出所有的点 $z^i,i \in I$ 满足如下： \alpha^b_i = \frac{z^i_2}{z^1_1 + z_2^i}使用break-even权重，$\max\{ \alpha z^i_1, (1-\alpha) z^i_2 \}$ 对每一个非支配点都可以如下这样： \max\{ \alpha z^i_1, (1-\alpha) z^i_2 \} = \begin{cases} \alpha z_1^i &\alpha \geq \alpha_b^i\\ (1-\alpha_b^i)z_2^i &\alpha \leq \alpha_b^i \end{cases} 定理一：对于所有的不等式，均满足：$\alpha_b^1 &gt; \alpha_b^2&gt;…&gt;\alpha_b^n$ ，具体推导见原论文。 Step.3：对于每一个非支配解，获得一个上界，$\alpha_U^i$，获得一个下界，$\alpha_L^i$ 。 \alpha_U^1 = 1\\ \alpha_L^1 = \alpha_U^2 = \frac{z_2^1}{z_1^2 + z_2^1}\\ .\\ .\\ \alpha_L^i = \alpha_U^{i+1} = \frac{z_2^i}{z_1^{i+1} + z_2^i}\\ .\\ .\\ \alpha_L^n = 0 Step.4：在步骤3分解的最优权重区间上对加权Tchebycheff函数进行积分，对在 Z 中每一个点有如下： IPF(Z) = \int^1_0 h(\alpha) \min_{i \in I}[\max\{ \alpha z^i_1,(1-\alpha)z_2^i \}]d \alpha\\ =\sum_{i \in I} \left( \int_{\alpha^i_L}^{\alpha^i_b} h(\alpha)(1-\alpha)z_2^i d \alpha\\+ \int_{\alpha^i_b}^{\alpha^i_U} h(\alpha)\alpha z_1^i d \alpha \right) 下面为一个实例： R Family与IPF指标类似，R族[73]也将DM s偏好纳入评价。然而，与IPF不同的是，R质量指标中的集成是基于效用函数(而不是权重空间)的。给定两个解集A和B，一个效用函数空间R和一个效用密度函数(U)，可以定义为 R(A,B,U) = \int_{u \in U} h(u)x(A,B,u)du根据结果函数 $x(A,B,u)$，R家族有三个指标。R1考虑DM优先选择其中一个的概率，R2考虑效用函数的期望值(就像IPF指标)，R3引入了基于R2的比值。其中R2使用频率最高，可以表示为 R2(A,B,U) = \int_{u\in U}h(u)u^*(A)du - \int_{u \in U}h(u)u^*(B)du其中，$u^*(A)$ 表示A在此特定效用函数上所获得的最佳值。可以看出，两个集合的R2值可以单独计算。与IPF指标一样，当偏好信息不可用时，$h(u)$可以均匀分布在 $u$ 上。然而，计算中通常使用离散有限集 $U$，这与IPF中考虑的连续集$W$相反。这可以使R2的计算变得友好。特别地，如果效用函数 $u$ 的集合可以用权值 $W$ 的集合和这些权值上的参数化效用函数来表示，那么R2可以进一步计算为 R2(A) = \frac{1}{|W|}\sum_{w \in W} u^*(A,w)与IPF一样，$u(A,w)$ 的物化过程中也存在多项选择，如加权线性和函数和加权Tchebycheff函数，但后者在实践中应用较为广泛。 当h(w)设为1时，将式(13){R2(A)}与式(10)(IPF(A))进行比较，R2和IPF指标表现得非常相似。IPF指标考虑的是一个连续的权值空间，它需要相对于目标维数呈指数增长的计算时间，而R2指标考虑的是一组离散的权值，它的计算速度很快，但其精度自然低于IPF。]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>AllaspectsQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Spread and Uniformity]]></title>
    <url>%2F2019%2F02%2F06%2Fspreadanduniformity%2F</url>
    <content type="text"><![CDATA[过年期间，学习就更加懈怠了。。 前言spread和uniformity的品质方面是密切相关的，需要将它们放在一起考虑，以反映解决方案集的diversity。这激发了QIs去覆盖spread和uniformity的品质。这类QIs大多数可以分为两类，distance-based indicators and region division-based indicators，尽管也有其他的选择，如基于集群的指标和基于容量的指标。如图： Distance-based QIs(59~62)该类中的QIs(表中项目59-62)通常考虑解与其邻域之间的距离，然后将这些距离相加，从而估计整个集合的覆盖范围。沿着这个思路的是 $\Delta$ ,然后是sparsity index，extended spread，and $\Delta_{Line}$ 。然而，这样的评估只能在双目标问题中工作，如非支配解连续位于两个目标上。这些QIs的另一个问题是，它们需要帕累托前沿(例如边界)的信息作为参考，这在实践中往往是未知的 $\Delta$此论文来自于伟大的$NSGA-II$ ！ \Delta = \frac{d_f + d_l + \sum_{i=1}^{N-1}|d_i-\bar{d}|}{d_f + d_l + (N-1)\bar{d}}具体参数可看下图，一目了然。 值得注意的是，这并不是可能的解决方案的最坏情况。我们可以假设$d_i$有很大的方差。在这种情况下，度量可能大于1。因此，上述度量的最大值可以大于1。然而，一个好的分布会使所有的 $d_i$ 距离都等于 $\bar{d}$ 并且会$d_f=d_l=0$(在非支配集中存在极端解)。因此，对于最广泛且均匀分布的非支配解集，其分子为零，使得度规 $\Delta$ 取的值为零。对于任何其他分布，度规的值都大于零。对于两个具有相同$d_f$和$d_l$值的分布，度规$\Delta$取的值较高，而极值解的分布较差。请注意，上述多样性度量可用于任何非支配解集，包括非帕累托最优集的解集。利用三角化技术或Voronoi图方法[1]进行计算$d_i%=$，可以将上述过程推广到估计高维解的spread。 Extended spreadS 是一个解集，$S^*$ 是已知的Pareto-optimal solutions。 $\Delta(S,Q)$ : 原始度规计算两个连续解之间的距离，这只适用于2个目标问题。我们通过计算一个点到它最近的邻居的距离来进行扩展: \Delta(S,S^*) = \frac{\sum_{i=1}^m d(e_i,S) + \sum_{X \in S^*} |d(X,S)-\bar{d}|}{\sum_{i = 1} ^m d(e_i,S) + |S^*| \bar{d}}其中，$\{ e_1,…,e_m \}$ 是 $ S^*$ 中的 m 个极值解，并且： d(X,S) = \min_{Y \in S,Y \neq X}||F(X)-F(Y)||^2\\ \bar{d} = \frac{1}{|S^*|} \sum_{X \in S^*} d(X,S).如果解决的方案有很好的distribute 并且包括这些极值点，那么，$\Delta(S,S^*) = 0$。 $\Delta_{Line}$(不是很懂)The $\Delta_{Line}$ measures the diversity and spread of approximate solutions without the need for the $PF_{true}$ . Let $\beta$ be the mid-points of equally divided intervals in the range of [0, 1] $\left( [0,\frac{1}{N}],[\frac{1}{N},\frac{2}{N}],…,[\frac{N-1}{N},1] \right)$ where $N$ is the number of solutions in approximate the PF, then the objective line distribution ( $\Delta^i_{Line}$ ) is defined as:但是并不是很懂下标的规定。 \Delta^i_{Line}(S,\beta) = \frac{\sum_{j=1}^{|\beta|}\min_{s \in S}|\beta_j - F_i(s)| }{|\beta|}其中，$F_i(s)$ 是第 $i$ 个目标的归一化近似解，第i个目标线分布的零值表示近似PF沿第i个目标的均匀分布。$S$ 是一个粗略前沿PF。 整体线分布测度定义为: \Delta_{Line}(S,\beta) = \frac{\sum_{j=1}^{M} \Delta^i_{Line}(S,\beta)}{M}其中，$M$ 是目标函数的个数。 Region Division-based QIs (63~74)此分类的基本思想把一个特定的空间分割成许多细胞(重叠)，然后计算细胞的数量有解决方案集。这是基于一组更多元化的解决方案通常占据更多的细胞。考虑到细胞的不同形状，大多数用于扩散和均匀性的QIs都属于这一类。 它们中的一些考虑以解为中心的niche-like细胞，比如，Chi-square-like deviation, U-measure and sparsity。 有些则考虑gird-like的单元格将空间划分为多个超盒，比如，cover rate, number ofdistinct choices , diversity metric , entropy and diversity comparison indicator. 其余的考虑fan-shaped(扇形)细胞，它们用一组均匀分布的光线来分割空间(权重向量)，比如，Sigma diversity metric, M-DI and DIR. 除此之外，考虑最小能量点(s-energy)划分空间[74]也是一种潜在的方法，因为它们可以很好地表示各种形状的空间。 Chi-square-like deviation把search space (也就是自变量的空间)，分成几等分，每一个小区域叫做subregion。 v = \sqrt{\sum_{i=1}^{q+1} \left( \frac{n_i-\bar{n_i}}{\sigma_i } \right) } $q$ 是所期望得到最优解的个数。第$(q + 1)$个子区域是受支配的区域 $n_i$ 是第 $i $ 个非支配子区域实际的个数 $\bar{n}_i$ 是第 $i$ 个非支配子区域的期望个数 $\sigma^2_i$ 是第 $i$ 个非支配子区域的个数的方差 通过概率论，他可以由下面估计： \sigma_i^2=\bar{n}_i(1-\frac{\bar{n}_i}{P})$P$ 是种群尺寸 因为并不希望任何的子代落到非支配区域，因此 $\bar{n}_{q+1} = 0$。并且有，$\sigma^2_{q+1} = \sum_{i = 1}^q \sigma^2_i$ 。如果点的分布是理想的话，那么$v=0$。因此，具有良好分布能力的算法具有低偏差度量的特点。 Sparsity index好像没有公式，只有一段话。。。 大体意思就是：找到一个超平面，把每一个解映射过去，并且每一个解占有一定的大小(size=d 的 hyper-box) 越重合就越不稀疏，再把体积求和，当然体积越大越好。主要在于d的取值，不易太大，不易太小。 U-Measure(太多 再说)Cover rateCover rate is the index(指标) for the diversity of the Pareto optimum individuals. The cover rate is derived in the following steps. At first, one of the object functions is focused. Secondly, the distance between the individuals that have the maximum and the minimum values is divided into the certain number of the division. Thirdly, the division area that have the Pareto optimum individuals is counted. Fourthly, the counted number is divided by the number of division. When every divided area has at least one Pareto optimum individual, this number becomes 1. When there are no area that has the Pareto optimum individuals, this number becomes 0. Fifthly, these steps are treated for every objective function. Finally, the cover rate is determined to average the number of each objective function. When the cover rate is close to 1, it means that the Pareto optimum individuals are not concentrated on one point but they spreads. In Figure 4, the concept of the cover rate are shown, when there are two objective functions. 翻译一下就是：如上图，为一个目标函数(个人觉得如果对于一个目标，理应是在一维坐标上)，找到最大值最小值，分成确定的几份，在每个小间隔中，如果有点就为1，否则为0。依次遍历每一个函数，最后求平均值。 Number of Distinct Choices ($NDC_\mu$)从设计者的角度来看，所观察到的帕累托解集中包含的点越多，可供选择的设计选项就越多。然而，如果观测到的帕累托解在目标空间中过于接近，那么对于设计者来说，观测到的帕累托解之间的变化可能无法区分。换句话说，观察到的帕累托解的数量越多，并不一定意味着设计选择的数量越多。简而言之，对于一个观察到的帕累托解集$p=(p_1,…,p_{\bar{np}})$ ，只有那些彼此之间有足够差异的解决方案才应被视为有用的设计选项。 设数量$\mu , \ (0 &lt; \mu &lt;1)$为设计人员指定的数值，可将m维目标空间划分为$1/\mu^m$的小网格。为了简单起见，将$1/\mu$作为整数。每个网格都是指一个正方形(m维中的超立方体)，即无差异区域$T_{\mu(q)}$，其中区域内任意两个解点$p_i$和$p_j$都被认为是相似的，或者设计人员对这些解不感兴趣。下图给出了二维目标空间中的量$\mu$ 和 $T_{\mu(q)}$。 $T_{\mu(q)}(q,P)$ 表示是否有任何点$p_k \in P$属于区域$T_{\mu}(q)$。当至少有一个解点$p_k$落在无差异区域$T_{\mu}(q)$中时，$T_{\mu(q)}(q,P)$等于单元(或1)。$T_{\mu(q)}(q,P)$等于0(或0)只要$T_{\mu}(q)$区域没有解。一般来说，$T_{\mu(q)}(q,P)$可以表述为: T_{\mu(q)}(q,P) = \begin{cases} 1 & \exists p_k \in P \ p_k \in T_\mu(q)\\ 0 & \forall p_k \in P \ p_k \notin T_\mu(q) \end{cases}质量度量$NDC_{\mu}(q)$，即预先指定的m值的不同选择的数量，可以定义为: NDC_{\mu}(P)=\sum_{l_m=0}^{v-1}...\sum_{l_2=0}^{v-1}\sum_{l_1=0}^{v-1}NT_\mu(q,P)where $q = (q_1,q_2,…,q_m)$ with $q_i=\frac{l_i}{v} $ 其中，$v=1/\mu​$ ，点 $q​$ 位于目标空间m-网格线的任意交点上，坐标为$(q_1,q_2,…,q_m)​$。如本节开头所示，，如果想让$NDC_{\mu}(P)​$值较高的观察到的Pareto解集，对于预先指定的 $\mu​$ 就要有相对于较低的值(网格越密，被删去的点就越少)。 $NDC_{\mu}$ 和 cover rate 是有区别的，前者是把目标空间看作整体，并分成了很多个hyper-box；后者是分析每一个维度，最后加权平均一下。 Diversity metric(DM)规定： $P^{(t)}$ 为每一代的种群。$\mathcal{F}^{(t)}$ 是 $P^{(t)}$ 的非支配解。目标(参考点集) $P^*$ 从 $P^{(t)}$ 中确定 $\mathcal{F}^{(t)}$ , 使 $\mathcal{F}^{(t)}$ 非支配于$P^*$ 对于网格的每一个索引 $(i,j,…)$ ，并计算下面两个： H(i,j,...)=\begin{cases} 1,& if \ the \ grid \ has \ a \ representative \ point \ in \ P^* \\ 0,& otherwise \end{cases} and h(i,j,...)=\begin{cases}1,& H(i,j,...)=1 \ and \ if \ the \ grid \ has \ a \ representative \ point \ in \ F^{(t)} \\0,& otherwise\end{cases} 给 $m(h(i,j,…))$ 赋值根据该索引本身与它邻居的$h()$。同样的，用 $H()$ ，来计算 $m(H(i,j,…))$ 计算多样性衡量标准 by averaging the individual $m()$ values for $h()$ with respect to that for $H()$: D(P^{(t)}) = \frac{ \sum_{i,j,...\\H(i,j,..) \ne 0} m(h(i,j,...)) }{\sum_{i,j,...\\H(i,j,..) \ne 0} m(H(i,j,...))}在这个简单的例子中，网格的值函数 $m()$ 可以通过使用它的 $h()$ 和相邻的两个 $h()$ 维度来计算。对于一组连续的三个二进制 $h()$ 值，总共有8种可能。任何值函数的赋值方法如下: 111 是最好的分布，000 是最坏的分布。 010 或 101表示具有良好扩展的周期模式，其值可能大于 110 或 011 。例如，上述估值将使网格覆盖率为50%的近似集具有更大的分布(如1010101010)，优于具有相同覆盖但分布较小的另一集(如1111100000)。 110 或 011 的值可能超过 001 或 100，因为有更多的网格覆盖。 h(…j-1…) h(…j…) h(…j+1…) m(h(…j…)) 0 0 0 0.00 0 0 1 0.50 1 0 0 0.50 0 1 1 0.67 1 1 0 0.67 0 1 0 0.75 1 0 1 0.75 1 1 1 1.00 对于 $H()$ 使用相同的值。在目前的研究中，通过计算上述度量维度来处理两个或多个维度的超平面，而通过考虑一组移动的超盒来非常谨慎设计地上述值函数的一个高维版本。对一个包含9个盒子的二维集合的考虑如下: 作为上述计算过程的说明，下图显示了一个两目标最小化问题的一组目标点(标记为填充圆$P^*$)和一组总体点(标记为阴影和打开的方框 $P^{(t)}$ )。用阴影框标记的点是相对于目标点的非支配点( $\mathcal{F}^{(t)}$ )，用于多样性计算(这是步骤1)。这里以f2 =0平面为参考平面，将 $f_1$ 值的完整范围划分为G=10个网格。下一步，计算每个网格的 $h()$ 和 $H()$ 值。对于边界网格(极端网格和网格$(…,j,…)$ 与 $H(…,j - 1 ….)= 0$ 。 在边界处的网格，假设一个假想的相邻网格的h()或h()值为1，例如上图的虚线格子。也注意到，有的格子里有不止一个点在其中，但也就算一个。移动的三个格子，确定中间位置的数值。为避免边界效应(使用虚网格的效应)，我们将上述度量归一化如下: \bar{D}(P^{(t)}) = \frac{ \sum_{i,j,...\\H(i,j,..) \ne 0} m(h(i,j,...)) -\sum_{i,j,...\\H(i,j,..) \ne 0} m(0)}{\sum_{i,j,...\\H(i,j,..) \ne 0} m(H(i,j,...))-\sum_{i,j,...\\H(i,j,..) \ne 0} m(0)}0 为值为零的数组。仔细想想就会发现，计算上述$\bar{D}(P^{(t)})$项和边界网格调整时$H(i,j…) \ne 0$ 允许使用一种通用方法来处理具有断开的pareto最优前端的问题。该度量不包括不存在参考解的网格的值函数。 如果不知道帕累托最优前沿(特别是对实际问题)，则可以用以下方法确定目标集。 首先，MOEA运行T代，并存储按代计算的总体($P^{(T)}， t = 0,1…T)$)。 然后,将每个种群的非支配成员 $\mathcal{F}^{(t)}$ 组合在一起，则target set 就是这些的总和。 P^*=Non-dominated(\cup^T_{t=0}\mathcal{F}^{(t)}) Diversity comparison indicator (DCI)很巧这里介绍了DM的缺点： a reference set，它要求是均匀分布在PF，这是必需的，以便准确反映分布的optimal front。也是要求，解决方案参考集近似近似的解决方案的数量以保证理想的分布近似可以达到最佳的DM值(一个)。这些要求在多目标优化问题中通常是不可用的。 DM需要访问网格中的每个hyperbox来估计分布，这对数据结构和计算成本都带来了很大的挑战。对于m个目标的优化问题，需要考虑 $r^{m-1}$ 超盒，其中r为每个维度的划分数。 在超盒的分布估计中，DM需要通过一个值函数给每个相邻的超盒分配一个合适的值，以区分其邻域内解的不同分布。由于超盒的邻居数量随着目标数量的增加呈指数增长(m维超盒最多有($3^m$-1)个邻居)，当涉及大量目标时，很难定义准确反映不同分布的值函数。 由于网格中解的邻域的指定，DM可能无法给出具有大量目标的近似的精确分集结果。DM中解的邻域的设置是基于解的网格坐标的曼哈顿距离，而不是它们的欧氏距离。它可能会误导性地消除相邻解，但将更远的解视为相邻解。 网格的位置和大小在该指标中具有重要意义。设置网格区域不应涉及整个目标空间，而应针对给定问题的帕累托前沿不远的区域，因为不同近似有意义的多样性比较的前提是它们已经接近最优前沿[50]。假设较高和较低的网格边界为：$LB=(lb_1,lb_2,…,lb_m)$ and $UB=(ub_1,ub_2,…ub_m)$ ,m 是目标函数的个数，如果一个解向量$(q_1,q_2,…,q_m)$ 在 $LB$ 与 $UB$ 之外，(也就是说 $k \in \{ 1,2,…,m \}:q_k ub_k$ ) 那么此解向量在indicator calculation 中会被忽略掉。 在将所提出的DCI应用于不同问题时，网格边界可以由用户定义的“满足区域”来确定，也可以由问题的理想点和最低点来设置。“满足区域”是用户的一种估计，即在该区域中所获得的解被认为满足收敛性的质量要求。当用户没有明确规定他/她“满意的地区,”网格边界可以通过给定问题的理想点和最低点(下图所示),理想点和最低点是两个重要的概念在多目标优化中,当PF是未知时他们可以通过一些有效的方法估计。在这里，将一个问题的理想点和最低点所构成的区域的轻微松弛看作网格环境： ub_k = np_k + \frac{np_k - ip_k}{2 \times div}\\ lb_k = ip_k其中，$ip_k$ 是第 $k$ 个目标的理想点，$np_k$ 是第 $k$ 个目标的最低点，$div$ 是一个常数(一个维度中目标空间的划分数，例如下图为5) 根据网格的边界和划分的数量，第 $k$ 个目标中的超盒大小 $d_k$ 可以形成如下图所示： d_k = \frac{ub_k-lb_k}{div}在这种情况下，通过下边界和超盒尺寸可以确定解在帕累托前近似中的网格位置如下(向下取整): G_k(q)=\lfloor (F_k(q)-lb_k)/d_k \rfloor其中$G_k (q)$ 表示第 $k$ 个目标中解 $q$ 的网格坐标。$F_k(q)$ 是 $q$ 在第 $k$ 个目标中的真实值。上图中，A，B，C，D 的坐标一次为 (0,4)，(0,3)，(2,2)，(4,0)。以下在引入关于距离的两个概念 $h_1,h_2$ 是网格中的两个超立盒子，那么两个网格的距离为： GD(h_1,h_2)=\sqrt{\sum_{k=1}^m (h_1^k - h_2^k)^2}$h_1^k，h_2^k$ 是 $h_1,h_2$ 的在第 $k$ 个目标函数的坐标。$m$ 是目标函数总数。例如 B 与 C 距离为 $\sqrt{(0-2)^2 + (3-2)^2} = \sqrt{5}$ . P 是也该粗略解集， h是一个超方体盒，从 P 到 h的最短格距离为： D(P,h) = \min_{p \in P} \{ GD(h,G(p)) \}例如，上图中 在粗略解集A，B，C，D中距坐标为(1,3)的超方体盒子的距离为 $GD(h^{(1,3)},G(B)) = 1$ 。显然，在网格环境中解分布均匀且分布广泛的近似，其到所有超盒的平均距离值较低。 不同的帕累托前近似解可能位于不同的超盒中。在这里，我们只考虑在混合逼近集中非支配解所在的超盒，因为受支配解的多样性对用户来说可能毫无意义。对于一个近似解，如果它的解覆盖或接近所有被考虑的超盒，那么与其他近似相比，它将获得一个相对较好的多样性;另一方面，如果它的解决方案远离大多数这些超盒，则会获得相对较差的多样性。算法1给出了计算待比较近似的DCI值的主要步骤。 贡献度(算法1的第6行)反映了近似对超盒的贡献，并由它们之间的距离决定。对于近似，如果所考虑的超盒中至少存在一个解，则可获得该近似对超盒的最大贡献程度。如果从近似值到超框的距离大于指定的阈值即，则贡献度为0。具体地说，近似P对超盒h的贡献程度定义为： CD(P,h) = \begin{cases} 1 - D(P,h)^2 / (m + 1) &d(P,h) < \sqrt{m+1}\\ 0 &d(P,h) \geq \sqrt{m+1} \end{cases}值得指出的是，我们将网格距离的阈值设置为$\sqrt{m+1}$，是为了保证相邻的两个个体始终能够交互(即，它们所在的超盒总是相邻的)。直观地说，如果两个超盒的个体可以任意接近，那么它们应该被视为邻居(在个体之间没有另一个超盒)。显然，满足上述条件的最远的两个超盒是网格距离为$\sqrt{m}$的对角超盒。由于超盒之间的网格距离始终为离散值$\sqrt{0},\sqrt{1},…,\sqrt{m},\sqrt{m+1}…$，将阈值设置为$\sqrt{m+1}$，只是使这些超盒对角相邻，在计算贡献度时可以相互作用。 图3为不同目标数下贡献度函数曲线。注意，贡献度取一个离散值，因为$D(P,h) \in \{\sqrt{0},\sqrt{1},…,\sqrt{m},\sqrt{m+1}…\}$。从图中可以看出，一些观察结果如下: 贡献度取0到1之间的值。在一定范围内，从近似到超盒在一定范围内，它单调地减小。 hyperbox的邻域半径随着目标数量的增加而增加。这表明，当目标数量增加时，可以考虑更大范围的个体进行交互。 当距离变量D(P, h)相等时，贡献度随着目标个数的增加而增加。这种增加似乎是合理的，因为随着网格中超盒总数的增长，超盒之间的相对距离变得更小。 总体而言，贡献度函数不仅考虑了近似到超盒的距离信息，还考虑了目标个数不同的网格环境的性质，对目标个数的变化具有良好的适应性。实际上，任何形式的函数都可以通过记住上述性质来赋值为贡献度函数。为了简单起见，这里使用二次函数。 根据贡献度函数，近似的DCI值是[0,1]区间内的一个数值。需要重申的是，DCI只是评估不同的帕累托前近似的相对分布质量，而不是为单个近似提供分布的绝对度量。最佳价值(即由近似得到的DCI = 1)不能反映其在整个帕累托前缘的均匀分布。相反，它表明该近似比其他近似有一个完美的优势。 上图展示了DCI的计算过程，有三个二元目标问题的pareto approximation $P_1,P_2,P_3$ ，并放在了网格环境中，有11个超方盒(灰色)被决定，其中解A，B并没有考虑其中，因为他们在混合解中被支配了。然后，对于每个超盒，根据前面提到的公式式计算三种近似的贡献度。比如，当考虑$h^{0,7}$ 时，$P_2$ 的贡献值为1，因为它在超方盒中。对于$P_1$ 来说：$1-1^2/3=2/3$ 作为$D(P_1,h^{0,7}) = 2/3$ ，对于$P_3$ 为0，因为$\sqrt{10} &gt; \sqrt{3}$ 。最后，根据算法1(第10行)求出各近似对这些超盒的平均贡献度，分别为：0.848，0.606，0.515。 M-DIM-DI 是在 DCI 的基础上修改的。 在DCI方法中，将各种算法的NDFs(non-dominated fronts)集合在一起，识别出一组帕累托最优解。使用由网格划分参数定义的网格，将每个算法的贡献与组合的帕累托最优解进行比较。 假设有两组帕累托前近似 $P_1$ 和 $P_2$ ，如下图所示。在这种情况下，$P_2$ 是组合的Pareto front，它的DCI度量是最高的(值1)，但是我们可以看到，这个值并没有反映出在front的极限之间的目标空间中解的均匀分布。在没有关于POF(Pareto Optimal Front)的任何资料的情况下，如果假定有一个连续的front，$RTF$ 很可能提供尽可能最好的解决办法。 在M-DI中，多样化是相对于RPF(Reference Pareto Front：单位截距在超平面上均匀分布的一组点)计算的。Nadir and Ideal point 计算方式与 DCI 相同。在RPF上的reference的数量 $W$ ，与在优化进程期间的人口尺寸 $N$ 有关 。For example, a population of 90 used for a 3-objective optimization problem would mean use of 91 reference points on the RPF. 其中 $CD(P,h)$ 仍不变，$h$ 是被RPF所占据的hyper-boxes。而不是在DCI 中的把所有解集合在一起，取出非支配解所占的hyper-boxes，因此： M-DI = \frac{\sum_{i=1}^{|h|}CD(P,h_i)}{|h|}EntropyIdeal/good points： 将理想点定义为目标空间中的一个点，该点的分量分别由目标函数的约束最小化得到： Minimize \ f_i(x) \quad s.t.:x \in DNadir/bad points： 在此论文中是，在本文中，我们任意地高估了目标的范围，以至于没有遇到违反估计上限的设计点。 Influence Function： 在决策空间中，第 i 个解的影响函数$\Omega_i:F^m \rightarrow R$ ，$\Omega_i$ 是随第 i 个解而下降的函数，种类很多，本轮中选择 Gaussian influence function。 \Omega(r) = \frac{1}{ \sigma \sqrt{2 \pi} } e^{-r^2/2\sigma^2}Density Function： 将可行目标空间各点的密度函数定义为各解点影响函数的集合，设共有 N 个解点，可行目标空间 $F_m$ 中任意点 $y$ 处的密度函数可得： D(y) = \sum_{i=1}^{N} \Omega_i(r_{i \rightarrow y})$r_{i \rightarrow y}$ 是一个标量，它展示了 $y$ 与 第 $i$ 个解点的欧式距离。$\Omega_i( . ) $ 是 第 $i$ 个点的影响函数，下图展示了在一维里一些点的影响距离。 Entropy： Claude Shannon 引入信息论熵来测量随机过程的信息含量，从而建立了信息论领域。从那时起，熵的许多不同的应用在不同的领域有他们自己的解释和定义。假设一个随机过程有n个可能的结果第i个结果的概率是。这个过程的概率分布可以表示为： P = [p_1,...,p_i,...,p_n];\sum_{i=i}^{n}p_i=1; p_i \geq0这个概率向量有一个相关的Shannon s熵，H的形式这个概率向量有一个相关的Shannon s熵，H的形式： H(P) = - \sum_{i=1}^n p_i \ ln(p_i)其中， 当 $p_i = 0$ 时，$p_i \ ln(p_i)=0$。最大值为 $H_{max}=ln(n)$ 当所有的值都相同的，最小值为0，当一个为1，其他的所有均为0。事实上，香农熵衡量的是 $P$ 的平整度，即。，如果向量中各分量的值近似相等，则熵值很大，但如果各分量的值相差很大~概率分布不均匀!，对应的熵值较低。 如下图所示 网格的尺寸是 $a_1 \times a_2$ 在feasible domain。对每一个密度函数，$D_{ij} = D(y_{ij})$，$a_1$ 和 $a_2$ 的数量确定为每个单元格的大小小于或等于无差异区域(无差异区域定义为任意两个解点被认为是相同的单元格大小，或决策者对这些解不感兴趣)。$a_1$ 和 $a_2$ 的数量可以根据设计者的经验或对类似问题的认识主观确定；或者客观地基于可用的计算能力和期望的精度。假设非常小的网格大小有助于提高准确性，但它也增加了计算熵的计算负担，这反过来可能使质量评估过程非常缓慢，甚至在计算上不可行。显然，适当的网格大小取决于问题，并且在不同的情况下有所不同。因为这些项的和。在香农熵的定义中，熵是1，我们定义一个归一化密度，$\rho_{ij}$ ，为: \rho_{ij} = \frac{D_{ij}}{\sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2} D_{k_1k_2}}实际上，上述归一化密度的定义对于空解集的定义并不好，这就是为什么在密度函数的定义中假设解集是非空的原因。我们将给空解集的熵赋值为0来表示最坏的情况，即0的多样性。现在我们有: \sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2} \rho_{k_1k_2} = 1\\ \rho_{k_1k_2} \geq 0,\forall k_1,k_2这样一个分布的熵可以定义为: H = -\sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2} \rho_{k_1k_2}ln(\rho_{k_1k_2})并且，对于m维目标空间，将目标空间中的可行域划分为a13a23。3am细胞，熵定义为: H = -\sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2}...\sum_{k_2=1}^{a_m} \rho_{k_1k_2...k_m}ln(\rho_{k_1k_2...k_m})熵值越大的解集在目标空间的可行域内分布越均匀，覆盖范围越广。 Sigma diversity metric提出了计算目标空间中解的位置的Sigma多样性度量。 如下图： 上左图，对于每一个线，都有$f_2 = af_1$，因此，$\sigma $ 便为： \sigma = \frac{ f_1^2 - f_2^2 }{f_1^2 + f_2^2}所有的在此线上的点都满足 $f_2 = af_1$ ，也就都有相同的 $\sigma = \frac{1-a^2}{1 + a ^2}$ 在一般情况下，$\sigma$ 被定义为 $ C_m^2$ 个元素的向量，其中m是目标空间的维数。在这种情况下，$\sigma$的每个元素都是上式中两个坐标的组合。例如f1、f2、f3三个坐标，定义如下:(如上又图) \sigma =\begin{pmatrix} f_1^2 - f_2^2 \\ f_2^2 - f_3^2 \\ f_3^2 - f_1^2 \end{pmatrix} /(f_1^2 + f_2^2 +f_3^2)例，$\sigma = (0 \ 0.5 \ -0.5)$ 时，$f_1 = 1,f_2 = 1,f_3 = 0$ 带入上面即可。 这意味着目标空间中的每个点都可以用向量来描述。直线上的所有点都有相同的向量在非常接近的直线上的解都有相似的向量。这是用来构造多样性度量的思想。 在计算近似集的多样性之前，必须先计算一组reference lines(参考线)。参考线数必须等于近似解的个数。必须强调的是，每个目标的参考线必须计算一次，并且可以存储在一个表中。那么Sigma多样性度量可以计算如下: 计算参考线 计算每条参考线的向量(参考向量 reference sigma vector)。 在每个参考向量旁边保留一个初始值为零的二进制标记。每个引用的旗帜 $\sigma$ 向量为 1,当至少有一个解决方案有一个向量 $\sigma$ 等于它或在一个距离(欧式距离)低于d。d的值取决于测试函数,但是它应该减少当有大量的参考线时。 计数器C对标记为1的引用行进行计数，多样性度量D变为： D = \frac{C}{number \ of \ reference \ lines}Sigma多样性度量表示的是在目标空间中非支配解的分布百分比 在极值解位于坐标轴上的情况下，我们可以得到D的一个高值，即,100%。对于离散解集或不连通解集，D的值永远达不到最大值。 由上式中的D可知，如果D的值很高，就意味着解的分布很好。但当D很小时，它意味着解是： 集中在空间的一部分 分布在前沿的小群体中 的确，这两种解决方案之间存在差异。下图显示了具有相同D值的两组解之间的差异。这两组解有不同的扩展，Sigma多样性度量无法区分它们。 红线是红点所占的reference lines；黑线是红黑点所占的reference lines。可以看到都是6条！ $\mathcal{M} ^* _2$Analogously, we define one metric $\mathcal{M}_2^*$ and on the objective space. Let $Y’,\bar{Y} \subseteq Y$ be the sets of objective vectors that correspond $X’$ to $\bar{X}$ and , respectively, and $\sigma ^* &gt; 0$ and $|| \cdot ||^*$ as before: 公式： \mathcal{M}^*_2({Y'}):=\frac{1}{|Y'-1|}\sum _{p'\in Y'} \{q' \in Y'; ||p'- q'||^* > \sigma ^*\}]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>diversity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Cardinality]]></title>
    <url>%2F2019%2F01%2F28%2Fcardinality%2F</url>
    <content type="text"><![CDATA[这部分要么太难，要么太简单。。。。怀疑自己智商 介绍基数的QIs可以归结为一个简单的概念——计算非支配解决方案的数量。基于基数的QIs所具有的一个可取(必要)的属性是，在考虑的集合中添加一个不同的非支配解决方案应该能够改进(而不是降低)评估结果。这(弱)单调的概念是一致的。下面列出了10个基数QIs(项目45-54)： 根据Pareto最优解的参与程度，基于基数的QIs可以分为两类(项目45-48和项目49-54)。 一种是直接考虑集合中的非支配解，例如 the indicators cardinality 𝐷 number of unique nondominated solutions overall nondominated vector generation (ONVG) ratio of nondominated individuals 另一类是比较解集中的非支配解(nondominated solutions)和问题的帕累托最优解(Pareto optimal solutions)，该类中的QIs通常返回属于帕累托最优集的非支配解与最优集大小的比值，(例如：C1，ONVG ratio）,或者或者与解集本身的比值(例如，C2，error ratio) 除此之外，还有其他一些指标，它们只是简单地计算属于最优集的解决方案的数量。 虽然这里描述的是对最终MOEA性能的度量，但是其中许多度量也可以用于跟踪世代总体的性能。然后，除了一个总体性能度量之外，它还指示执行期间的性能(例如，到MOEA optimu的收敛速度)。虽然使用了两个目标的例子，但是这些指标可以扩展到具有任意数量的目标维度的PF。 由于解决方案集的基数通常几乎没有与帕累托前沿的代表性相关的信息，因此通常认为它不如其他三个质量方面重要。然而，如果优化器能够找到问题的很大一部分帕累托最优解决方案，那么评估基数质量可能会变得更加合理。这对于一些组合多目标优化问题尤其适用，其中帕累托最优解的总数很小。在这类问题中，计算得到的帕累托最优解的个数是反映解集质量的可靠指标。事实上，这种评价在一些组合问题的早期研究中经常使用。 Error ratio此indicator与onvg与onvg ratio 出自同一本书，并且有如下定于，为了方便引用原文： 简而言之：$P$ (也就是 pareto optimal set 包含于solution set) 是针对自变量来说的 也就是decision space。 $PF$ (也就是 Pareto Front ) 是针对目标函数值来说的，也就是objective space An MOEA reports a finite mumber of vectors in $PF_{known}$ which are or are not members of $PF_{true}$. If they are not members of $PF_{true}$ the MOEA has erred or perhaps not converged. This metric is mathematically represented by: E=\frac{\sum_{i=1}^{n}}{n}e_iwhere n is the number pf vectors in $PF_{known}$ and e_i = \begin{cases} 0 & if \ vector \ i,i=(1,...,n) \in PF_{true},\\ 1 & otherwise \end{cases}$E=0$，表明$PF_{known}$都在 $PF_{true}$。 $E=1$，表明$PF_{known}$中一个都不在 $PF_{true}$。 上图的值：$E=\frac{2}{3}$ 。 我们还注意到一个类似的度量，它度量由另一个 $P_{true}$ 支配的$P_{known}$ 中的解决方案的百分比。 然而，由于ER只在帕累托最优解中起作用，它可能会带来一些反直觉的情况。例如，在一个集合中添加更多的非支配解决方案可能会导致更差的分数。因此，考虑比较集本身中的非支配解可能是更好的选择，而且也不需要帕累托前沿。 Overall Nondominated Vector Generation and Ratio(ONVG)测试的MOEAs 每一代把 $P_{current}$ 添加到 $P_{known}$ 中，可能导致不同的数量的$P_{known}$ 。这个测量度计算的是在MOEA期间所找到的所有非支配解的数量，定义如下： ONVG = |PF_{known}|Schott 使用这个测量标准(尽管是在帕累托最优集上定义的，例如$|P_{known}|$ ) 。基因型或表现型地的定义这个测量标准可能是偏好问题，但是我们再一次注意到多个解可以映射到相同的向量上，或者换句话说，$|P_{known}| \geq |PF_{known}|$ (多对一)。尽管计算非支配解的数量可以让我们了解MOEA在生成所需解方面的有效性，但它并没有反映出$|P_{known}|$中的向量与$|PF_{known}|$ 之间的“距离”有多“远”。此外，太少的向量和$|PF_{known}|$的代表性可能很差;太多的向量可能会压倒DM。 ONVG Ratio很难确定$|ONVG|$的最佳值是多少。$PF_{known}$ 的基数可能在不同的计算分辨率下发生变化，也可能在拖把之间存在差异(可能是根本的)。报告$PF_{known}$的基数与离散$P_{true}$的比值可以让我们对找到的非支配向量的数量与要找到的存在向量的数量有一定的感觉。然后将这个度量定义为: ONVGR = \frac{|PF_{known}|}{|PF_{true}|}上图中，可知 $ONVG=3$，$ONVGR=0.75$。 C1如果已知由所有有效解组成的参考集R，那么看起来最自然的质量度量就是找到的参考点的比例。度量可以用以下方式定义： C1_R(A)=\frac{|A \cap R| }{ |R| }C2如果参考集不包含所有的非支配点，那么A中的被R所非支配的点集也许是属于非支配解集上的，这种情况，使用下列方法也许更可行： C2_R(A)=\frac{ | \{ u \in A | \ \nexists r \in R , r \succ u \}| }{|A|}然而，这些主要措施也有一些明显的缺点。他们对近似值的改进无动于衷。例如，考虑图11中所示的两个近似参考集，这两个近似是针对一个双目标背包问题得到的。显然，近似1比近似2好得多。近似1中的所有点都非常接近参考集，它们覆盖了参考集的大部分区域。然而，这两种近似的测度值都是相同的C1和C2。 图12中的示例说明了主要度量的另一个缺点。这两个近似由5个不占主导地位的点组成，所以它们的基数测度是相等的。然而，构成逼近3的所有点在目标空间中都是非常接近的，即它们代表了非支配前沿的同一区域。另一方面，近似值4的点分散在整个参考集合中。它们携带着丰富得多的信息，例如关于可能的目标范围的信息。这个例子表明，对于基本测度，无论它们的接近程度和关于非支配集形状的信息如何，逼近中的每个点都具有相同的权重。 Ratio of non-dominated individuals (RNI)这个绩效指标被定义为非主导个体的比率(RNI)对于给定的总体X， RNI(X)=\frac{nondom\_indiv}{P}nondom_indiv是种群X中非支配的个体数量，P是种群X的大小。因此，RNI = 1的值表示种群中所有的个体都是不受支配的，RNI = 0表示种群中没有一个个体是非支配的。由于通常需要大于零的总体大小，所以在$0 \leq RNI \leq 1$的范围内总有至少一个非支配个体。]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>CardinalityQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Uniformity]]></title>
    <url>%2F2019%2F01%2F26%2Funiformity%2F</url>
    <content type="text"><![CDATA[又看了好几天的剧。。。罪恶感啊 介绍均匀性的质量指标衡量集的解分布的均匀程度。由于解集的质量可以看作是其表示PF的能力，所以一个均匀分布的解集比一个非均匀分布的解集提供更好的帕累托前沿表示，可以认为它具有更好的质量。理想的均匀性QI应该排在由完全相等的解组成的集合的最高位置，对这个集合的一点干扰会导致更糟糕的评估结果。具体的indicators如下: 通常可以通过测量解之间距离的变化来评价解集的均匀性。这个类中的许多QIs都是按照这些思路设计的，例如，$spacing(SP)$[39],$deviation \ measure \Delta$[35] , $ uniformity \ distribution$[43], $\ minimal \ spacing$[38], $ spacing \ measure$[37] 和$uniformity$[41]。 其他则考虑解之间的最大最小距离[40 36 44]，和构建集群[34]或最小生成树[42]。 Minimal spacing前言先介绍spacing： S=\sqrt{\frac{1}{|Q|} \sum_{i=1}^{|Q|}(d_i-d)^2 } $d_i=min_{k\in Q\ and\ k \ne i}\sum_{m=1}^M|f_m^i-f_m^k|$ 。离$Q_i$最近的点的距离，距离公式每一维度(目标函数)的差值的平均值。 $f_m^i​$：在最后的非支配解$Q​$中第$i​$个解的第$m​$个目标函数值。 $d$：所有$d_i$的平均值。 $S$越接近0越说明解集是更加的均匀分布的帕累托最优前沿。 但此算法在上图所示中便展现出缺点： 可以直观的看出fig(b)的一致性比fig(a)要好，但通过公式却体现出相反的结论。 原因：离a最近的是b，离b最近的a，离c最近的是d，离d最近的是c。那么S值一定比fig(b)的低。而忽略了fig(a)中b与c之间很大的距离。 正文此算法更像是一个流程，总结下来就是把每个解看成一个点，每一个点只访问一次，求把所有点连起来的距离总和的最小值。 把所有点设为unmarked状态，随机找一个解，作为seed，此点变为marked。 在所有的unmarked点中，找到离刚刚设为marked/seed点最近的点。此点设为marked， 依次循环(2)，直至所有点均是marked，并记录路径的距离和。 把每个点都作为seed，取路径和最小。最后再处以$|Q|-1$ 其中，由于每个目标函数的性质可能不同，他们的取值范围也就可能不同，距离公式归一化修改为： d_i=\frac{1}{|F^{max}_m-F^{min}_m|}min_{k\in Q\ and\ k \ne i}\sum_{m=1}^M|f_m^i-f_m^k|$F^{max}_m$，$F^{min}_m$第m个目标的最大值和最小值。 如此算法,易得fig(b)的值会比fig(a)更小，更有效！ Spacing(SP)SP 测量一组解集之间解的距离变化。特别的，$A = \{a_1,a_2,…,a_N \}$, SP(A)=\sqrt{\frac{1}{N-1} \sum^N_{i=1} (\bar{d} - d_1(a_i,A/a_i))^2 }其中： $\bar{d}$ 是所有 $d_1(a_1,A/a_1)$ $d_1(a_2,A/a_2)$ $d_1(a_2,A/a_2)$,…, $d_1(a_N,A/a_N)$ 的平均值，$d_1(a_i,A/a_i)$ 是 $a_i$ 对 $A/a_i$ 的一范数(Manhattan distance)， d_1(a_i,A/a_i)=\min_{a \in A/a_i} \sum_{j=1}^m|a_{ij}-a_j|$m$ 是目标函数的个数，$a_{ij}$ 是第 $a_i$ 的解的第 $j$ 个目标的值。SP被最小化;数值越低，均匀性越好。SP值为0表示解集的所有成员在曼哈顿距离的基础上间距相等。请注意，SP仅测量解决方案的“邻域”分布。即使与MS一起工作，SP也不能涵盖集合的多样性质量，尽管这两个指标在文献中经常一起使用来达到这一目的。以下图为例，图2(b)和(c)中的解集均采用SP和MS满分;然而，它们分别位于帕累托前沿的边界和极端点。 Spacing metric假设有两个目标函数， spacing = \left[ \frac{1}{N-1}\sum_{i=1}^{N-1}(1 - \frac{d_i}{\bar{d}}) \right]为了计算$d_i$，我们考虑第一个目标，将$PF$中的所有点按升序排序。接下来，为了计算$d_i$，我们使用下面的公式: d_i = \sqrt{(f_1(\vec{x_i} )-f_1(\vec{x_i+1}) )^2+((f_2(\vec{x_j} )-f_2(\vec{x_j+1}) )^2}$\bar{d}$ 便为 $d_i$ 的和的平均值。 Deviation measure $\Delta$由于优化解的多样性是多目标优化中的一个重要问题，我们设计了一种基于最终总体中最优非支配前沿解之间连续距离的度量方法。将得到的第一组非优解与均匀分布进行比较，计算偏差如下:(这个有特殊的背景才可适用) \Delta = \sum_{i=1}^{|\mathcal{F}_1|}\frac{|d_i-\bar{d}|}{|\mathcal{F}_1|}$\mathcal{F} = \{ \mathcal{F}_1,\mathcal{F}_2,… \}$ 是所有的非支配前沿。 为了确保这种计算考虑到解在真实前沿的整个区域的扩散，我们将边界解包含在非主导锋$\mathcal{F}_1$中。对于离散的帕累托最优前沿，我们为每个离散区域计算上述度量的加权平均值。在上式中，$d_i$是目标函数空间中最终总体的第一非支配前沿上两个连续解之间的欧式距离。参数$\bar{d}$是这些距离的平均值。 Cluster ($CL_\mu$)需要先介绍 Number of Distinct Choices ($NDC_\mu$). $NDC_\mu$从设计者的角度来看，所观察到的帕累托解集中包含的点越多，可供选择的设计选项就越多。然而，如果观测到的帕累托解在目标空间中过于接近，那么对于设计者来说，观测到的帕累托解之间的变化可能无法区分。换句话说，观察到的帕累托解的数量越多，并不一定意味着设计选择的数量越多。简而言之，对于一个观察到的帕累托解集$p=(p_1,…,p_{\bar{np}})$ ，只有那些彼此之间有足够差异的解决方案才应被视为有用的设计选项。 设数量$\mu , \ (0 &lt; \mu &lt;1)$为设计人员指定的数值，可将m维目标空间划分为$1/\mu^m$的小网格。为了简单起见，将$1/\mu$作为整数。每个网格都是指一个正方形(m维中的超立方体)，即无差异区域$T_{\mu(q)}$，其中区域内任意两个解点$p_i$和$p_j$都被认为是相似的，或者设计人员对这些解不感兴趣。下图给出了二维目标空间中的量$\mu$ 和 $T_{\mu(q)}$。 $T_{\mu(q)}(q,P)$ 表示是否有任何点$p_k \in P$属于区域$T_{\mu}(q)$。当至少有一个解点$p_k$落在无差异区域$T_{\mu}(q)$中时，$T_{\mu(q)}(q,P)$等于单元(或1)。$T_{\mu(q)}(q,P)$等于0(或0)只要$T_{\mu}(q)$区域没有解。一般来说，$T_{\mu(q)}(q,P)$可以表述为: T_{\mu(q)}(q,P) = \begin{cases} 1 & \exists p_k \in P \ p_k \in T_\mu(q)\\ 0 & \forall p_k \in P \ p_k \notin T_\mu(q) \end{cases}质量度量$NDC_{\mu}(q)$，即预先指定的m值的不同选择的数量，可以定义为: NDC_{\mu}(P)=\sum_{l_m=0}^{v-1}...\sum_{l_2=0}^{v-1}\sum_{l_1=0}^{v-1}NT_\mu(q,P)where $q = (q_1,q_2,…,q_m)$ with $q_i=\frac{l_i}{v} $ 其中，$v=1/\mu$ ，点 $q$ 位于目标空间m-网格线的任意交点上，坐标为$(q_1,q_2,…,q_m)$。如本节开头所示，，如果想让$NDC_{\mu}(P)$值较高的观察到的Pareto解集，对于预先指定的 $\mu$ 就要有相对于较低的值(网格越密，被删去的点就越少)。 正文上一节的质量度量，即 $NDC_{\mu}(P)$。然而，仅使用这个质量度量，无法正确解释集群现象。例如，假设有一个预先指定的m值，观察到的Pareto解集$P_1$提供了10个不同的解，有$NDC_{\mu}= 10$。现在假设，这里有另一组解$P_2$ 它提供了100个解，$NDC_{\mu}=10$ 。可以看出，设计人员并不希望看到解决方案集P2，因为该集中的许多解决方案可能是集群的。因此，引入了质量度量集群$CL_\mu(P)$: CL_\mu(p)=\frac{N(P)}{NDC_\mu(P)}其中$N(P)$为观察到的帕累托解的个数。在理想情况下，得到的每一个帕累托解都是distinct的，那么数量$CL_\mu(p)$的值等于1。在所有其他情况下，$CL_\mu(p)$都大于1。此外，集群数量$CL_\mu(p)$的值越高，解决方案集的集群化程度就越高，因此解决方案集的受欢迎程度就越低。 Hole relative size当我们看到这组度量标准，特别是间距度量(spacing metric)标准时，我们意识到它们有时在显示沿帕累托边界(帕累托边界上的一个洞)的点分布的不连续时是不准确的。因此，为了克服这个缺点，我们设计了一种新的度量，称为孔相对大小(hr)。 HRS度量允许计算沿帕累托边界分布的点的最大孔的大小。然后用孔的大小除以点与点之间的平均间距进行归一化。如下所示： HRS = \frac{\max_i d_i}{\bar{d}}$d_i$ 两个相邻解的距离。 $\bar{d}$ 点间的平均距离。 这个度量比间距度量提供的信息更多，但是，在尝试规避间距度量中的一个缺点时，我们在HRS度量中引入了另一个缺点:它不能在不连续的帕累托边界上工作。事实上，不连续的帕累托边界有天然的漏洞。因此，对于帕累托边界上的固定数量的解，HRS度规总是以高概率测量相同的值。因此，我们建议在不连续测试问题中不要使用这个度量。 Uniformity assessment(没看懂，心力憔悴)]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>UniformityQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Spread]]></title>
    <url>%2F2019%2F01%2F23%2Fspread%2F</url>
    <content type="text"><![CDATA[玩了好几天，看了好多剧，所以这几天的进度稍微有点慢，另外，《一起同过窗》真香！ 延展特性涉及解集覆盖的区域。一个具有良好分布的解集应该包含来自PF每个部分的解集，而不遗漏任何区域。然而，大多数扩展的QIs只度量解决方案集的范围(extent)。下表为总结： 这些QIs通常考虑the range formed by the extreme solutions of the set(由集合的极值解构成的范围)，例如maximum spread，和它的变体[25] [27] [28] [29] [30] [33]，或者 考虑the range enclosed by the boundary solutions of the set(集合的边界解所围成的范围)，实例如下图： 只考虑这些解的QIs可能会忽略PF的内部区域。 幸运的是，确实存在一些为解决方案集的整个覆盖范围设计的QIs。 例如，[23]测量解集的支持点的面积和长度；[24]计算解集在PF的最大不相似度；[31]将每个解与解集的其余解的不同之处相加。 Maximum Spread (MS)$MS(or \ \mathcal{M}_3^*)$ 被广泛的使用于延展性indicator。它通过考虑每个目标的最大范围来度量解决方案集的范围，公式为： MS(A) = \sqrt{\sum^m_{j=1} \max_{a,a' \in A} (a_j-a_j')^2 }m 是目标函数的个数。MS应求极大值。值越高，说明的可延展性越好，在二元目标情形下，非支配解集的MS值为其两个极值解的欧氏距离。 但是，如前所述，MS只考虑集合的极值解，不能反映扩散的特性。此外，由于它不涉及集合的收敛性，远离PF的解通常对MS值有很大贡献。这很容易引起误导性的评价。例如，一个解集集中于PF的一小部分，但有一个离PF很远的离群值，那么它的MS值就很好。为了解决这一问题，引入帕累托前缘的范围作为评价的参考，例如[27] [28]。 Extension规定： \min_{x \in X} F(x) = (f_1(x),f_2(x),...,f_l(x))\\ U_i = \max_{x \in Pareto(U)}f_i(x)\\ L_i = \min_{x \in Pareto(U)}f_i(x)\\ i=1,...,l令：$P_r = \{ F_1^1,…,F_l^1 \}$，其中，$F_i^1=(L_1,…,L_{i-1},U_i,L_{i+1},…,L_l),[i=1,…,l]$ 规定： d_r^p=\min\{ d(p_r,p)|p \in P \},p_r \in P_r因此，得表达式： EX=\sqrt{\sum(d_r^p)^2 }/l易知，$d_r^p$ 越小，说明有更好的延展性。 如果是三维图的话，$P_r$ 分别如下： Modified MS(勿看，瞎记的)只有知道正常和期望的条件，才能定义和避免异常和不期望的条件，例如解在目标空间的次优区域的分散，或者收敛到感兴趣区域之外的次优解。换句话说，为了克服收敛性和多样性的矛盾要求，需要一个应用相关的尺度来定义低、理想和高多样性的近似概念，这在高维问题中尤为明显。在所提议的机制的上下文中，决策人员DM1(通常，最好是领域专家)只需要对所需折衷表面的定义极值提出近似估计。这些极值将作为包含理想的PF的超立方体的顶点。 I_s = D / \left[ \sum_{m=1}^M \left( \max_{z_* \in Z_*}\{z_{*_m}\} - \min_{z_* \in Z_*} \{z_{*_m} \} \right)^2 \right]^{1/2}$z_t \in Z_t$ 可以表示PF的目标集。$I_s$ 能取任何正的实数值。理想情况下，要找到一个接近统一($I_S = 1$)的指标值(理想的多样性)。小于1 ($I_S &lt; 1$)的指示值表示与期望的解决方案的扩展相比，操作的解决方案之间的多样性较低。另一方面，指示符值大于1($I_S &gt; 1$)突出了目标空间中解的过度分散(高多样性)。这种超空间的过度分散很可能导致解与PF的发散，并通过引入循环行为，迫使MOEA反复探索空间中以前访问过的区域，从而阻碍了优化过程。 第二个多样性管理机制是DM2，它预测NSGA-II中使用的多项式突变算子可能会使潜在的解点广泛分散。DM2试图通过引入自适应突变算子，以一种可控的方式控制这种离散。这个新的变异算子试图定义组决策变量的变异范围在每一代的基础上的多样性程度的局部non-dominated集解决方案,为每个单独的决策变量中设置,在当地的多样性以NSGA-II年代拥挤的措施。 计算第i代近似集的扩展指标。 if $I_s&lt;1$ 在变异选择和生存选择过程中激活多样性促进机制。 Else If $I_s \geq 1$ 在变异选择和生存选择过程中，失活多样性促进机制。 Coverage error $\epsilon$$\epsilon$ 的概念： 解释一下就是：有两个集合$D,Z$，$D$ 是 $Z$ 的一部分，如果想要用 $D$ 代表 $Z$，那么就要用符号 $d_{\epsilon}$ 表示。并规定，遍历 $Z$ 中的每一个点，画一个圆，半径是 $\epsilon$ ，都要有 $D$ 中的解存在，并且找最小的 $\epsilon$。 $\delta$ 的概念： 翻译一下：这个是单对 $D$ 集合来说的，$D$ 中两两点的最小距离。 例子如下：实心 + 空心 = Z；实心 = D 因此 $\epsilon$ 要尽可能的小，$ \delta$ 尽可能的大。 \epsilon = \max_{z \in Z} \min_{x \in D} d(z,x)For a fixed element $z$ of $Z$, how well it is covered is determined by the closest point to $z$ in the representation $D$. How well the entire set $Z$ is covered depends on how well an arbitrary element of $Z$ is covered, and thus the coverage error \epsilon$ is equal to the maximum of coverage error quantities for individual points in Z. Similarly, the uniformity level $\delta$ is determined by the quantity. \delta=\min_{x,y \in D,x \ne y}d(x,y)the fact that $D$ is of finite cardinality, computing the uniformity level $\delta$ is simple as long as the metric $d$ is computable. PD PD(X) = \max_{s_i \in X}(PD(X-s_i)+d(s_i,X-s_i))where d(s,X)=\min_{s_i \in X}(dissimilarity(s,s_i))$d(s_i,X-s_i)$ 是从一个物种 $s_i$ 到另一个种群 $X$ 的相异度。 下图提供了一个方式展示了PD是如何计算的，在左图,解$s_i$和其他方案 $X−si$ 视为两个社区，他们的多样性之和是 $X−si$ (black dots)的和 与 $si$ 到 $X−si$ 的相异值的和组成： 每个解与整个总体的不同之处是可以计算的，每个解都与其最近的未复制邻居相关联。然后，这些差异的和导致了整个种群的多样性，可以看作是X的结构，上右图所示，(其中较暗的线比较亮的线连接得早)。具体算法如下： 其中： $d$ 是n*n的矩阵，例如(i,j)就是 第i个解与第j个解的p范数距离($L_p-norm$)，因此是对称矩阵。 $min(d,[],2)$ 出自于matlab语法，对每一行取最小值，因此输出是一列。 另外，这位老师居然还是我们学校的老师，在电院，好奇翻了一下个人主页，居然有代码！我会附录在本博客最后，其中中文为我注释。 不同的相异评价在计算PD占很重要的作用。通常采用两个解之间的距离作为它们的相异之处。但是请注意，欧几里得距离不太适合在高维空间中测量邻域。由于MaOPs的解分布在高维目标空间中，基于$L_2$范数的欧氏距离不适用于PD中的不相似度计算。 从下图中我们可以清楚地看到，p越小，各维$L_p$对0越敏感。相反，基于$L_p-norm-based$的距离测度不适用于测量p&gt;1的高维数据的差异性。因此，为了测量MaOPs的多样性，需要将p设置为p &lt; 1。已有研究表明，只要p&lt; 1，该测度的有效性对p不敏感。因此，p在PD中不是一个参数，本文将p设为0.1。 指示器使用单个标量值来描述m维分布。因此，无论哪个指标，都会丢失一些信息。因此，尽管不同的指标可能捕获不同的信息，但希望捕获一些关键信息。当得到PF f1 +f2 +f3 = 1的三个极值点时，在这三个极值点的集合中加入不同的解，多样性度量的值是不同的。下图为在三个极值点集合中加入PF的另一个解时PD、MS、NDC (b =4)、熵(b =4)的变化值，其中颜色表示矩阵的大小(颜色较深的点值小于颜色较浅的点值)。如果根据这些指标选择一个解决方案以增加多样性，下图中较亮的部分优先于较暗的部分。一旦得到极值点。MS值达到最大值。因此，没有任何解决方案能够改进MS。虽然中间部分是由NDC和熵推动的，但解在网格内是无法区分的。对于PD，中间部分提升，值不断变化。从图4可以看出，PD通常可以促进不同的解决方案。 Overall Pareto Spread当设计的目标函数都被考虑时，总体的PF延展性度量量化了所观测的目标在目标空间中的延展能力。这个度量被定义为两个超矩形的体积比，其中一个是 $HR_{gb}$ ，它对于每一个所设计的目标的好点与坏点。类似地， $HR_{ex}$ 定义了所观察到的Pareto解集的极值点。整个PF的延展性变为$HR_{gb}$与 $HR_{ex}$ 之比： OS(P)=\frac{HR_{ex}(P)}{HR_{gb}}$P$ 是所观测的Pareto解，$m$ 为目标函数个数，其中： OS(P)= \frac{\prod_{i=1}^{m}|\max_{k=1}^{\bar{np}} (p_k)_i -\min_{k=1}^{\bar{np}}(p_k)_i |}{\prod_{i=1}^{m}|(p_b)_i-(p_g)_i|}\\ =\prod_{i=1}^{m}| \max_{k=1}^{\bar{np}}[\bar{f_i}(x_k)] -\min_{k=1}^{\bar{np}}[\bar{f_i}(x_k)] | 例如，在图4所示的两个目标空间中，PF-spread的计算公式为: OC(P) = \frac{h_1h_2}{H_1H_2}其中： $P_1,P_2$ 是两个Pareto solution sets。if $OS(P_1)&gt;OS(P_2)$, then the solution set P1 is preferred to P2 . h_1=|\bar{f_1}_{max}-\bar{f_1}_{min}|\\ h_2=|\bar{f_2}_{max}-\bar{f_2}_{min}|\\ H_1=|(p_g)_1-(p_b)_1|\\ H_2=|(p_g)_2-(p_b)_2|PD’s code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990% Authors: Handing Wang, Yaochu Jin, Xin Yao% University of Surrey, UK, and University of Birmingham, UK% EMAIL: wanghanding.patch@gmail.com, yaochu.jin@surrey.ac.uk, X.Yao@cs.bham.ac.uk% WEBSITE: http://www.surrey.ac.uk/cs/people/handing_wang/% DATE: March 2016% ------------------------------------------------------------------------% This code is part of the program that produces the results in the following paper:% Handing Wang, Yaochu Jin, Xin Yao, Diversity Assessment in Many-Objective Optimization, Cybernetics, IEEE Transactions on, Accepted, 10.1109/TCYB.2016.2550502.% You are free to use it for non-commercial purposes. However, we do not offer any forms of guanrantee or warranty associated with the code. We would appreciate your acknowledgement.% ------------------------------------------------------------------------function [ pd ] = PD( X )% Usage: [ pd ] = PD( X )%% Input:% X -Objective values of the population n*m (n solutions with m objectives)%% Output: % pd -PD value of population X%p=2;%lp norm setting0.1C=zeros(size(X,1),size(X,1));%connection arrayD=zeros(size(X,1),size(X,1));%dissimilarity array%Calculate the dissimilarity between each two solutionsfor i=1:size(X,1)-1 for j=i+1:size(X,1) d=sum(abs(X(j,:)-X(i,:)).^p,2).^(1/p); D(i,j)=d; D(j,i)=d; endendDMAX=max(max(D))+1;D(logical(eye(size(D))))=DMAX;n=size(X,1);pd=0;for k=1:n-1 %Find the nearest neighbor to each solution according to D in each row. [d,J]=min(D,[],2); %Find solution i with the maximal di to its neighbor j [dmx,i]=max(d); while liantong(C,i,J(i))==1 %i and j are connected by previous assessed solutions if D(J(i),i)~=-1 D(J(i),i)=DMAX; %Mark the connected subgraph end if D(i,J(i))~=-1 D(i,J(i))=DMAX; end [d,J]=min(D,[],2); %Find solution i with the maximal di to its neighbor j [dmx,i]=max(d); end C(J(i),i)=1; C(i,J(i))=1; pd=pd+dmx; if D(J(i),i)~=-1 D(J(i),i)=DMAX;%Mark the used dissimilarity di. end D(i,:)=-1;%Mark the chosen solution iendendfunction [w]=liantong(C,I,J)% Usage: [w]=liantong(C,I,J)%% Input:% C -Connection array% I -index I% J -index J%% Output: % w -1 if solutions I and J are connected, 0 if solutions I and J are not connected.%V=I;Child=find(C(V,:)==1);if isempty(find(Child==J))==0 % 直接连接 w=1; returnelse C(V,:)=0; % 删掉点I C(:,V)=0; for i=1:size(Child,2) % 遍历连接点I的其他点 w=liantong(C,Child(i),J); % 进行递归 if w==1 return end endendw=0;end]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>SpreadQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convergence--Distance-based QIs]]></title>
    <url>%2F2019%2F01%2F21%2Fdistancebased%2F</url>
    <content type="text"><![CDATA[可谓呕心沥血，翻译的累死我了，这篇是关于收敛性的indicators的《Distance-based QIs》。 分类可以进一步分为两类： 测量所考虑的解集到从帕累托前缘导出的一个或多个特定点的距离：the ideal point, knee point(s) , the Zeleny point and the seven particular points。 the ideal point是由帕累托前缘各目标的最优值所构造的点。 knee point(s)是帕累托前缘上的点，它具有从相邻点计算出的最大反射角。 the Zeleny point是通过分别最小化每个目标得到的点。 the seven particular points是由二元多目标问题的帕累托前缘的理想点和极值点导出的7个特殊点 测量到一个很好地表示帕累托前沿的reference set的距离。在这一组中，最常用的指标是GD。GD首先计算解集中每个解到参考集中最近点的欧氏距离，然后求所有这些距离的二次平均值。这一组中的其他QIs可以看作是GD的变体，例如taking the arithmetic mean of the distances，the powermean，considering the Tchebycheff distance，introducing the dominance relation between solutions and points in the reference set。 measure the distance of the considered solution set to one or several particular points derived from the PFTchebycheff distance to the knee point d(z,z^*,\lambda)=\max_{1 \leq j \leq k}\{ \lambda_j |z_j^* - z_j| \}其中： k：目标函数的数量。 $\lambda_i=\frac{1}{R_i}$，$R_i$是第i个目标函数的范围(range)。 Defined this way, the knee of the Pareto front is the point in the feasible objectivespace, $ \Lambda$, which corresponds to $ \min_{z∈\Lambda} d(z, z^∗, \lambda)$. 如果一个偏好关系的$PF_{approx}$集合比另一个关系的$PF_{approx}$集合包含更多的膝节点周围的解，则该偏好关系优于其他关系。 Seven point average distance该问题用于二元目标优化。 如果有效集的先验知识是可用的，MCGA完全解析E的能力可以很精确的理解为它与个体的标准和其他(已决定的)点有多接近。由于E对于任何一个测试问题都是未知的，因此为每个问题生成7点法，以衡量算法的有效性。个体准则最优约束了两个标准问题的有效集，但也可通过单独优化每一个准则而不考虑另一个准则来求出个体准则最优。有了这两点，七点法再$J_1-J_2$被定义如下原点[0,1]，最大点(在E范围内)[0,$J_2^{worst}$]和[$J_1^{worst}$,0]，和在原点与最大值之间的每个轴上的两个点。 原文： With the resulting two points at hand, the seven comparison points are denned on $J_1-J_2$as the origin [0,0], the maximum (within the range of E) of each criterion [0, $J_2^{worst}$] and [$J_1^{worst}$, 0], and two points on each axis between the origin and the maximum value. 全距离测量是通过距离处以7，该距离是从七个点中每一个点到离此点最近的MCGA种群的点的距离和。因此，每次创建距离度量时，使用总体中的7个成员。他的优点是比较不同人群在某一特定问题上的相对优势的准确方法。对于给定的问题，距离度量值最小的总体将是最接近E的总体。 这七个点具体是什么我实在没有翻译出来，在查找文献时《Evolutionary Algorithms for Solving Multi-Objective Problems 》作者：Carlos Coello Coello， David A. Van Veldhuizen， Gary B. Lamont时，有如下叙述： measure the distance to a reference setGenerational Distance (GD)GD首先计算解集中每个解到参考集中最近点的欧氏距离，然后取所有这些距离的二次平均。 公式： GD(A)=\frac{1}{N} ( \sum_{i=1}^{N} (d_2(a_i,PF)^2)^{1/2}a solution set $A=\{ a_1,a_2…,a_N\}$ $d_2(a_i,PF)$是$a_i​$到PF的2范式距离(欧几里距离) 在实际应用中使用了一个很好地表示PF的参考集R。 d_2(a_i,PF)=\min_{r \in R}d_2(a_i,r)$d_2(a_i,r)$是$a_i$与$r$的欧几里距离。如果前端的几何性质是已知的，GD不一定需要一个表示PF的引用集。 GD的值理应是要极小的。如果值为0表明该集合位于Pareto front /reference set中。作为为后代间的代际评估而设计时，GD通常用于度量solution set 向PF的演化过程。然而，由于GD考虑的是二次平均值(quadratic mean)，因此它对异常值非常敏感，无论其他解的表现如何，它都会返回一个异常值得分很低的解集。当$ N \rightarrow \infty, \ GD \rightarrow 0 $，尽管这个集合远离PF。因此，只有当考虑的集合具有相同/或非常相似的大小时，GD才可靠地可用。幸运的是，公式中的算术平均数代替二次平均数，这个问题可以解决。事实上,在一些最近的研究，GD指标的一般形式的指数“p”和“1 /p”而不是“2”和“1/2”。设置p = 1现在已经被普遍接受，并与它的反转版本IGD一起使用(度量从帕累托前的点到所考虑集合中最近解的距离的算术平均值)。 来自“Measuring the Averaged Hausdorff Distance to the Pareto Front of a Multi-Objective Optimization Problem”的下文： 虽然在许多研究中使用了GD，但并不是EMO社区的所有研究人员都接受GD。我们推测一个可能的原因(可能是主要的原因)是它的归一化策略，如下面的例子所示:假设我们有一个(任意的)点$a \in Q$，在不丧失通用性的情况下，让图像F(a)到PF的距离为1。现在将 archive $A_n$定义为由a的n个副本给出的multisets，即$A= {a,…,a}$。“平均”距离的F(A)向PF，有: GD(F(A_n),F(P_Q))=\frac{||(1,...,1)^T||_p}{n}=\frac{\sqrt[p]{n}}{n}我们可以看到，随着n的增加，近似质量就会变得越来越“好”，尽管估计值并没有怎么变，archives $A_n$甚至收敛到“完美”估计： \lim_{x \to \infty}{GD(F(A_n),F(P_Q))=0}由上述的结果可以推广:例如，我们可以考虑a的小扰动，而不是multisets。或者，如果$F(A)​$是有界的，不管$A_n​$的a是否被支配，也不管$F(a)​$离PF有多远，甚至满足$|A_n|=n​$的任意archive序列任何$A_n​$都能被选择。因此，在EMO上下文中，从这个角度来看，用进一步的、甚至占主导地位的解决方案“填充”归档文件是有好处的，因为通常较大的集合会产生更好的GD值。在社区中，它的建立是为了固定种群大小，以便对不同的算法进行比较(例如，N = 100)。然而，这给基于不受先验定义值限制的存档的MOEAs带来了麻烦。因此，“完美的”归档器(关于GD)可以接受所有(或至少是尽可能多的)候选解决方案。这当然不是我们想要的效果。 为解决以上问题，便提出了$GD_p$： $GD_p$ GD_p(X,Y)=\left(\frac{1}{N} \sum_{i=1}^{N}dist(x_i,Y)^p\right)^{1/p}$dist(x_i,Y)=\inf_{v \in Y}||x_i,v||$，$\inf$ 为下界(最小值)。 公式上的区别：把$\frac{1}{N}$在$()^{1/p}$从放括号外变为括号里。 我们把这个新指标命名为$GD_p$(索引p)只区分经典版本，这是需要在这项工作中进一步比较。“新”指标不具有上述讨论的不需要的特征，因此在比较具有不同大小的集合时似乎更为公平。特别是，大型候选集不再必须是“好”的。例如上例中$GD(F(A_n),F(P_Q))=1$ 对于所有的$n \in \mathbb{N}$ 。 \min_{x \in Q}{F(x)}\\ F(x) = (f_1(x),...,f_k(x)),the \ vector \ of \ the \ objective \ functions命题1：令$k=2​$(二元目标优化问题)，$F(P_Q)​$是连接的，有$a,b\in Q​$，有： a \prec b \ \Rightarrow \ dist(F(a),F(P_Q)) 0$dist(F(b),F(P_Q))$是固定值 r($r \ne 0$)，以$F(b)$为圆心，r为半径画一个圆，交点便是$p_b$(有点圆与$P_Q$相切的感觉)。分情况讨论： 当 $a \in P_Q$ 时，那么$dist(F(a),F(P_Q))=0$ ，以此得结论结果。 当 $a \notin P_Q $ 时 当 $ p_b \prec a$ 时 因为$a \prec b$ dist(F(a),F(P_Q)) \leq||F(a)-F(p_b)|| < ||F(b)-F(p_b)||=dist(F(b),F(P_Q)) 当 $p_b \nprec a$ 时，也就是 $p_b$ 和 $a$ 互相非支配，那么应该存在$i,j \in \{1,2\}, i \ne j$ f_i(p_b) < f_i(a) \ \ and \ \ f_j(p_b) > f_j(a） ​ 因为$a \notin P_Q$，那么也会存在$p_a \in P_Q$ 令 $p_a \prec a$(满足上面两个都是小于号) ，因为$F(P_Q)$ 是 ​ 连贯的( index from &gt; to &lt; 一定有一个=)，这存在一条$F(p_a)$到$F(p_b)$ 的路径， ​ 那么一定存在 $\bar{p} \in P_Q \ let: \ f_j(\bar{p})=f_j(a)$ ，又因为 $\bar{p}$ 和 $p_b$ 互相不支配(同在$P_Q$)， ​ 那么有： dist(F(a),F(P_Q)) \ \leq \ ||F(a)-F(\bar{p})|| \ = |f_i(a)-f_i(\bar{p})| \ < \ |f_i(b)-f_i(p_b) | \\ \ \leq \ ||F(b)-F(p_b)|| \ = \ dist(F(b),F(P_Q))证明完毕。其中要解释一下，为何： |f_i(a)-f_i(\bar{p})| \ < \ |f_i(b)-f_i(p_b) | $f_i(b) &gt; f_i(a)$ ，这是因为 $ a \prec b$ $f_i(p_b) &lt; f_i(\bar{p})$，这个比较麻烦QWQ $\bar{p} \ and \ a $ = $\begin{cases} f_j(\bar{p})=f_j(a) &amp; (1 \\ f_i(\bar{p}) &lt; f_i(a) &amp; (2 \end{cases}$ $ p_b \ and \ a $= $\begin{cases} f_j(p_b) &gt; f_j(a) &amp;(3 \\ f_i(p_b ) &lt; f_i(a) &amp;(4 \end{cases}$ ​ $ (1,(2 \Rightarrow f_j(p_b) &gt; f_j(\bar{p}) $ ，又因为 $\bar{p}$ 和 $p_b$ 互相不支配，那么$for \ i \ must \ be:f_i(p_b) &lt; f_i(\bar{p})$ 一个有趣的问题当然是如果拖把涉及两个以上的目标会发生什么。但是，我们不得不把这个问题留到以后调查。 当帕累托前缘断开时，上述结果不成立。然而，如果一个元素足够接近帕累托集合，这种“单调行为”仍然成立。下面的例子和命题分别给出了反例和证明。 例如：$F(P_Q)=\{(10,0)^T,(0,1)^T \}$ , $F(a)=(11,3)^T,F(b)=(5,2)^T \ so \ a \prec b, but$ $dist(F(b),F(P_Q)) = \sqrt{1^2 + 3^2}=\sqrt{10} &lt; \sqrt{29}=\sqrt{5^2 + 2^2} = dist(F(a),F(P_Q))$ 命题2： 翻译一下就是：对于一个$k$个目标的问题，任何一个维度$i$，存在$y(a,i)$的目标值向量属于$F(P_Q)$，并且满足$y(a,i)$在除了第$i$维度上的值与 $F(a)$ 相同,（第$i$维任意）。【其中与命题1的差别是，在1中$k = 2$，但在此命题中，并没有这个限制】 证明：推到与前一个类似，只是推广到高纬度上了$k&gt;2$。 因为$P_Q$是紧凑的，所以一定存在 $p_b\in P_Q$，满足： dist(F(b),F(P_Q)) =||F(b)-F(p_b)|| 当 $ p_b \prec a$ 时 因为$a \prec b$ dist(F(a),F(P_Q)) \leq||F(a)-F(p_b)|| < ||F(b)-F(p_b)|| 当 $ p_b \nprec a$ 时，存在$i \in \{1,…k\}$，满足$f_j(p_b) &gt; y(a,i)_i$ (翻译一下：一个解y(ami)，它的第i维满足$f_j(p_b)$ 与，其他维度的数值与$a$相同)并且： dist(F(a),F(P_Q)) \ \leq \ ||F(a)-y(a,i)|| \ = f_i(a)-y(a,i)_i \ < \ f_i(b)-f_i(p_b) \\ \ \leq \ ||F(b)-F(p_b)|| \ = \ dist(F(b),F(P_Q)) 这个结果的关键是投影$y(a, i)$的存在性，$F(a)$足够接近帕累托前沿，在这种情况下不需要$F(P_Q)$的连通性。如下图： 总结，假使PF是连贯的(至少对于k = 2)，主导解(dominating solutions) $a$ 产生更好的 $dist$ 值比其被支配解(dominated points)$b$。此外，这个依然保留的话，要么当F (a)是“足够远”帕累托前面(在这种情况下，声明：$dist(F(b),F(P_Q))=||F(b)-F(p_b)|| &gt; 0$，则必须 $p_b$ 支配 $a$ )，要么就足够接近(命题2)。 从GDp的角度来看，这些结果可以解释为:如果新的归档结果来自于前一个归档，用一个支配解替代了一个被支配解，那么$GD_p$值就会下降。对于$A1 = \{b, x_2，…， x_n\}$， $A2 = \{a, x_2，…， x_n\}$，其中$a$和$b$为上式，则为: GD_p(F(A_2),F(P_Q)) < GD_p(F(A_1),F(P_Q))然而，下面的结果更为普遍，则需要进一步的假设： 命题3： $A,B \subset \mathbb{R}^n \ be \ finite \ sets \ such \ that​$ $ \forall a \in A \ \exists b \in B:F(b) \leq_p F(a) $ $ \forall b \in A \ \exists a \in B:F(b) \leq_p F(a) $ $ \exists b \in B \backslash A ,\ \exists a \in A\backslash B:b \prec a$ $ \forall a \in A \ \forall b \in B:if \ a \prec b \Rightarrow dist(F(a),F(P_Q))&lt;dist(F(b),F(P_Q)) $ 那么： GD_p(F(B),F(P_Q))]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>ConvergenceQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convergence--Dominance-based QIs]]></title>
    <url>%2F2019%2F01%2F14%2Fdominancebased%2F</url>
    <content type="text"><![CDATA[此篇介绍的是QIs for Convergence的第一部分《Dominance-based QIs》。看了一部分方法的论文，剩下一部分实在看不下去了，想继续看看别的，有时间有精神回来补一下~ QIs用于收敛性收敛性作为解集质量的一个重要方面，在集的评价中受到了广泛的关注。文献中存在两类收敛QIs。一是考虑解或集之间的帕累托支配关系(表2项目1-9);另一种方法是考虑解集到帕累托前的距离，或者从帕累托前导出的一个/多个点(项目10 -22)。 Dominance-based QIsA type of frequently-used dominance-based QIs is to consider the dominance relation between solutions of two sets , such as the C indicator , $\widetilde{C}$ indicator, $\sigma- \tau- and \ \kappa-$ metrics , and contribution indicator.Other QIs concerning solutions’ dominance include wave metric, purity, Pareto dominance indicator, and dominance-based quality. The wave metric crunches the number of the nondominated fronts in a solution set. The purity indicator counts nondominated solutions of the considered set over the combined collection of all the candidate sets. The Pareto dominance indicator measures the ratio of the combined set’s nondominated solutions that are contributed by a particular set. The dominance-based quality considers the dominance relation between a solution and its neighbours in the set. $C-metric$定义为： C(A,B)=\frac{|\{b\in B:\exists a \in A,a \preceq b \}|}{|B|}$C​$一方面可以计算出$B​$中的解被$A​$中解所支配的比例部分，另一方面也可以计算出$A​$相对于$B​$的性能。 当$C(A,B)=1​$时，意味着$B​$中的所有解都被$A​$中的所$\preceq​$。 当$C(A,B)=0​$时，意味着$B​$中的所有解都无法被$A​$中的$\preceq​$。 注意：$C(A,B) \ne 1-C(B,A)$ $ C(A,A) \ne 0$ 如果$W​$是一个非支配解集，$A,B​$满足$A \subseteq W​$，$B \subseteq W​$，但$C(A,B)​$可为[0,1]中的任意一个值。 $\widetilde{C}-metric$多目标优化环境下的性能度量是评价优化器定量性能的数学工具，它通过单独考虑优化器或与其他优化器进行比较来评价的。这种方法可以与优化器在线评估和性能改进的优化器结合使用，也可以离线应用于两个或两个以上优化器的最终结果，以比较它们的性能、产生的结果的质量和/或要求的计算努力。 性能指标可以大致分为两类： 基本度量:一个标准度量满足一定要求的解决方案的数量或比例，比如度量关系。 序数或几何度量:这些方法不度量数量，而是通过考虑几何位置来度量。 \widetilde{C}(A,B)=\frac{|\{b\in B:\exists a \in A,a \prec b \}|}{|B|}$ C(A,A) = 0$ ，因为$A$是非支配解集。 对于$\widetilde{C}(A,B)$、$C(A,B)$，值越高说明B中的解受A所$\preceq$的比例越多。 如果$W$是一个非支配解集，$A,B$满足$A \subseteq W$，$B \subseteq W$，但$C(A,B)=0$。 $\widetilde{C}(A,B)$与$C(A,B)$都没有考虑到前沿的延展性(extent)与一致性(uniformity)。 上图可以看出(minimise)：A的一致性(uniformity)更好，而B集中聚到了一个区域。 但有：$ C(A,B) = C(B,A) =\widetilde{C}(A,B) =\widetilde{C}(B,A) =\frac{4}{12}​$，即使A的元素在B的大部分区段上占主导地位。 上图，尽管B有很好的延展性(extent)， 但是：$ C(A,B)=\widetilde{C}(A,B) =\frac{2}{12}$ ， $C(B,A)=\widetilde{C}(B,A) =\frac{0}{12}$ ，从$C$、$\widetilde{C}$中的值看出$A$优于$B$。 Contribution indicatorThe contribution of algorithm $PO_2$ relatively to $PO_2$ is roughly the ratio of non dominated solutions produced by $PO_2$. 规定： $C = PO_1 \cap PO_2$ 集合$W_1$为$PO_1$中支配$PO_2$的解集，集合$W_2$为$PO_2$中支配$PO_1$的解集。 集合$L_1$为$PO_1$中被$PO_2$支配的解集，集合$L_2$为$PO_2$中被$PO_1$支配的解集， 集合$N_1$为$PO_1$中不可与$PO_2$构成不可比较的解集，即$PO_1 \backslash (C \cup W_1 \cup L_1) $ 集合$N_2​$为$PO_2​$中不可与$PO_1​$构成不可比较的解集，即$PO_2 \backslash (C \cup W_2 \cup L_2) ​$ 表达式为： CONT(PO_1 / PO_2) = \frac{\frac{|C|}{2}+|W_1|+|N_1|}{|C|+|W_1|+|N_1|+|W_2|+|N_2|}可知： ​ 如果$PO_1$与$PO_2$是相同的解集，那么$CONT(PO_1 / PO_2)=CONT(PO_2 / PO_1)=1/2$ ​ 如果$PO_2$中的所有解都被$PO_1$所支配，那么，$CONT(PO_2 / PO_1)=0$。 我的理解： CONT(PO_1 / PO_2) = \frac{\frac{|C|}{2}+|W_1|+|N_1|}{|C|+|W_1|+|N_1|+|W_2|+|N_2|}\\ =\frac{\frac{|C|}{2}+\frac{|W_1|+|W_1|}{2}+\frac{|N_1|+|N_1|}{2}}{|C|+|W_1|+|W_2|+|N_1|+|N_2|}\\ =\frac{1}{2}\frac{|C|+|W_1|+|W_1|+|N_1|+|N_1|}{|C|+|W_1|+|W_2|+|N_1|+|N_2|}也就是说：对于$CONT(PO_1 / PO_2)$，如果$|W_1|+|N_1| &gt; |W_2|+|N_2|$，则大于0.5。 也就是说：$PO_1$中支配$PO_2$的解和不能与$PO_2$比较的解越多，$CONT(PO_1 / PO_2)$越大。 $\sigma-\ \ \tau- \ \ \kappa- \ metric$前言对于一个评价指标，无论是类别如何，想要使他可用，都要满足以下五个特征： Monotonicity/compatibility(单调性/兼容性)：对于两个PFs的支配关系，度量标准应该满足单调性/兼容性，例如，设度量标准为$\xi$，如果A支配B，A就应该比B好或至少不能差于B。因此 $A \succeq B \Rightarrow \xi (A) \geq \xi(B)$或严格单调$A \succ B \Rightarrow \xi (A) &gt; \xi(B)$ 。 Transitivity(传递性)：在所比较的所有PFs的完全顺序中，一个度量应该是可传递的。如果A优于B，B优于C，那么通过$\xi()$也应得出，A优于C。直接比较度量通常会在被比较的不同PFs之间产生不可传递关系。传递性通常只在引用度量和独立度量中得到保证。这是因为这两种方法都为每个PF分配一个数字，并且实数之间的比较是可传递的。 Scaling/meaningfulness(缩放性/有意义性)：目标函数通常需要进行缩放，例如进行单调变换以映射给定范围内的目标值，例如在[0,1]中。在这种情况下，一个度量应该是缩放不变的或有意义的，即，该度量不应受任何缩放的影响。尺度不变度量通常只利用解之间的优势关系，而不是它们的绝对客观值。 Computational effort(计算工作量)：此属性用于计算给定pf的度量值所需的计算资源。为了比较不同度量的性能，通常只考虑运行时复杂性作为所需的计算工作。 Additional information(附加信息)：许多指标依赖于不同类型的附加问题信息。一些假设问题的POF是已知的，而另一些则依赖于一些用户定义的依赖于问题的引用目标向量或引用PFs。因此，希望一个度量具有尽可能少的参数。 $\sigma-metric$规定：a dominates b is $a \succ b$ 原文： Sigma-metric($\sigma $-metric): The performance value, $\sigma_{ij} $, assigned to the j-th PF of the i-th optimizeris the number of solutions of the r-th optimizer which are strictly dominated by at least one solution of that PF of the i-th optimizer,where $i,r \in {1,2}$ and $i \ne r$. 公式： \sigma_{ij}=\sum_{s=1}^{F_r}\sum_{t=1}^{L_{rs}}\max_{k\in \{1,...L_{ij} \}}I(p_{ijk}\succ \succ p_{rst})具体规定如下： optimizer\ i_{th}=\begin{cases} PF_1 & |PF_1|=L_{i1} \\ PF_2 & |PF_2|=L_{i3} \\ ...\\ PF_j & |PF_j|=L_{ij}\\ ...\\ PF_{F_i} & |PF_{F_i}|=L_{i{F_i}} \\ \end{cases}\\ optimizer\ r_{th}=\begin{cases} PF_1 & |PF_1|=L_{r1} \\ PF_2 & |PF_2|=L_{r3} \\ ...\\ PF_j & |PF_j|=L_{rj}\\ ...\\ PF_{F_r} & |PF_{F_{r}}|=L_{rF_r} \end{cases}有两个优化器 $i$ (optimizer)，每个优化器都$F_i$个$PFs$，对于第$i$个优化器，第$j$个$PF$，它有$L_{ij}$个解(solutions)。而$p_{ijk}$则为第$i$个优化器，第$j$个$PF$的第$k$个解。 $I(\bullet)$如果内部true则返回1，否则返回0。 \max_{k\in \{1,...L_{ij} \}}I(p_{ijk}\succ \succ p_{rst})翻译为：对于指定的解 $p_{rst}$ 如果在第$i$个优化器，第$j$个$PF$中有$\succ \succ p_{rst} $关系的解，就为1，都没有则为0。 整体来看：对于第$r$个优化器的所有解中，被第$i$个优化器的第$j$个$PF$的所有$L_{ij}$个解所支配的个数。 因此，最大值为$optimizer\ r_{th}$的所有解的个数。 ps.原论文写的是$F_rL_{rs}$,但是我不赞同…..我认为是$\sum_{s=1}^{F_r}{L_{rs}}$，当$L_{r1}=L_{r2}=…=L_{F_r}$时与原论文一致。 $\tau-metric$原文： Tau-metric ($\tau -metric$): The performance value, $\tau_{ij}$, assigned to the j-th PF of the i-th optimizer is the number of solutions of the r-th optimizer which are weakly dominated by at least one solution of that PF of the i-th optimizer,where $i,r \in \{1,2\}$ and $i \ne r$. Further, $\tau_{ij} $may also be rewarded if the j-th PF of the i-th optimizer weakly outperforms a PF of the r-th optimizer. Since the metricis based on the concept of weak dominance,it may be done just as an attempt to take into account the compatibility of the metric with the ‘‘weak outperformance relation’’ given indefinition (8). However, it would be a new dimension of research in order to generalize the outperformance relations in terms of multiple(more than two) PFs.​ 公式： \tau_{ij}=\sum_{s=1}^{F_r}\{ [\sum_{t=1}^{L_{rs}}\max_{k\in \{1,...L_{ij} \}}I(p_{ijk}\succeq p_{rst})] + I(A_{ij} \ \vartheta_w \ A_{rs} ) \}规定： $\vartheta_w$ (weakly outperform): $A \ \vartheta_w \ B$ means $ A \succeq B $ and $\exists c \in A \ but \ c \notin B $。 ​ A不会比B差，并且A有B不存在的解。 在遍历$r_{th}\ optimizer$的$PF_s$时，如果与第$i$个优化器，第$j$个$PF$ 满足 ： $A_{ij} \ \vartheta_w \ A_{rs} $，再加1。 因此，相对于$\sigma-metric$最大值再加上$F_r$即$F_r(L_{rs}+1)$。 $\kappa-metric$原文： Kappa-metric ($ \kappa-metric$): The performance value, $\kappa_{ij}$ , assigned to the j-th PF of the i-th optimizer is the number of solutions of the r-th optimizer which cannot weakly dominate a given solution of that PF of the i-th optimizer,where $i,r \in \{1,2\}$; and $i \ne r$. For the same reason as in the case of the $\tau-metric$, $k_{ij}$ may also be rewarded if the j-th PF of the i-th optimizer weakly outperforms a PF of the r-th optimizer. 公式： \kappa_{ij}=\sum_{s=1}^{F_r}\{ \sum_{l=1}^{L_{ij}} \sum_{t=1}^{L_{rs}} I(p_{rst}\nsucceq p_{ijl}) + I(A_{ij} \ \vartheta_w \ A_{rs} ) \}遍历$r_{th}\ optimizer$的所有解，对于每一个解$p_{rst}$，如果$p_{rst} \nsucceq p_{ijl} (l \in [1,…,L_{ij}])$，则加1。 如果与第$i$个优化器，第$j$个$PF$ 满足 ： $A_{ij} \ \vartheta_w \ A_{rs} $，再加1。 因此，最大值为 $F_r(L_{ij}L_{rs}+1)$。 至此三种indicator已介绍完毕。 再分析当初说的五个特点，探究是否满足： Monotonicity/compatibility(单调性/兼容性)：对于两个PFs的支配关系，度量标准应该满足单调性/兼容性。如果A支配B，通过度量标准得出的结果，A就应该比B好或至少不能差于B，这个概念可应用与两个PF之间，但并不能应用于M-ary度量标准，M-ary它是和很多个PFs进行比较的而不是仅仅和另一个PF比较。如果$A$与$\{ B_1,B_2,…B_m\}$进行比较，这是不可能的说A的分数和$B_i’s$的总分数有什么样的关系，尤其在$A$支配一些$B_i’s$ 或/和 $A$被一些$B_i’s$支配 或/和 $A$和一些/全部$B_i’s$交叉。在一些特殊的情况，比如当$A$支配所有的$B_i’s$时，$A$相对于与其他的所有$B_i’s$比较时，一定比任何$B_i$分数高。另一方面，当仅仅比较两个PFs时来作为简化的例子，M-ary度量标准遵守单调/兼容性，只要一个PF支配另一个PF而不是部分PF。 Transitivity(传递性)：就像刚刚谈及Monotonicity时解释的一样，当前的概念并不适用于M-ary度量指标。在对某些PFs进行成对比较简化的情况下，在提出的基于基数的M-ary度量中，并不能保证传递性。例如$\sigma(A,B) &gt; \sigma(B,A) \ and \ \sigma(B,C) &gt; \sigma(C,B)$并不能得出$\sigma(A,C) &gt; \sigma(C,A)$。正如Knowlesand Corne所观察到的，直接的比较指标往往会在被比较的不同PFs之间产生这种不可传递关系。这种情况在Noilublao and Bureerat被称为“剪刀-纸-石头”的情况。 Scaling/meaningfulness(缩放性/有意义性)：所提出的度量标准是基于解决方案之间不同形式的优势关系设计的。由于两个解之间的优势关系是基于它们在目标空间中的相对位置，所以这些关系不会因为它们的双射值的缩放而改变(例如在给定范围内的单调变换)。因此，所提出的度量是缩放不变的。 Computational effort(计算工作量)：因为一个优化器的PF与其它优化器相比,提出的每一个最糟糕的复杂性度量是$O(dFL^2)$,d是目标的数量,F是PFs的数量与一个给定的PF相比,和L的最大尺寸是比较PFs。 Additional information(附加信息)：除了比较优化器的PFs之外，所提出的度量中不需要其他信息。 实例讨论这些测试首先在一组基准实例上进行，这些基准实例包含不同共拓扑的PFs，并且知道PFs之间的确切关系。最后。这些指标应用于另一组实例，并与三个已知指标的结果进行比较。在这个集合中，每个优化器都涉及从多次运行中获得的多个PF，并且不知道PFs之间的确切关系。 izarraga等人提出了8个测试用例来评估指标的性能。测试用例是这样构造的:考虑的PFs之间的确切关系是已知的。每个测试用例包含五个PFs (A, B, C, D和E),除了第六测试此用例只包含两个PFs (A和B)。三维版本中也是如此创建的模式和关系,每个测试用例的PFs是类似的。 假设每一个优化器只有一个PF，并且也已知与其他优化器的PFs的关系如何。 a：此测试样例是关于PFs收敛性分析，$AO_cB; BO_cC;CO_cD;DO_cE​$，除此之外，所有的PFs都有相同数量的解集，多样性，延展性。 b：此测试样例是关于收敛性与多样性分析，$AO_cB,C; B,CO_cD,E$。$B$与$C$，$D$与$E$之间没有任何关系。所有的PFs有相同数量的解集，相同的多样性，但不同的延展性。 c：此测试样例中，所有的PFs有相同数量的解集，相同的收敛性，但是每一个PF都有一个洞，每个洞的大小不一。 d：此测试样例仅关于多样性。所有PFs有相同的收敛性和延展性但多样性不同。A是一致性分布，剩余的PFs都添加了一致性噪音(uniform noise)，但并没有影响其收敛性与延展性。 e：此测试样例用来独立评估收敛性和多样性的用例。A有三个均匀分布的解。B是通过给A添加一个新的非支配解来构造的，C是通过给B添加一个新的非支配解来构造的，以此类推，从而得到$EO_wDO_wCO_wBO_wA$。PFs也是这样构造的，E相对于D有一个更好的多样性，D相对于C有一个更好的多样性，依此类推。 f：此测试样例用于检测是否受到PF的凸性影响，所考虑的PFs具有相同的收敛性、多样性、扩散性和solu离子个数，但它们具有不同的凸性。 g：此测试样例是检查一个度量是否受到PF位置的影响，所有设计的PFs都具有相同的收敛性、多样性、扩张性和解的个数，但是它们都位于POF的不同位置。 h：最后一个测试样例被设计来研究具有多个解决方案的度量的行为。所考虑的五种PFs具有相同的收敛性、相同的扩散性和均匀的多样性，但解的个数不同。 最终实验结果如下： 请注意，测试用例5、7和8的PFs(图2(e)、g)和(h)由于以下原因不能正确区分。在测试用例5中，通过向A添加一个新的非支配解来构造B，通过向B添加一个新的非支配解来构造C，以此类推。因此，A完全被B重叠，B完全被C重叠，以此类推。因此使PFs不可区分。同样的，由于测试用例8的PFs具有相同的收敛性、相同的发散性和一致的多样性，所以它们之间是重叠的。另一方面，虽然测试用例7的每个PF在一个唯一的位置上都有一个曲线的模式，但是由于PF中有大量的高密度的解，所以PFs的指示符号并不能被清晰地识别出来。 论文中也介绍了一个optimizer with multiple PFs的情况。 但是实在不想翻译了。。。。。累死人 Purity原文： 其中的rank one可难倒我了，以为要看前文才能理解，结果看完了还是不懂，直到我查阅材料时发现以下这段话： an iterative ranking procedure: First all non-dominated individuals are assigned rank one and temporarily removed from the population. Then, the next nondominated individuals are assigned rank two and so forth. Finally, the rank of an individual determines its fitness value. 我才恍然大悟，原文里说的是$r_i$be the number of rank one solutions obtained from each MOO strategy.注意是solutions而不是nondominated solutions ，所以就会分等级制度，rank one、rank two。。。具体分法那段话就是步骤。 规定： 有N个MOO策略，$\{R^1_1,…R_1^N\} \ N &gt; 2$ ，下标是rank，上标是第几个策略。 $r_i$ 是第i个策略 $R_i$的等级1(rank one)的个数。 $R^* = \bigcup^N_{i=1}\{R^i_1\}$ 是所有集合的等级1集合的并集。 $r_i^*=|\{\gamma| \gamma \in R_1^i and \ \gamma \in R_1^*\}|$ 是在$R^i_1$与$R^*$的交集。 表达式为： P_i = \frac{r_i^*}{r_i}, i = 1,2,...,N该值是在[0,1]区间，并且越接近1越有良好的性能，纯度越高。 现在想想纯度的的命名还是很形象的。 Wave metric这个度量标准允许我们从这个解集中提取的以帕累托边界的个数来计算解集的深度。我们应该说“pareto layers”，但是下面的算法解释了为什么“pareto frontier”更适合这种情况。Wave metric只能应用于有限尺寸的解集。要计算wave，通常是这样进行的： set i=1 compute the Pareto frontier using solutions set points. Remove Pareto frontier points from the solutions set if the result set is empty, Wave = i else i = i + 1 and go to step 2 一个好的方法必须产生低wave的结果集。当wave = 1时，解集中的所有解都属于帕累托边界。在下图中，我们可以看到波度规在一个简单集合上的结果。对于这个集合，wave = 4： 实际上，我们可以从这个集合中提取4个帕累托边界。 wave metric有两个严重的缺点:它不能在两个解集之间进行区分，而且不可能在两个不同的解集上比较波度量计算的相同结果。 例如,如果我们计算解决方案上图的wave metric，,我们有wave = 4因为我们可以从这个集合中提取四个帕累托前沿。如果我们再加10分的Pareto frontier和计算wave metric,度量的值还是一样的。所以这个度规不能在这两个集合之间进行区分。第二，如果我们没有任何关于解集的先验信息，我们就不能说4是好是坏。 Dominance-based quality（有时间再说）Dominance ranking(太长了再说)Pareto dominance indicator(有时间再说)总结然而，所有dominance-based QIs都有一些弱点。它们提供的信息很少，不知道一组在多大程度上优于另一组。更重要的是，如果集的所有解互不支配，它们可能会使解集变得不可比较，这在多目标优化中经常发生。此外，值得注意的是，一些dominance-based QIs可能部分表示着解集的基数(cardinality)，因为一组尺寸大一点的解可能会导致更多的非支配解。]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>ConvergenceQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[毕设]]></title>
    <url>%2F2019%2F01%2F12%2Findicator%2F</url>
    <content type="text"><![CDATA[放假回家了也要准备准备我的毕业设计，题目是《基于自适应的indicators的多目标优化算法》，如题古老的多目标优化的题目，首先当然是要先了解了解indicators，老师就把他最近写好的关于indicators的综述发给了我，真可谓综述啊！足足100个indicators，路漫漫。。。。。 概念介绍以下是解与解、集合与集合之间的关系： 把解与解的总结到表格里： 一般来说，解集的质量可以解释为它如何很好地表示帕累托前沿，可以分为四个方面:收敛性(convergence)、扩散性(spread)、一致性(uniformity)和基数性(cardinality)。 解集的收敛性(convergence)是指解集与帕累托前缘的距离。解集的扩展考虑集覆盖的区域。它涉及到集合的外部和内部部分。这不同于只考虑集合的边界的质量的广泛性(extensity)。注意，在存在问题帕累托前沿的情况下，解集的扩展也称为集的覆盖(coverage)。集的均匀性(uniformity)是指解分布在集中的均匀程度;解决方案之间的等距间距是所希望的。传播和均匀性是密切相关的,他们共同被称为一组的多样性(diversity)。解集的基数(cardinality)是指解决方案集的数量。总的来说,我们的期望足够的解决方案明确地描述集,但不是太多,可能会损害DM与选择。然而，如果使用相同数量的计算资源生成两个集，则认为具有更多解决方案的集是首选的。 比较解决方案集的质量的一种直接方法是将这些集可视化，并直观地判断一个集相对于另一个集的优越性。这种目视比较是最常用的方法之一，非常适用于双目标拖把或三目标拖把。当目标个数大于3时，解决方案集的直接观察不可用时(散点图),人们可能会求助于从数据分析领域的工具。然而，这些可视化方法可能无法清晰地反映解决方案集质量的所有方面;例如，常用的平行坐标只能部分反映收敛性、扩散性和均匀性。此外，可视化比较不能量化解决方案集之间的差异，因此不能用于指导最优化。 质量指标(Quality indicators, QIs)通过将解决方案集映射为实数来克服可视化比较的问题，从而提供解决方案集之间的数量差异。QIs能够提供解决方案集质量的精确表述，例如，在这些表述中，一个集的质量优于另一个集，以及一个集在某些方面比另一个集好多少。原则上，将一组向量映射成标量值的任何函数都可以看作是一个潜在的质量指示器，但通常它可能需要反映集合质量的一个或多个方面:收敛性、扩展性、一致性和基数性。注意，当比较由精确方法生成的解集时，由于生成的解集是问题的帕累托前沿的子集，所以不考虑解集的收敛性评价。 本节根据Qls主要捕获的质量方面来审查Qls。一般来说，QIs可分为六类:1)QIs用于收敛，2)QIs用于扩展，3)QIs用于均匀性，1)QIs用于基数性，5)QIs用于扩展和均匀性，6)Qls用于四个质量方面的组合质量。在每个类别中，我们还详细介绍了一个或几个示例指示器。这些QIs通常在文献中使用，并且/或在它们的类别中具有代表性。表2总结了所有100篇文献。请注意，它不包括由多个QIs组合而成的度量。 当当当！！！这是论文中总结的100个indicators！！WTF！！！老师让了解了解的时候我是崩溃的。我慢慢来… 加黑加粗的是我已经整理好的~ 安排： QIs for Convergence Dominance-based QIs Dominance-based QIs QIs for Spread]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>indicators</tag>
        <tag>MOEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab]]></title>
    <url>%2F2019%2F01%2F02%2Fmatlab%2F</url>
    <content type="text"><![CDATA[此文会持续更新，记录一些在matlab中的一些常用函数。 repmat123456&gt;&gt; a = [1 2 3];&gt;&gt; repmat(a,2,3) %把矩阵整体堆叠成新矩阵ans = 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 sort12345678910&gt;&gt; a = [6 3 2 1 4 5];&gt;&gt; [~,ans] = sort(a) % 默认从小到大的索引值ans = 4 3 2 5 6 1&gt;&gt; a(ans)ans = 1 2 3 4 5 6 尺寸扩展12345678&gt;&gt; a = ones(3);&gt;&gt; a(1,(4:5)) = 10a = 1 1 1 10 10 1 1 1 0 0 1 1 1 0 0]]></content>
      <categories>
        <category>matlab</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(三)]]></title>
    <url>%2F2019%2F01%2F02%2Fmoead3%2F</url>
    <content type="text"><![CDATA[“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”第三部分，论文中一些具体的细节。 测试函数以下为具体函数，和所给定的前端解 ZDT1 f_1(1)=x_1 \\f_2=g(x)[1-\sqrt{\frac{f_1(x)}{g(x)}}] \\where \quad g(x)=1 + \frac{9(\sum_{i=2}^{n}{x_i})}{n-1} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=30 ZDT2 f_1(x) = x_1 \\f_2=g(x)[1-(\frac{f_1(x)}{g(x)})^2] \\where \quad g(x)=1 + \frac{9(\sum_{i=2}^{n}{x_i)}}{n-1} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=30 ZDT3 f_1(x) = x_1 \\f_2=g(x)[1-\sqrt{\frac{f_1(x)}{g(x)}}-\frac{f_1(x)}{g(x)}sin(10\pi x_1)] \\where \quad g(x)=1 + \frac{9(\sum_{i=2}^{n}{x_i)}}{n-1} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=30 ZDT4 f_1(x) = x_1 \\f_2=g(x)[1-\sqrt{\frac{f_1(x)}{g(x)}}] \\where\quad g(x)=1 + 10(n-1)+\sum_{i=2}^{n}[x_i^2-10cos(4\pi x_i)] \\x=(x_1,...x_n) ，x_1\in [0,1] \times [-5,5]^{n-1},n=10 ZDT6 f_1(x)=1-exp(-4x_1)sin^6(6\pi x_1) \\f_2=g(x)[1-(\frac{f_1(x)}{g(x)})^2] \\g(x)=1 + 9[\frac{\sum_{i=2}^{n}{x_i}}{n-1}]^{0.25} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=10 DTLZ1 f_1(x)=(1+g(x))x_1x_2 \\f_2(x)=(1+g(x))x_1(1-x_2) \\f_3(x)=(1+g(x))(1-x_1) \\where\quad g(x)=100(n-2)+100\sum_{i=3}^{n}{\{(x_i-0.5)^2-cos[20\pi (x_i-0.5)]\}} \\x=(x_1,...,x_n)^T \in [0,1]^n,n=10The function value of a Pareto optimal solution satisfies$\sum_{i=1}^{3}{f_i}=1,f_i \geq0$ DTLZ2 f_1(x)=(1+g(x))cos(\frac{x_1\pi}{2})cos(\frac{x_2\pi}{2}) \\f_2(x)=(1+g(x))cos(\frac{x_1\pi}{2})sin(\frac{x_2\pi}{2}) \\f_3(x)=(1+g(x))sin(\frac{x_1\pi}{2}) \\where\quad g(x)=\sum_{i=3}^{n}{x_i^2}, \\x=(x_1,...x_n)^T\in [0,1]^2\times [-1,1]^{n-2},n=10The function value of a Pareto optimal solution satisfies$\sum_{i=1}^{3}{f_i}^2=1,f_i \geq0$ 基本参数设置12345678910N=300;%种群大小T=20;%邻居规模大小max_gen=250;%进化代数pc=1;%交叉概率pm=1/x_num;%变异概率fun='DTLZ2';%有 ZDT1 ZDT2 ZDT3 ZDT4 ZDT6 DTLZ1 DTLZ2yita1=2;%模拟二进制交叉参数2yita2=5;%多项式变异参数5x_num = ;%根据以上每一个函数的定义f_num = ;%根据以上每一个函数的定义 权值向量初始化1234567891011121314151617181920212223242526272829303132function lamda = genrate_lamda( N,f_num )%产生初始化向量lamdalamda2=zeros(N+1,f_num);%初始化if f_num==2 array=(0:N)/N;%均匀分布的值 for i=1:N+1 lamda2(i,1)=array(i); lamda2(i,2)=1-array(i); end len = size(lamda2,1); index = randperm(len); index = sort(index(1:N)); lamda = lamda2(index,:);elseif f_num==3 k = 1; array = (0:25)/25;%产生均匀分布的值 for i=1:26 for j = 1:26 if i+j&lt;28 lamda3(k,1) = array(i); lamda3(k,2) = array(j); lamda3(k,3) = array(28-i-j); k=k+1; end end end len = size(lamda3,1); index = randperm(len); index = sort(index(1:N)); lamda = lamda3(index,:);endend 建立权值向量的邻域1B=look_neighbor(lamda,T); 其中look_neighbor.m为： 12345678910111213141516function B = look_neighbor( lamda,T )%计算任意两个权重向量间的欧式距离N =size(lamda,1);B=zeros(N,T);distance=zeros(N,N);for i=1:N for j=1:N l=lamda(i,:)-lamda(j,:); distance(i,j)=sqrt(l*l'); endend%查找每个权向量最近的T个权重向量的索引for i=1:N [~,index]=sort(distance(i,:)); B(i,:)=index(1:T);end 种群初始化1234567function X = initialize( N,f_num,x_num,x_min,x_max,fun )% 种群初始化X = repmat(x_min,N,1)+rand(N,x_num).*repmat(x_max-x_min,N,1); for i=1:N X(i,(x_num+1:(x_num+f_num))) = object_fun(X(i,:),f_num,x_num,fun); X(i,(x_num+f_num+1)) = 0;end 其中object_fun.m: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485function f = object_fun( x,f_num,x_num,fun )% 测试函数的设置%--------------------ZDT1--------------------if strcmp(fun,'ZDT1') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*(sum/(x_num-1)); f(2)=g*(1-(f(1)/g)^0.5);end%--------------------ZDT2--------------------if strcmp(fun,'ZDT2') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*(sum/(x_num-1)); f(2)=g*(1-(f(1)/g)^2);end%--------------------ZDT3--------------------if strcmp(fun,'ZDT3') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*(sum/(x_num-1)); f(2)=g*(1-(f(1)/g)^0.5-(f(1)/g)*sin(10*pi*f(1)));end%--------------------ZDT4--------------------if strcmp(fun,'ZDT4') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+(x(i)^2-10*cos(4*pi*x(i))); end g=1+9*10+sum; f(2)=g*(1-(f(1)/g)^0.5);end%--------------------ZDT6--------------------if strcmp(fun,'ZDT6') f=[]; f(1)=1-(exp(-4*x(1)))*((sin(6*pi*x(1)))^6); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*((sum/(x_num-1))^0.25); f(2)=g*(1-(f(1)/g)^2);end%--------------------------------------------%--------------------DTLZ1-------------------if strcmp(fun,'DTLZ1') f=[]; sum=0; for i=3:x_num sum = sum+((x(i)-0.5)^2-cos(20*pi*(x(i)-0.5))); end g=100*(x_num-2)+100*sum; f(1)=(1+g)*x(1)*x(2); f(2)=(1+g)*x(1)*(1-x(2)); f(3)=(1+g)*(1-x(1));end%--------------------------------------------%--------------------DTLZ2-------------------if strcmp(fun,'DTLZ2') f=[]; sum=0; for i=3:x_num sum = sum+(x(i))^2; end g=sum; f(1)=(1+g)*cos(x(1)*pi*0.5)*cos(x(2)*pi*0.5); f(2)=(1+g)*cos(x(1)*pi*0.5)*sin(x(2)*pi*0.5); f(3)=(1+g)*sin(x(1)*pi*0.5);end%--------------------------------------------end 交叉变异操作模拟二进制交叉(SBX)for j = 1.....x_num x'_{1j}(t)=0.5\times[(1+\lambda_j)x_{1j}+(1-\lambda_j)x_{2j}(t)] \\x'_{2j}(t)=0.5\times[(1-\lambda_j)x_{1j}+(1+\lambda_j)x_{2j}(t)]其中： \lambda_j=\begin{cases} (2u_i)^{\frac{1}{\eta+1}}, & u_j < 0.5\\ \frac{1}{2(1-u_i)}^{\frac{1}{\eta+1}}, & other \end{cases}随机$u_j$，使$0 \leq u_j \leq 1$. endfor 多项式变异for j = 1.....x_num x_{1j}(t)=x_{1j}(t) + \Delta_j其中： \Delta_j=\begin{cases} (2u_i)^{\frac{1}{\eta+1}}-1, & u_j < 0.5\\ 1-(2(1-u_i))^{\frac{1}{\eta+1}}, & other \end{cases}随机$u_j$，使$0 \leq u_j \leq 1$. endfor 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273function chromo_offspring = cross_mutation( chromo_parent_1,chromo_parent_2,f_num,x_num,x_min,x_max,pc,pm,yita1,yita2,fun )%模拟二进制交叉与多项式变异%%%模拟二进制交叉if(rand(1)&lt;pc) %初始化子代种群 off_1=zeros(1,x_num+f_num); %进行模拟二进制交叉 u1=zeros(1,x_num); gama=zeros(1,x_num); for ind=1:x_num u1(ind)=rand(1); if u1(ind)&lt;=0.5 gama(ind)=(2*u1(ind))^(1/(yita1+1)); else gama(ind)=(1/(2*(1-u1(ind))))^(1/(yita1+1)); end off_1(ind)=0.5*((1-gama(ind))*chromo_parent_1(ind)+(1+gama(ind))*chromo_parent_2(ind)); %使子代在定义域内 if(off_1(ind)&gt;x_max(ind)) off_1(ind)=x_max(ind); elseif(off_1(ind)&lt;x_min(ind)) off_1(ind)=x_min(ind); end end %计算子代个体的目标函数值 off_1(1,(x_num+1):(x_num+f_num))=object_fun(off_1,f_num,x_num,fun);end% %%%多项式变异 注释这种方法为上方公式代码，但在ZDT4，DTLZ1中效果不好，% if(rand(1)&lt;pm) 因此换成下方代码，效果甚好！% u2=zeros(1,x_num);% delta=zeros(1,x_num);% for j=1:x_num% u2(j)=rand(1);% if(u2(j)&lt;0.5)% delta(j)=(2*u2(j))^(1/(yita2+1))-1;% else% delta(j)=1-(2*(1-u2(j)))^(1/(yita2+1));% end% off_1(j)=off_1(j)+delta(j);% %使子代在定义域内% if(off_1(j)&gt;x_max(j))% off_1(j)=x_max(j);% elseif(off_1(j)&lt;x_min(j))% off_1(j)=x_min(j);% end% end% %计算子代个体的目标函数值% off_1(1,(x_num+1):(x_num+f_num))=object_fun(off_1,f_num,x_num,fun);% end% chromo_offspring=off_1;% end%%%多项式变异 具体改变：一次变异只改变一个位置，并不是像之前那样都要变异if(rand &lt; pm) r=randperm(x_num); ind=r(1); u2=rand; if(u2 &lt; 0.5) delta=(2*u2)^(1/(yita2+1))-1; else delta=1-(2*(1-u2))^(1/(yita2+1)); end off_1(ind)=off_1(ind)+delta*(x_max(ind)-x_min(ind)); %使子代在定义域内 if(off_1(ind)&gt;x_max(ind)) off_1(ind)=x_max(ind); elseif(off_1(ind)&lt;x_min(ind)) off_1(ind)=x_min(ind); end %计算子代个体的目标函数值 off_1(1,(x_num+1):(x_num+f_num))=object_fun(off_1,f_num,x_num,fun);endchromo_offspring=off_1;end 更新领域解1X=updateNeighbor(lamda,z,X,B(i,:),off,x_num,f_num); 其中updateNeighbor.m： 1234567891011function X = updateNeighbor( lamda,z,X,Bi,off,x_num,f_num )%更新领域解for i=1:length(Bi) gte_xi=tchebycheff_approach(lamda,z,X(Bi(i),(x_num+1):(x_num+f_num)),Bi(i)); gte_off=tchebycheff_approach(lamda,z,off(:,(x_num+1):(x_num+f_num)),Bi(i));% gte_xi=ws_approach(lamda,X(Bi(i),(x_num+1):(x_num+f_num)),Bi(i));% gte_off=ws_approach(lamda,off(:,(x_num+1):(x_num+f_num)),Bi(i)); if gte_off &lt;= gte_xi X(Bi(i),:)=off; endend 其中tchebycheff_approach.m： 123456789function fs = tchebycheff_approach( lamda,z,f,i)%tchebycheff_approachfor j=1:length(lamda(i,:)) if(lamda(i,j)==0) lamda(i,j)=0.00001; endendfs=max(lamda(i,:).*abs(f-z));end 评价指标C-metric令 A和 B是一个 MOP中两个接近PF的集合，定义 C(A,B)如： C(A,B)=\frac{\{u\in B|\exists v\in A:v\quad dominates\quad u\}}{|B|}C(A,B)不等于 1-C(B,A)。C(A,B)=1意味着 B中所有的解都被 A中的某些解支配了， C(A,B)=0意味着 B中没有解被 A中的解支配。 1234567891011121314151617181920212223242526function C_AB = cal_c(A,B,f_num)[temp_A,~]=size(A);[temp_B,~]=size(B);number=0;for i=1:temp_B nn=0; for j=1:temp_A less=0;%当前个体的目标函数值小于多少个体的数目 equal=0;%当前个体的目标函数值等于多少个体的数目 for k=1:f_num if(B(i,k)&lt;A(j,k)) less=less+1; elseif(B(i,k)==A(j,k)) equal=equal+1; end end if(less==0 &amp;&amp; equal~=f_num) nn=nn+1;%被支配个体数目n+1 end end if(nn~=0) number=number+1; endendC_AB=number/temp_B;end D-metric令 $P^*$为一组均匀分布在 PF上的点集合。 A是一个接近 PF的集合。 的集合。 $P^*$到 A的平均距离定义为： D(A,P)=\frac{\sum_{v\in P^*}d(v,A)}{|P^*|}这里 $𝑑(𝑣,𝐴)$是v和A中的点最小欧式距离。如果 $P^*$足够大,说明其可以很好的代表PF。$D(A,P^*)$可以从某种意义上评估A的收敛性和多样。为了让$D(A,P^*)$的值很低，必须设置 A非常接近PF，并且不能缺失整个PF的任何部分。 12345678910function D_AP = cal_d(A,P)[temp_A,~]=size(A);[temp_P,~]=size(P);min_d=0;for v=1:temp_P d_va=(A-repmat(P(v,:),temp_A,1)).^2; min_d=min_d+min(sqrt(sum(d_va,2)));endD_AP=(min_d/temp_P);end ‘]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(二)]]></title>
    <url>%2F2019%2F01%2F01%2Fmoead2%2F</url>
    <content type="text"><![CDATA[“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”第二部分，算法的流程框架。 规定本文提出的基于分解的多目标进化算法(MOEA/D)需要对MOPs进行分解。任何分解方法都可以达到这个目的。在下面的描述中，我们假设使用了Tchebycheff方法。在使用其他分解方法时，修改下面的MOEA/D也非常简单。 $\lambda^1​$,…$\lambda^N​$ 是均匀分布的权值向量 $z^*$ 是reference point 选用Tchebycheff Approach把多目标问题拆成N个标量优化子问题，表达式如下: g^{te}(x|\lambda^j,z^*)=\max\limits_{1\leq i \leq m}\{\lambda_i^j|f_i(x)-z_i^*|\} 其中 $\lambda ^j=(\lambda_1^j,…\lambda_m^j)^T$. $\lambda=(\lambda^1,…,\lambda^N)$ 可知$g^{te}$是关于$\lambda$连续的，当$\lambda^i$与$\lambda^j$彼此接近，那么接近$\lambda ^i$向量的$g^{te}$权向量的信息也对最优解$g^{te}(x|\lambda^j,z^*)$有一定的作用。这也是MOEA/D的理论基础。 在MOEA/D中，权向量的邻域被定义为它的几个最近的权向量的集合。第$i$个子问题的邻域由所有的子问题组成，这些子问题的权向量来自于第$i$个子问题的邻域。在MOEA/D中，只有相邻子问题的当前解被用来优化子问题。 切比雪夫法的MOEA/D算法中，有以下规定： $x^1,…x^N \in \Omega$ $x^i$是当前的第i个子问题 $FV^1,…,FV^N$ ，其中 $FV^i = F(x^i)$ $ x \in [1,N]$ $z=(z_1,…z_m)^T $ ，$z_i$ 是目前对目标$f_i$所找到的最好的点。 Input MOP(1) 一个终止准则 N：子问题的个数 N 个均匀分布的权值向量$\lambda_1,…\lambda_N$ T 每一个权值向量的邻居的数量Output: EP STEP 1) Initialization:Step 1.1) 使EP为空集 Step 1.2) 计算任意两个权值向量间的欧式距离，并找到离每个权值距离最近的T个点 ​ $B(i)=\{i_i,…i_T\}$ ，其中，$\lambda^{i_1},…\lambda^{i_T}$就是T个最近的权值向量 Step 1.3) 随机产生初始化种群 $x^1,…,x^N$ ，规定$FV^i=F(x^i).$ Step 1.4) 初始化 $z=(z_1,…z_m)^T $ STEP 2) Update:for i=1,…N Step 2.1) 复制 ：从$B(i)$随机产生两个索引$k,l$ ，然后通过遗传算子从$x_k,x_l$ 中产生新的子代$y$ Step 2.2) 提升 ：通过提升或者修理来启发式的由$y$产生$y’$ Step 2.3) 更新参考点$z$：if $z_j &lt; f_j(y’)$ then $z_j = f_j(y’)$ $j \in 1,…m$ Step 2.4) 更新相邻解：对于每一个$j \in B(i)$,if $g^{te}(y’|\lambda^j,z)\leq g^{te}(x^j|\lambda^j,z)$ then $x^j=y’, FV^j=F(y’)$ Step 2.5) 更新EP：​ — 从 EP中移除被 $F(y’)$支配的所有向量​ — 如果 EP中没有向量支配 $F(y’)​$，就将 F(y’)加入到EP中 STEP 3) Stopping Criteria 如果停止准则满足，并输出EP。否则，转向 STEP 2)。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(一)]]></title>
    <url>%2F2018%2F12%2F31%2Fmoead1%2F</url>
    <content type="text"><![CDATA[最近在复现“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”这篇论文，但多目标优化门都没入，所以作为复现的第一篇MOEA算法，我要趁此好好肢解这篇论文，尽量理解。 在Weighted Sum Approach表达式为： min\quad g^{ws}(x|\lambda)=\sum_{i=0}^{m}\lambda_if_i(x)m：m个优化目标， $\sum_{i=1}^{m}\lambda_i = 1$ $\lambda​$ 被称为权重向量。 通过公式，把算法求出的一个目标点和原点相连构造成一个向量与对应权重向量点乘，由向量点乘的几何意义可知，所得的数为该向量在权重向量方向上的投影长度，因为权重向量不变，最大/小化该长度值其实就是在优化该向量。可知若要增大该向量在权重向量上投影的长度，一方面可以增大/减小与权重向量的夹角，另一方面可以增大/减小该向量的长度。样例图如下： 红色权重向量，因为是最小化问题，所以减小长度，增大夹角都是可行的方案，绿色为等高线，垂直于权重向量。阴影部分为所有解，因此，在每一个绿色的等高线上找角度最大的即为边界。 Tchebycheff Approach表达式为： minimize\quad g^{te}(x|\lambda,z^*)=\max \limits_{1\leq i \leq m}\{\lambda_i|f_i(x)-z^*|\}注意该方法中不再含有$\sum$符号，故不能再从向量点乘的角度理解。该方法大致思想是减少最大差距从而将个体逼近PF。 首先解释等高线为什么是这样的。单看$f_1$函数，即只考虑纵坐标，若两点等值，必然是$\lambda_i|f_i(x)-z^*|$式中$f_1$的函数值相等（因为另外两个量是不变的），即纵坐标相等，所以$f_1$函数的等高线是一组平行于横轴的直线。$f_2$类似，为一组平行于纵轴的直线。第一次相比较的是m个维度中最大的$max ( \lambda _1(y-z_1),\lambda _2(x-z_2))$，所以等高线便是一个点之内各个维度的比较。那么，图中的等高线是横竖相交且刚好交在权重向量的方向上的，证明：可知，对于任何一个可行的解，我们从$f_1$的角度上可以得到一个$f_1$的值y，从$f_2$的角度上可以得到一个$f_2$ 的值x，他们的切比雪夫值是相等的，自然想到：点(x,y)（图中紫色点）为该切比雪夫值得横纵两条等值线的交点，那么有：$\lambda _1(y-z_1)=\lambda_2(x-z_2)$，化简的$(y-z_1)/(x-z_2)=\lambda_2/\lambda_1$，可知该交点位于权重向量的方向上。需要注意一点，这里的权重向量起点是$z^*$，不再是原点。此时可知，若某个个体位于其($\lambda -z^*$)向量方向的上部，则max得到的一定是其$f_1$部分，故优化也需要减小其$f_1$的值，即个体向下移动，相反，若在($\lambda -z^*$)向量方向的下部，则应像左移动。以此来保证个体目标值落在黄点附近。 一种可能的个体运动路线如下图，橘色—&gt;黄色所示： Boundary IntersectionApproach表达式为： minimize\quad g^{bi}(x|\lambda,z^*)=d \\subject\quad to\quad z^*-F(x)=d\lambda \\x \in \Omega参数含义如下如所示： 式子中等式约束其目的是为了保证F(x)位于权重向量λ的方向上，通过减小d来使算法求出的解逼近PF。但该条件不太容易实现，故将其改进为下边这种方法。 Penalty-based Boundary Intersection Approach minimize\quad g^{bip}(x|\lambda,z^*)=d_1 + \theta d_2 \\subject \quad to \quad x \in \Omega \\where \quad \quad d_1 =\frac{||(z_*-F(x))^T\lambda||}{||\lambda||} \\and \quad d_2 = ||F(x)-(z^*-d_1\lambda)||参数含义如下如所示： 可知算法放宽了对算法求出的解得要求，但加入了一个惩罚措施：你可以不把解生成在权重向量的方向上，但如果不在权重向量方向上，你就必须要接收惩罚，你距离权重向量越远，受的惩罚越厉害，以此来约束算法向权重向量的方向生成解。 接下来是关于$d_1$和$d_2$两个参数的计算表达式的含义说明，我依然是从几何角度理解的。 $d_1$——观察$d_1$的计算表达式，$Z^*-F(x)$可以看做原点到$Z^*$点的向量减去原点到$F(x)$的向量，得到的是从$F(x)$出发指向$Z^*$的一个向量，暂且命名为$\mu$，之后$\mu$与$\lambda$相乘得到$\mu$在方向上的投影，这$\lambda$个长度值与λ的长度值之比为$d_1$。$d_2$——其表达式的含义其实也无非就是利用向量运算构造出$d_2$所表示的向量，取模即可得到$d_2$.构造过程如下： $Z^*$表红色向量，$d_1\lambda$表蓝色向量（因为减法，所以方向取反），红色减蓝色得紫色向量，$F(x)$表绿色向量，绿色减紫色得黄色向量，即$d_2$表黄色向量的长度 引自]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(0)]]></title>
    <url>%2F2018%2F12%2F30%2Fmoead0%2F</url>
    <content type="text"><![CDATA[最近在复现“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”这篇论文这是一个后补上的文章，几乎是翻译的原论文呢】，因为课程设计凑字数，也为了省事，就干脆发在我的小博客上了。 多目标优化问题可以表示如下： maximize \quad F(x)=(f_1(x),...,f_m(x))^T \\subject \ to \ x \in \Omega其中，$\Omega$是决策空间，$F$：$\Omega \rightarrow R^m$是m个实数目标函数，$R^m$叫做目标空间，可实现的目标定义如下： \Omega=\{x \in R^n|h_j(x)\leq 0,j=1,...,m\}$h_j$是连续的函数，因此，我们也称$F(x)$是连续的MOP问题。 在现实生活中，大多数的目标函数却是相互矛盾的，并不存在$\Omega$可以同时放大所有的目标值。因此需要找相应的方法去平衡这些目标。目标之间的最佳权衡可以用帕累托(Pareto)最优性来定义。 定义$u,v\in R^m$,如果对于$\forall i \in \{1,…,m\}$，使得$u_i\geq v_i$，并且$\exists j \in \{1,…,m\} $，使得$u_i &gt; v_i$，则称$u$支配$v$。如果存在这种点$x^\in \Omega$，不存在点$x$，使$F(x)$支配$F(x^)$，那么称$F(x^*）$为帕累托最优目标向量。换言之，一个目标中帕累托最优点的任何改进都必须导致至少另一个目标的恶化。所有帕累托最优点的集合称为帕累托集合(PS)，所有帕累托最优目标向量的集合称为帕累托阵(PF)。 在多目标优化的许多实际应用中，决策者需要近似于PF来选择最终的首选解决方案。大多数MOPs可能有许多甚至无限帕累托最优向量。获取完整的PF是非常耗时的。另一方面，由于信息的溢出，决策者可能对拥有过多的帕累托最优向量不感兴趣。因此，许多多目标优化算法都是为了找到一个可管理的帕累托最优向量。一些研究者也尝试用数学模型来近似PF。 目前没有涉及到分解的大部分多目标进化算法，将MOP视为一个整体。它们不会将每个单独的解决方案与任何特定的标量优化问题关联起来。在标量目标优化问题，所有的解决方案都可以在它们目标函数值的基础上进行比较，标量目标的任务进化算法(EA)往往是寻找一个单一的最优的解决方案。然而，在MOPs中，支配并非定义目标函数中解的完整顺序空间，MOEAs旨在产生一些帕累托最优尽可能多样化的解决方案来代表整体PF。 因此，最初设计用于标量优化的传统选择算子不能直接用于非分解MOEAs。那么可以说，如果有一种适合度分配方案，用于为单个解决方案分配一个相对适合度值，以反映其选择的实用价值，那么标量优化EAs可以很容易地扩展到处理MOPs。因此，适应度分配一直是当前的一个主要问题MOEA研究。目前流行的适应度分配策略包括基于交互目标的适应度分配，如向量评价遗传算法(VEGA);基于优势的适应度分配，如帕累托存档进化策略（PAES）。 分解的思想在一些针对MOPs的元启发式中得到了一定程度的应用。例如，两阶段局部搜索(TPLS)考虑了一组标量优化问题，其中目标是所考虑的MOP中的目标的集合，基于集合系数的序列将标量优化算法应用于这些标量优化问题中，将前一个问题得到的解作为下一个问题求解的起点，因为它的集合目标与前一个问题的集合目标略有不同。多目标遗传局部搜索(MOGLS)旨在同时优化加权和方法或Tchebycheff方法构建的所有聚合。在每次迭代中，它优化随机生成的聚合目标。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode配置c环境]]></title>
    <url>%2F2018%2F12%2F22%2Fvscode%E9%85%8D%E7%BD%AEc%2F</url>
    <content type="text"><![CDATA[在sublime和vscode的权衡下，选择了vscode，毕竟之前一直用的是sublime，想换一换了。于是就遇到一个老问题，配环境！ 此内容几乎完全来自于某乎 安装 vscode LLVM 选Pre-Built Binaries中的Clang for Windows (64-bit)，不需要下.sig文件 添加环境变量：Add LLVM to the system PATH for all users 安装路径推荐：C:\LLVM 工具链：MinGW 其他默认 MinGW-w64 - for 32 and 64 bit Windows 链接，提取码：dclo 下好后，把x86_64-7.2.0-posix-seh-rt_v5-rev0.7z\mingw64 中所有的文件都复制到 C:\LLVM中 检验：打开cmd 输入gcc，如果为no input files而不是其他，即为成功。 ​ 输入clang，如果为no input files而不是其他，即为成功。 ​ 插件一定要下： C/C++ C/C++ Clang Command Adapter Code Runner 自由推荐： Bracket Pair Colorizer：彩虹花括号 Include Autocomplete：提供头文件名字的补全 One Dark Pro：VS Code安装量最高的主题 环境配置打开vscode，一定要选 open folder 选择刚才那个文件夹，点VS Code上的新建文件夹，名称为.vscode（这样做的原因是Windows的Explorer不允许创建的文件夹第一个字符是点），然后创建 launch.json，tasks.json，settings.json，c_cpp_properties.json放到.vscode文件夹下 launch.json:12345678910111213141516171819202122232425262728// https://github.com/Microsoft/vscode-cpptools/blob/master/launch.md&#123; &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;(gdb) Launch&quot;, // 配置名称，将会在启动配置的下拉菜单中显示 &quot;type&quot;: &quot;cppdbg&quot;, // 配置类型，这里只能为cppdbg &quot;request&quot;: &quot;launch&quot;, // 请求配置类型，可以为launch（启动）或attach（附加） &quot;program&quot;: &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot;, // 将要进行调试的程序的路径 &quot;args&quot;: [], // 程序调试时传递给程序的命令行参数，一般设为空即可 &quot;stopAtEntry&quot;: false, // 设为true时程序将暂停在程序入口处，我一般设置为true &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, // 调试程序时的工作目录 &quot;environment&quot;: [], // （环境变量？） &quot;externalConsole&quot;: true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 &quot;internalConsoleOptions&quot;: &quot;neverOpen&quot;, // 如果不设为neverOpen，调试时会跳到“调试控制台”选项卡，你应该不需要对gdb手动输命令吧？ &quot;MIMode&quot;: &quot;gdb&quot;, // 指定连接的调试器，可以为gdb或lldb。但目前lldb在windows下没有预编译好的版本。 &quot;miDebuggerPath&quot;: &quot;gdb.exe&quot;, // 调试器路径，Windows下后缀不能省略，Linux下则去掉 &quot;setupCommands&quot;: [ // 用处未知，模板如此 &#123; &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: false &#125; ], &quot;preLaunchTask&quot;: &quot;Compile&quot; // 调试会话开始前执行的任务，一般为编译程序。与tasks.json的label相对应 &#125; ]&#125; tasks.json:命令行参数方面，-std根据自己的需要修改。如果使用Clang编写C语言，把command的值改成clang。如果使用MinGW，编译C用gcc，编译c++用g++，并把-target和-fcolor那两条删去。 123456789101112131415161718192021222324252627282930313233// https://code.visualstudio.com/docs/editor/tasks&#123; &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ &#123; &quot;label&quot;: &quot;Compile&quot;, // 任务名称，与launch.json的preLaunchTask相对应 &quot;command&quot;: &quot;clang++&quot;, // 要使用的编译器 &quot;args&quot;: [ &quot;$&#123;file&#125;&quot;, &quot;-o&quot;, // 指定输出文件名，不加该参数则默认输出a.exe，Linux下默认a.out &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot;, &quot;-g&quot;, // 生成和调试有关的信息 &quot;-Wall&quot;, // 开启额外警告 &quot;-static-libgcc&quot;, // 静态链接 &quot;-fcolor-diagnostics&quot;, // 彩色的错误信息？但貌似clang默认开启而gcc不接受此参数 &quot;--target=x86_64-w64-mingw&quot;, // clang的默认target为msvc，不加这一条就会找不到头文件；Linux下去掉这一条 &quot;-std=c++17&quot; // C语言最新标准为c11，或根据自己的需要进行修改 ], // 编译命令参数 &quot;type&quot;: &quot;shell&quot;, // 可以为shell或process，前者相当于先打开shell再输入命令，后者是直接运行命令 &quot;group&quot;: &#123; &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true // 设为false可做到一个tasks.json配置多个编译指令，需要自己修改本文件，我这里不多提 &#125;, &quot;presentation&quot;: &#123; &quot;echo&quot;: true, &quot;reveal&quot;: &quot;always&quot;, // 在“终端”中显示编译信息的策略，可以为always，silent，never。具体参见VSC的文档 &quot;focus&quot;: false, // 设为true后可以使执行task时焦点聚集在终端，但对编译c和c++来说，设为true没有意义 &quot;panel&quot;: &quot;shared&quot; // 不同的文件的编译信息共享一个终端面板 &#125; // &quot;problemMatcher&quot;:&quot;$gcc&quot; // 如果你不使用clang，去掉前面的注释符，并在上一条之后加个逗号。照着我的教程做的不需要改（也可以把这行删去) &#125; ]&#125; settings.json: Code Runner的命令行和某些选项可以根据自己的需要在此处修改，用法还是参见此扩展的文档和百度gcc使用教程。如果你要使用其他地方的头文件和库文件，可能要往clang.cflags和clang.cxxflags里加-I和-L，用法百度gcc使用教程。12345678910111213141516171819202122232425262728293031&#123; &quot;files.defaultLanguage&quot;: &quot;cpp&quot;, // ctrl+N新建文件后默认的语言 &quot;editor.formatOnType&quot;: true, // 输入时就进行格式化，默认触发字符较少，分号可以触发 &quot;editor.snippetSuggestions&quot;: &quot;top&quot;, // snippets代码优先显示补全 &quot;code-runner.runInTerminal&quot;: true, // 设置成false会在“输出”中输出，无法输入 &quot;code-runner.executorMap&quot;: &#123; &quot;c&quot;: &quot;cd $dir &amp;&amp; clang $fileName -o $fileNameWithoutExt.exe -Wall -g -Og -static-libgcc -fcolor-diagnostics --target=x86_64-w64-mingw -std=c11 &amp;&amp; $dir$fileNameWithoutExt&quot;, &quot;cpp&quot;: &quot;cd $dir &amp;&amp; clang++ $fileName -o $fileNameWithoutExt.exe -Wall -g -Og -static-libgcc -fcolor-diagnostics --target=x86_64-w64-mingw -std=c++17 &amp;&amp; $dir$fileNameWithoutExt&quot; &#125;, // 设置code runner的命令行 &quot;code-runner.saveFileBeforeRun&quot;: true, // run code前保存 &quot;code-runner.preserveFocus&quot;: true, // 若为false，run code后光标会聚焦到终端上。如果需要频繁输入数据可设为false &quot;code-runner.clearPreviousOutput&quot;: false, // 每次run code前清空属于code runner的终端消息 &quot;C_Cpp.clang_format_sortIncludes&quot;: true, // 格式化时调整include的顺序（按字母排序） &quot;C_Cpp.intelliSenseEngine&quot;: &quot;Default&quot;, // 可以为Default或Tag Parser，后者较老，功能较简单。具体差别参考cpptools扩展文档 &quot;C_Cpp.errorSquiggles&quot;: &quot;Disabled&quot;, // 因为有clang的lint，所以关掉 &quot;C_Cpp.autocomplete&quot;: &quot;Disabled&quot;, // 因为有clang的补全，所以关掉 &quot;clang.cflags&quot;: [ // 控制c语言静态检测的参数 &quot;--target=x86_64-w64-mingw&quot;, &quot;-std=c11&quot;, &quot;-Wall&quot; ], &quot;clang.cxxflags&quot;: [ // 控制c++静态检测时的参数 &quot;--target=x86_64-w64-mingw&quot;, &quot;-std=c++17&quot;, &quot;-Wall&quot; ], &quot;clang.completion.enable&quot;:true // 效果效果比cpptools要好&#125; c_cpp_properties.json: 1234567891011121314151617181920212223&#123; &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;MinGW&quot;, &quot;intelliSenseMode&quot;: &quot;clang-x64&quot;, &quot;compilerPath&quot;: &quot;C:/LLVM/bin/gcc.exe&quot;, &quot;includePath&quot;: [ &quot;$&#123;workspaceFolder&#125;&quot; ], &quot;defines&quot;: [], &quot;browse&quot;: &#123; &quot;path&quot;: [ &quot;$&#123;workspaceFolder&#125;&quot; ], &quot;limitSymbolsToIncludedHeaders&quot;: true, &quot;databaseFilename&quot;: &quot;&quot; &#125;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++17&quot; &#125; ], &quot;version&quot;: 4&#125; 编译技巧 ctrl+shift+B单纯编译 按F5为运行并调试（运行前会自动编译） 加断点在列号前面点一下就行，如果想从一开始就停下来，可以加在main函数那里，或者launch.json中设置&quot;stopAtEntry&quot;: true。 按f11可以一步一步进行，箭头所指的那行代码就是下一步要运行的代码。 左边有个调试栏，可以看到变量的值,自动栏没有的可以手动添加表达式 把鼠标放到变量上可以看到变量的值，但是只能识别简单的表达式 栈帧对于递归很有用；在某些时候还可以抓取“异常”。 如果你不需要调试，可以直接右键选run code。 输出端可以输入，在settings.json中添加&quot;code-runner.runInTerminal&quot;: true]]></content>
      <categories>
        <category>vscode</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo恢复]]></title>
    <url>%2F2018%2F12%2F22%2Fhexo%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[想重新开始写博客，第一件事当然是恢复博客的正常使用啦！搜了小半天终于找到了符合我条件的教程。 背景：起初已配置好，但之后从未使用，期间重新做了一次系统。待我有时间再查询一下如何备份至云端。(已完成) 恢复安装git、node.js在原来储存博客的文件夹中(blog)`右键`-&gt;`选择`-&gt;`Git Bash Here` 再输入：1npm install hexo -g 因为重装系统有可能删除了配置文件包括环境变量里面的，没有配置 name 和 email 的话，git 是无法正常工作的。所以首先得重新配置name跟email在git bash里面输入下面两行 12git config --global user.name &quot;你的名字&quot;git config --global user.email &quot;你的邮箱&quot; 如果上面两条命令fail了的话，记得先用命令git init再输入上面两条命令 创建SSH输入 ssh-keygen -t rsa -C &quot;myemail@example.com&quot; 再按两次回车输入 cd ~/.ssh 再输入 cat id_rsa.pub会输出 12ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxmyemail@example.com 登陆我的Github在settings中找到ssh and GPG keys点击new ssh key，title随意 把ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx输入到key位置在git bash输入ssh -T git@github.com 可验证时候正确 修改blog目录下_config.yml如果执行hexo deploy提示 123Logon failed, use ctrl+c to cancel basic credential prompt.bash: /dev/tty: No such device or addressINFO Catch you later 则需要把下方的 1234deploy: type: git repo: https://github.com/mygithubName/mygithubName.github.io.git branch: master 修改成： 1234deploy: type: git repo: ssh://git@github.com/mygithubName/mygithubName.github.io.git branch: master 执行 hexo g -d 大功告成 常规操作： 1234hexo cleanhexo generatehexo server(本地测试用)hexo deploy 至此，网站已基本恢复。 云备份至Github为了以后更方便的从云端备份下来，我又查了一些教程，下面便是详细步骤 基本原理网站的部署其实就是生成静态文件，hexo下所有生成的静态文件会放在public/文件夹中，所谓部署deploy其实就是 将public/文件夹中内容上传到git仓库myname.github.io中。也就是说，你的仓库myname.github.io中的文件只是blog（或者命名为hexo）文件夹下的public/下的文件。本背景下，方便放在myname.github.io的repository下创建一个分支来管理 建立分支hexo 在本地磁盘下（位置任意）右键 -&gt; Git bash here，执行以下指令将myname.github.io项目文件克隆到本地： 1git clone git@github.com:myname/myname.github.io.git 此目录下便有myname.github.io文件夹，把此文件夹中除了.git之外的所有文件删掉 把blog中所有文件复制到myname.github.io 文件夹中，其中会提示是否替换，选择跳过。 如果有.gitignore文件，把里面的内容修改成 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 如果没有此文件，便在git bash中输入touch .gitignore 在myname.github.io 文件夹中右键 -&gt; Git bash here 创建一个叫hexo的分支并切换到这个分支上 git checkout -b hexo 提交复制过来的文件到暂存区git add --all 提交git commit -m &quot;&quot; 推送分支到githubgit push --set-upstream origin hexo在github上可以看到 branch中有master和hexo，至此，已经成功。并且hexo中的文件便在.gitirnore所忽略而剩下需要备份的文件， 更新文章，修改主题等步骤 在github中myname.github.io中，找到settings -&gt; Branches 将hexo设为默认 从此更新文章，修改主题等操作一直都在myname.github.io了 执行如下 123456hexo cleanhexo generatehexo deploygit add .git commit -m &quot;&quot;git push origin hexo 注意 -m “要写一点东西” 从github上还原此部分完全摘抄自网站，我并非试过，并不知道是否可行。 克隆项目 1git clone -b hexo git@github.com:myname/myname.github.io.git 进入博客目录 1cd myname.github.io.git 切换到博客文件分支 1git checkout -b hexo origin/hexo 安装hexo 1nmp install hexo --save 编辑，查看 12hexo ghexo s 提交git若提交过程中出现ERROR Deployer not found: git,可执行以下代码，然后重新提交 1npm install hexo-deployer-git --save 新的文章等更新 123git add .git commit -m &quot;新增博客&quot;git push origin hexo END]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小小念]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%B0%8F%E5%B0%8F%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[​ 啊啊啊啊啊啊，当时心一热搭建了一个博客，时隔四个月一直却没有更新博客善哉善哉，但期间也经历了好多，从准备保研的焦头烂额，到现在天天看剧打游戏的糜烂生活，落差之大，以至于日日积累的罪恶感促使我又有好好学习之意，遂重新在网上找了小半天的教程，把静静躺在H盘的blog文件夹重新唤醒。​ 当时心心念的保研，经历了很多很多次的失败，多方权衡下，最后以去南方科技大学而告终，毕业设计的题目也基本确定，很经典的问题——多目标优化，这也可能是我研究生研究的方向了。 ​ 从今天开始，可能就要持续更新我的小博客，记录一下~~]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>感慨</tag>
        <tag>随想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2018%2F08%2F23%2Fmarkdown%2F</url>
    <content type="text"><![CDATA[Typora For Markdown 语法，才刚刚学习，用着可能不熟练，先自行收藏一下~ 数学表达式要启用这个功能，首先到Preference-&gt;Editor中启用。然后使用`符号包裹Tex命令，例如：`$lim_{x \to \infty} \ exp(-x)=0将产生如下的数学表达式： $\lim_{x \to \infty} \exp(-x)=0$ 下标下标使用~包裹，例如：H~2~O将产生H~2~O, 即水的分子式。 上标上标使用^包裹，例如：y^2^=4将产生表达式y^2^ = 4 插入表情:happy:使用:happy:输入表情:happy:,使用:sad:输入表情:sad:,使用:cry:输入表情:cry:等。以此类推！ 下划线用HTML的语法&lt;u&gt;Underline&lt;/u&gt;将产生下划线Underline. 删除线GFM添加了删除文本的语法，这是标准的Markdown语法木有的。使用~~包裹的文本将会具有删除的样式，例如~删除文本~将产生删除文本的样式。 代码 使用`包裹的内容将会以代码样式显示，例如 1使用`printf()` 则会产生printf()样式。 输入`12* ​1234public Class HelloWorld&#123; System.out.println("Hello World!");&#125;​ 1234567将会产生~~~javapublic Class HelloWorld&#123; System.out.println(&quot;Hello World!&quot;);&#125; 强调使用两个*号或者两个_包裹的内容将会被强调。例如 12**使用两个*号强调内容**__使用两个下划线强调内容__ 将会输出 使用两个*号强调内容使用两个下划线强调内容Typroa 推荐使用两个*号。 斜体在标准的Markdown语法中，*和_包裹的内容会是斜体显示，但是GFM下划线一般用来分隔人名和代码变量名，因此我们推荐是用星号来包裹斜体内容。如果要显示星号，则使用转义： 1\* 插入图片我们可以通过拖拉的方式，将本地文件夹中的图片或者网络上的图片插入。 ​ ​ 插入URL连接使用尖括号包裹的url将产生一个连接，例如：&lt;www.baidu.com&gt;将产生连接:. 如果是标准的url，则会自动产生连接，例如:www.google.com 目录列表Table of Contents（TOC）输入[toc]然后回车，将会产生一个目录，这个目录抽取了文章的所有标题，自动更新内容。 水平分割线使用***或者---，然后回车，来产生水平分割线。 标注我们可以对某一个词语进行标注。例如 12某些人用过了才知道[^注释][^注释]:Somebody that I used to know. 将产生： 某些人用过了才知道注释注释: Somebody that I used to know. 把鼠标放在注释上，将会有提示内容。 表格12345|姓名|性别|毕业学校|工资||:---|:---:|:---:|---:||杨洋|男|重庆交通大学|3200||峰哥|男|贵州大学|5000||坑货|女|北京大学|2000| 将产生: 姓名 性别 毕业学校 工资 杨洋 男 重庆交通大学 3200 峰哥 男 贵州大学 5000 坑货 女 北京大学 2000 其中代码的第二行指定对齐的方式，第一个是左对齐，第二个和第三个是居中，最后一个是右对齐。 数学表达式块输入两个美元符号，然后回车，就可以输入数学表达式块了。例如： 1$$\mathbf&#123;V&#125;_1 \times \mathbf&#123;V&#125;_2 = \begin&#123;vmatrix&#125; \mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp; \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; 0 \\\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp; \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; 0 \\\end&#123;vmatrix&#125;$$ 将会产生: \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\\frac{\partial X}{\partial u} & \frac{\partial Y}{\partial u} & 0 \\\frac{\partial X}{\partial v} & \frac{\partial Y}{\partial v} & 0 \\\end{vmatrix}任务列表使用如下的代码创建任务列表，在[]中输入x表示完成，也可以通过点击选择完成或者没完成。 1234- [ ] 吃饭- [ ] 逛街- [ ] 看电影- [ ] 约泡 [x] 吃饭 ​ [x] 逛街 ​ [x] 看电影 ​ [x] 约泡 列表输入+, -, *,创建无序的列表，使用任意数字开头，创建有序列表，例如： 1234**无序的列表*** tfboys* 杨洋* 我爱你 无序的列表 tfboys 杨洋 我爱你 1234**有序的列表**1. 苹果6. 香蕉10. 我都不喜欢 有序的列表 苹果 香蕉 我都不喜欢 块引用使用&gt;来插入块引用。例如： 1&gt;这是一个块引用！ 将产生： 这是一个块引用！ 标题使用#表示一级标题，##表示二级标题，以此类推，有6个标题。]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello]]></title>
    <url>%2F2018%2F08%2F22%2FHello%2F</url>
    <content type="text"><![CDATA[大学已然过三年，也浑浑噩噩过了三年。一时兴起，想搞一个属于自己的博客，把未来生活与学习路上的点点滴滴记录下来，万事开头难，于是偷个懒，就把建这个网站的过程来作为我的第一篇博客吧，记录一下，哈哈哈哈哈 安装安装git、node.js新建一个储存博客的文件夹(blogblog)打开后右键-选择-Git Bash Here输入12npm install hexo -g hexo init -g表示全局安装, npm默认为当前项目安装 node_modules：是依赖包 public：存放的是生成的页面 source：用命令创建的各种文章 themes：主题 _config.yml：整个博客的配置 db.json：source解析所得到的 package.json：项目所需模块项目的配置信息 输入123hexo cleanhexo generatehexo server 游览器打开 http://localhost:4000 但是只能在本地登录，下一步便是可以从其他地点登录 搭桥到github 选择New repository/myname.github.iomyname 必须为github的账号名 输入 12git config --global user.name &quot;my name&quot;git config --global user.email &quot;my email&quot; 创建SSH输入 ssh-keygen -t rsa -C &quot;myemail@example.com&quot; 再按两次回车输入 cd ~/.ssh 再输入 cat id_rsa.pub会输出 12ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxmyemail@example.com 把ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx输入到key位置输入ssh -T git@github.com 可验证时候正确 打开在blogblog目录下的_config.yml 注意冒号后有一个空格 1234deploy: type: git repo: https://github.com/mygithubName/mygithubName.github.io.git branch: master 注意：如果同一个电脑建第二个hexo需要如下： 1234deploy: type: git repo: git@github.com:mygithubName/mygithubName.github.io.git branch: master 在blogblog目录中打开 gitbash执行npm i hexo-server再执行npm install hexo-deployer-git --save执行 123hexo cleanhexo generatehexo deploy 打开 myname.github.io 就可以看到了~ 绑定域名 买一个域名，我是在阿里云买的 在项目的source文件夹中新建一个名为CNAME的文件(不需要文件后缀)，编辑文档时把所购 买的域名添加其中，注意，只可添加一个 在DNS中添加一条记录，也可以直接通过新手引导设置，其中所需的地址只需在cmd中执行 ping myname.github.io 再执行一次 123hexo cleanhexo generatehexo deploy 更换主题可以访问hexo的主题官网，我选择的是NexT主题，一来好看实用；二来很多功能都已经写好，添加功能时会更方便一些(渣渣没办法…)，因此以下为安装NexT主题为例。 执行$ git clone https://github.com/theme-next/hexo-theme-next-themes/next 打开blogblog目录的_config.yml ,其中，修改为 theme: next emmmmm…. 没错 主题就换完了，打开试试，突然就高大上了~ 修改blogblog下_config.yml的:1234567title: 清 泉subtitle:description:keywords:author: springlanguage: zh-CNtimezone: 修改blogblog/themes/next/_config.yml: 123456789menu: home: / || home #about: /about/ || user #tags: /tags/ || tags #categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 我习惯修改为 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 想要选择哪个把前面的#去掉即可 对于tags项： 执行hexo new page &quot;tags&quot;打开\source\tags\index.md 123456---title:date: 2018-08-21 14:56:51type: &quot;tags&quot;comments: false--- 对于categories项： 执行hexo new page &quot;categories&quot; 打开\source\categories\index.md 123456--- title: date: 2018-08-21 14:57:23 type: &quot;categories&quot; comments: false --- Next主题 又分为四种形式，可自选： 12345# Schemesscheme: Muse#scheme: Mist#scheme: Pisces#scheme: Gemini 头像12345678avatar: url: #/images/avatar.gif 你的头像图片的路径 # If true, the avatar would be dispalyed in circle. rounded: false # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false 删除底部隐藏由Hexo强力驱动、主题—NexT.Mist 打开blogblog/themes/next/layout/_partials/footer.swig，注释掉相应代码 1234567891011121314151617181920212223242526//用下面的符号注释，注释代码用下面括号括起来 &lt;!-- --&gt; &lt;!-- &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &#123;% if theme.footer.powered %&#125; &lt;div class=&quot;powered-by&quot;&gt;&#123;# #&#125;&#123;&#123; __(&apos;footer.powered&apos;, &apos;&lt;a class=&quot;theme-link&quot; target=&quot;_blank&quot; href=&quot;https://hexo.io&quot;&gt;Hexo&lt;/a&gt;&apos;) &#125;&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.theme.enable %&#125; &lt;div class=&quot;theme-info&quot;&gt;&#123;# #&#125;&#123;&#123; __(&apos;footer.theme&apos;) &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class=&quot;theme-link&quot; target=&quot;_blank&quot; href=&quot;https://github.com/iissnan/hexo-theme-next&quot;&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt; &#123;% endif %&#125; &#123;% if theme.footer.custom_text %&#125; &lt;div class=&quot;footer-custom&quot;&gt;&#123;# #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;--&gt; 背景动态 canvas_nest git clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest 把&lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt; 插入至\blogblog\themes\next\layout\_layout.swig如下： 1234567891011&lt;html&gt;&lt;head&gt; ...&lt;/head&gt;&lt;body&gt; ... ... ... 插入到这里&lt;/body&gt;&lt;/html&gt; 再修改主题配置文件 打开/next/_config.yml,修改如下： 123# Canvas-nest# Dependencies: https://github.com/theme-next/theme-next-canvas-nestcanvas_nest: true 添加DaoVoice在线联系 首先到DaoVoice注册账号，邀请码是0f81ff2f ，登录成过后，进入到后台管理，点击应用设置——&gt;安装到网站查看安装代码和AppID。 找到app_id ，在主题配置文件中找到(没有的话添加) 123# Online contact daovoice: truedaovoice_app_id: 这里填你的刚才获得的 app_id 打开/themes/next/layout/_partials/head.swig ,代码放进去，哪行都可以 123456789&#123;% if theme.daovoice %&#125; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i[&quot;DaoVoiceObject&quot;]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset=&quot;utf-8&quot;;m.parentNode.insertBefore(a,m)&#125;)(window,document,&quot;script&quot;,(&apos;https:&apos; == document.location.protocol ? &apos;https:&apos; : &apos;http:&apos;) + &quot;//widget.daovoice.io/widget/0f81ff2f.js&quot;,&quot;daovoice&quot;) daovoice(&apos;init&apos;, &#123; app_id: &quot;&#123;&#123;theme.daovoice_app_id&#125;&#125;&quot; &#125;); daovoice(&apos;update&apos;); &lt;/script&gt;&#123;% endif %&#125; 在DaoVoice中找到聊天设置调节窗口的颜色以及位置我的参数：右侧像素20.0，下侧像素：80.0 在右上角或者左上角实现fork me on github 点击这里 或者 这里挑选自己喜欢的样式，并复制代码。 然后粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中(放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面)，并把href改为你的github地址 。 添加RSS 在blogblog中打开githash 执行 npm install --save hexo-generator-feed 在blogblog/_config.yml中添加 123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed 在主题配置文件中修改为： 1234# Set rss to false to disable feed link.# Leave rss as empty to use site&apos;s feed link.# Set rss to specific value if you have burned your feed already.rss: /atom.xml 添加音乐 在博客配置文件中执行npm install hexo-tag-aplayer@2.0.1 新建themes\next\source\dist\music.js ,添加内容： 12345678910111213141516171819202122232425const ap = new APlayer(&#123; container: document.getElementById(&apos;aplayer&apos;), fixed: true, autoplay: false, audio: [ &#123; name: &quot;Dream It Possible&quot;, artist: &apos;Delacey&apos;, url: &apos;http://www.ytmp3.cn/down/47868.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/84.jpg&apos;, &#125;, &#123; name: &apos;いとしすぎて&apos;, artist: &apos;KG&apos;, url: &apos;http://www.ytmp3.cn/down/35726.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/8.jpg&apos;, &#125;, &#123; name: &apos;茜さす&apos;, artist: &apos;Aimer&apos;, url: &apos;http://www.ytmp3.cn/down/44578.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/96.jpg&apos;, &#125; ]&#125;); 修改网站主题字体大小在主题配置文件中123456789101112131415161718192021222324252627282930313233font: enable: true # Uri of fonts host. E.g. //fonts.googleapis.com (Default) # 亲测这个可用，如果不可用，自己搜索 [Google 字体 国内镜像]，找个能用的就行 host: https://fonts.cat.net # Global font settings used on &lt;body&gt; element. # 全局字体，应用在 body 元素上 global: external: true family: Lato size: 16 #csdn上就是16看着舒服多了 # 标题字体 (h1, h2, h3, h4, h5, h6) headings: external: true family: Roboto Slab # 文章字体 posts: external: true family: # Logo 字体 logo: external: true family: Lobster Two size: 24 # 代码字体，应用于 code 以及代码块 codes: external: true family: Roboto Mono 站点收录百度收录在主题配置文件中修改成： 1baidu_site_verification: true 进入百度站点检验网站 ，选择http:// ,purespring.top 信息技术 由于前两个验证一直通过不了，所以我选择了CNAME验证 进入阿里云 我是在阿里云买的域名，所以进入那里。 进入解析设置 添加记录 类型： CNAME 主机记录： xxxxx.purespring.top(这个会告诉你) 记录值：zz.baidu(这个会告诉你) 就可以完成确认 在hexo g操作之前，验证文件先不加入到source目录中，等待hexo g执行成功后，手动把验证文件copy到Hexo根目录下的public目录中，然后执行hexo d上传到远程仓库，最后再测试平台上的验证文件，可以看到百度、谷歌都验证成功了。最后就等待收录通过后，输入site:地址，测试一下是否搜索到博客地址。谷歌的收录速度很快，上传到仓库后几个小时就可以搜索到我的网站了，而百度则慢点，可能要一两个星期，有备案填入在百度上则会加快收录速度。 更改细节主题在文件\themes\next\source\css\_custom\custom.styl中，放入如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600// Custom styles//首页头部样式.header &#123; background: url("/images/header-bk.jpg");&#125;.site-meta &#123; margin-left: 0px; text-align: center;&#125;.site-meta .site-title &#123; font-size: 20px; font-family: 'Comic Sans MS', sans-serif; color: #fff; letter-spacing: 1px; width: 81%;&#125;// 点文章进去的页面背景色.container &#123; background-color: rgba(255, 255, 255, 0.747);&#125;// 页面留白更改.header-inner &#123; padding-top: 0px; padding-bottom: 0px;&#125;.posts-expand &#123; padding-top: 80px;&#125;.posts-expand .post-meta &#123; margin: 5px 0px 0px 0px;&#125;.post-button &#123; margin-top: 0px;&#125;// 顶栏宽度.container .header-inner &#123; width: 100%;&#125;// 站点名背景.brand&#123; background-color: rgb(56, 53, 53); margin-top: 15px; padding: 0px;&#125;// 站点名字体.site-title &#123; line-height: 35px; letter-spacing: 3px;&#125;// 站点子标题.site-subtitle&#123; margin: 0px; font-size: 16px; letter-spacing: 1px; padding-bottom: 3px; font-weight: bold; color: rgb(219, 95, 95); border-bottom-width: 3px; border-bottom-style: solid; border-bottom-color: rgb(161, 102, 171);&#125;.logo-line-after &#123; display: none;&#125;.logo-line-before &#123; display: none;&#125;// 菜单.menu &#123; float: none;&#125;// 菜单超链接字体大小.menu .menu-item a &#123; font-size: 14px; color: rgb(15, 46, 65); border-radius: 4px;&#125;// 菜单各项边距.menu .menu-item &#123; margin: 5px 15px;&#125;// 菜单超链接样式.menu .menu-item a:hover &#123; border-bottom-color: rgba(161, 102, 171, 0);&#125;// 文章.post &#123; margin-bottom: 50px; padding: 45px 36px 36px 36px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255);&#125;// 文章标题字体.posts-expand .post-title &#123; font-size: 26px; font-weight: 700;&#125;// 文章标题动态效果.posts-expand .post-title-link::before &#123; background-image: linear-gradient(90deg, #a166ab 0%, #ef4e7b 25%, #f37055 50%, #ef4e7b 75%, #a166ab 100%);&#125;// 文章元数据（meta）留白更改.posts-expand .post-meta &#123; margin: 10px 0px 20px 0px;&#125;// 文章的描述description.posts-expand .post-meta .post-description &#123; font-style: italic; font-size: 14px; margin-top: 30px; margin-bottom: 0px; color: #666;&#125;// [Read More]按钮样式.post-button .btn &#123; color: rgba(219, 210, 210, 0.911)!important; background-color: rgba(56, 52, 52, 0.911); border-radius: 3px; font-size: 15px; box-shadow: inset 0px 0px 10px 0px rgba(0, 0, 0, 0.35); border: none !important; transition-property: unset; padding: 0px 15px;&#125;.post-button .btn:hover &#123; color: rgba(219, 210, 210, 0.911) !important; border-radius: 3px; font-size: 15px; box-shadow: inset 0px 0px 10px 0px rgba(0, 0, 0, 0.35); background-image: linear-gradient(100deg, #a166ab 0%, #ef4e7b 25%, #f37055 50%, #ef4e7b 75%, #a166ab 100%);&#125;// 去除在页面文章之间的分割线.posts-expand .post-eof &#123; margin: 0px; background-color: rgba(255, 255, 255, 0);&#125;// 去除页面底部页码上面的横线.pagination &#123; border: none; margin: 0px;&#125;// 页面底部页码.pagination .page-number.current &#123; border-radius: 100%; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgba(255, 255, 255, 0.35);&#125;.pagination .prev, .pagination .next, .pagination .page-number &#123; margin-bottom: 10px; border: none;&#125;.pagination .space &#123; color: rgb(255, 255, 255);&#125;// 页面底部页脚.footer &#123; line-height: 1.5; background-color: rgba(255, 255, 255, 0.75); color: #333; border-top-width: 3px; border-top-style: solid; border-top-color: rgb(161, 102, 171); box-shadow: 0px -10px 10px 0px rgba(0, 0, 0, 0.15);&#125;// 文章底部的tags.posts-expand .post-tags a &#123; border-bottom: none; margin-right: 0px; font-size: 13px; padding: 0px 5px; border-radius: 3px; transition-duration: 0.2s; transition-timing-function: ease-in-out; transition-delay: 0s;&#125;.posts-expand .post-tags a:hover &#123; background: #eee;&#125;// 文章底部留白更改.post-widgets &#123; padding-top: 0px;&#125;.post-nav &#123; margin-top: 30px;&#125;// 文章底部页面跳转.post-nav-item a &#123; color: rgb(80, 115, 184); font-weight: bold;&#125;.post-nav-item a:hover &#123; color: rgb(161, 102, 171); font-weight: bold;&#125;// 文章底部评论.comments &#123; background-color: rgb(255, 255, 255); box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.35); margin: 80px 0px 40px 0px;&#125;// 超链接样式a &#123; color: rgb(80, 115, 184); border-bottom-color: rgb(80, 115, 184);&#125;a:hover &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;// 分割线样式hr &#123; margin: 10px 0px 30px 0px;&#125;// 文章内标题样式（左边的竖线）.post-body h2, h3, h4, h5, h6 &#123; border-left: 4px solid rgb(161, 102, 171); margin-left: -36px; padding-left: 32px;&#125;// 去掉图片边框.posts-expand .post-body img &#123; border: none; padding: 0px;&#125;.post-gallery .post-gallery-img img &#123; padding: 3px;&#125;// 文章``代码块的自定义样式code &#123; margin: 0px 4px;&#125;// 文章```代码块顶部样式.highlight figcaption &#123; margin: 0em; padding: 0.5em; background: #eee; border-bottom: 1px solid #e9e9e9;&#125;.highlight figcaption a &#123; color: rgb(80, 115, 184);&#125;// 文章```代码块diff样式pre .addition &#123; background: #e6ffed;&#125;pre .deletion &#123; background: #ffeef0;&#125;// 右下角侧栏按钮样式.sidebar-toggle &#123; right: 10px; bottom: 43px; background-color: rgba(247, 149, 51, 0.75); border-radius: 5px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.35);&#125;.page-post-detail .sidebar-toggle-line &#123; background: rgb(17, 185, 163);&#125;// 右下角返回顶部按钮样式.back-to-top &#123; line-height: 1.5; right: 10px; padding-right: 5px; padding-left: 5px; padding-top: 2.5px; padding-bottom: 2.5px; background-color: rgba(247, 149, 51, 0.75); border-radius: 5px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.35);&#125;.back-to-top.back-to-top-on &#123; bottom: 10px;&#125;// 侧栏.sidebar &#123; box-shadow: inset 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgba(0, 0, 0, 0.75);&#125;.sidebar-inner &#123; margin-top: 30px;&#125;// 侧栏顶部文字.sidebar-nav li &#123; font-size: 15px; font-weight: bold; color: rgb(7, 179, 155);&#125;.sidebar-nav li:hover &#123; color: rgb(161, 102, 171);&#125;.sidebar-nav .sidebar-nav-active &#123; color: rgb(7, 179, 155); border-bottom-color: rgb(161, 102, 171); border-bottom-width: 1.5px;&#125;.sidebar-nav .sidebar-nav-active:hover &#123; color: rgb(7, 179, 155);&#125;// 侧栏站点概况行高.site-overview &#123; line-height: 1.3;&#125;// 侧栏头像（圆形以及旋转效果）.site-author-image &#123; border: 2px solid rgb(255, 255, 255); border-radius: 100%; transition: transform 1.0s ease-out;&#125;img:hover &#123; transform: rotateZ(360deg);&#125;.posts-expand .post-body img:hover &#123; transform: initial;&#125;// 侧栏站点作者名.site-author-name &#123; display: none;&#125;// 侧栏站点描述.site-description &#123; letter-spacing: 5px; font-size: 15px; font-weight: bold; margin-top: 15px; margin-left: 13px; color: rgb(243, 112, 85);&#125;// 侧栏站点文章、分类、标签.site-state &#123; line-height: 1.3; margin-left: 12px;&#125;.site-state-item &#123; padding: 0px 15px; border-left: 1.5px solid rgb(161, 102, 171);&#125;// 侧栏RSS按钮样式.feed-link &#123; margin-top: 15px; margin-left: 7px;&#125;.feed-link a &#123; color: rgb(255, 255, 255); border: 1px solid rgb(158, 158, 158) !important; border-radius: 15px;&#125;.feed-link a:hover &#123; background-color: rgb(161, 102, 171);&#125;.feed-link a i &#123; color: rgb(255, 255, 255);&#125;// 侧栏社交链接.links-of-author &#123; margin-top: 0px;&#125;// 侧栏友链标题.links-of-blogroll-title &#123; margin-bottom: 10px; margin-top: 15px; color: rgba(7, 179, 156, 0.74); margin-left: 6px; font-size: 15px; font-weight: bold;&#125;// 侧栏超链接样式（友链的样式）.sidebar a &#123; color: #ccc; border-bottom: none;&#125;.sidebar a:hover &#123; color: rgb(255, 255, 255);&#125;// 自定义的侧栏时间样式#days &#123; display: block; color: rgb(7, 179, 155); font-size: 13px; margin-top: 15px;&#125;// 侧栏目录链接样式.post-toc ol a &#123; color: rgb(75, 240, 215); border-bottom: 1px solid rgb(96, 125, 139);&#125;.post-toc ol a:hover &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;// 侧栏目录链接样式之当前目录.post-toc .nav .active &gt; a &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;.post-toc .nav .active &gt; a:hover &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;/* 修侧栏目录bug，如果主题配置文件_config.yml的toc是wrap: true */.post-toc ol &#123; padding: 0px 10px 5px 10px;&#125;/* 侧栏目录默认全展开，已注释.post-toc .nav .nav-child &#123; display: block;&#125;*/// 时间轴样式.posts-collapse &#123; margin: 50px 0px;&#125;@media (max-width: 1023px) &#123; .posts-collapse &#123; margin: 50px 20px; &#125;&#125;// 时间轴左边线条.posts-collapse::after &#123; margin-left: -2px; background-image: linear-gradient(180deg,#f79533 0,#f37055 15%,#ef4e7b 30%,#a166ab 44%,#5073b8 58%,#1098ad 72%,#07b39b 86%,#6dba82 100%);&#125;// 时间轴左边线条圆点颜色.posts-collapse .collection-title::before &#123; background-color: rgb(255, 255, 255);&#125;// 时间轴文章标题左边圆点颜色.posts-collapse .post-header:hover::before &#123; background-color: rgb(161, 102, 171);&#125;// 时间轴年份.posts-collapse .collection-title h1, .posts-collapse .collection-title h2 &#123; color: rgb(255, 255, 255);&#125;// 时间轴文章标题.posts-collapse .post-title a &#123; color: rgb(80, 115, 184);&#125;.posts-collapse .post-title a:hover &#123; color: rgb(161, 102, 171);&#125;// 时间轴文章标题底部虚线.posts-collapse .post-header:hover &#123; border-bottom-color: rgb(161, 102, 171);&#125;// archives页面顶部文字.page-archive .archive-page-counter &#123; color: rgb(255, 255, 255);&#125;// archives页面时间轴左边线条第一个圆点颜色.page-archive .posts-collapse .archive-move-on &#123; top: 10px; opacity: 1; background-color: rgb(255, 255, 255); box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5);&#125;// 分类页面.post-block.page &#123; margin-top: 40px;&#125;.category-all-page &#123; margin: -80px 50px 40px 50px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 86px 36px 36px 36px;&#125;@media (max-width: 767px) &#123; .category-all-page &#123; margin: -73px 15px 50px 15px; &#125; .category-all-page .category-all-title &#123; margin-top: -5px; &#125;&#125;// 标签云页面.tag-cloud &#123; margin: -80px 50px 40px 50px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 86px 36px 36px 36px;&#125;.tag-cloud-title &#123; margin-bottom: 15px;&#125;@media (max-width: 767px) &#123; .tag-cloud &#123; margin: -73px 15px 50px 15px; padding: 86px 5px 36px 5px; &#125;&#125;// 自定义的TopX页面样式#top &#123; display: block; text-align: center; margin: -100px 50px 40px 50px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 106px 36px 10px 36px;&#125;@media (max-width: 767px) &#123; #top &#123; margin: -93px 15px 50px 15px; padding: 96px 10px 0px 10px; &#125;&#125;// 自定义ABOUT页面的样式.about-page &#123; margin: -80px 0px 60px 0px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 106px 36px 36px 36px;&#125;@media (max-width: 767px) &#123; .about-page &#123; margin: -73px 0px 50px 0px; padding: 96px 15px 20px 15px; &#125;&#125;h2.about-title &#123; border-left: none !important; margin-left: 0px !important; padding-left: 0px !important; text-align: center; background-image: linear-gradient(90deg, #a166ab 0%, #a166ab 40%, #ef4e7b 45%, #f37055 50%, #ef4e7b 55%, #a166ab 60%, #a166ab 100%); background-size: cover; -webkit-background-clip: text; -webkit-text-fill-color: transparent; user-select: none;&#125;// 本地搜索框.local-search-popup .search-icon, .local-search-popup .popup-btn-close &#123; color: rgb(247, 149, 51); margin-top: 7px;&#125;.local-search-popup .local-search-input-wrapper input &#123; padding: 9px 0px; height: 21px; background-color: rgb(255, 255, 255);&#125;.local-search-popup .popup-btn-close &#123; border-left: none;&#125;// 选中文字部分的样式::selection &#123; background-color: rgb(255, 241, 89); color: #555;&#125;/* 设置滚动条的样式 *//* 参考https://segmentfault.com/a/1190000003708894 */::-webkit-scrollbar &#123; height: 5px;&#125;/* 滚动槽 */::-webkit-scrollbar-track &#123; background: #eee;&#125;/* 滚动条滑块 */::-webkit-scrollbar-thumb &#123; border-radius: 5px; background-color: #ccc;&#125;::-webkit-scrollbar-thumb:hover &#123; background-color: rgb(247, 149, 51);&#125;// 音乐播放器aplayer.aplayer &#123; font-family: Lato, -apple-system, BlinkMacSystemFont, "PingFang SC", "Hiragino Sans GB", "Heiti SC", STHeiti, "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", "Droid Sans Fallback", "Microsoft YaHei", sans-serif !important;&#125;.aplayer-withlrc.aplayer .aplayer-info &#123; background-color: rgb(255, 255, 255);&#125;// 音乐播放器aplayer歌单.aplayer .aplayer-list ol &#123; background-color: rgb(255, 255, 255);&#125;// 修视频播放器dplayer页面全屏的bug.use-motion .post-body &#123; transform: inherit !important;&#125;// 自定义emoji样式img#github-emoji &#123; margin: 0px; padding: 0px; display: inline !important; vertical-align: text-bottom; border: none; cursor: text; box-shadow: none;&#125;.site-meta .brand &#123; width: 10%;&#125;// 页面最顶部的横线.headband &#123; height: 1.5px; background-image: linear-gradient(90deg, #F79533 0%, #F37055 15%, #EF4E7B 30%, #A166AB 44%, #5073B8 58%, #1098AD 72%, #07B39B 86%, #6DBA82 100%);&#125; 打开网站缓冲条式特效打开\themes\next\layout\_partials\head\head.swig文件 在下面增加如下代码 123456789101112131415161718&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1&quot;/&gt;&lt;!-- S 新增代码 --&gt;&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;style&gt; .pace .pace-progress &#123; background: #24292e; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt;&lt;!-- E 新增代码 --&gt; 至此，网站已基本配置完成。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>教程</tag>
      </tags>
  </entry>
</search>
