<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2019%2F07%2F26%2Ftest%2F</url>
    <content type="text"><![CDATA[Welcome to this article, please enter password to read. Incorrect Password! No content to display! U2FsdGVkX1/HVmcmVIFuMVdQU3wVNu7I/+k3m06ugMbp/+M6YDg7zkQZm9WnODE9QsT36tQgrSpE/KteqGETZayyQeQl8QNPvf9kntKixxilzkfSg6j+V8JwA4K6ahZY0xypW4dNoRcPSWO2nURU6J5AMYv2AersK6o9qri7GA+zvOewW6hyqoqm5KzdRpqdLhgv4ar+vIAwgQ+I28ODzU7yigsdRRvkNUy8mYfJD587CP2wc4ly+Jf13zTjjDdhbjv/BUENUFrosrLH1Vcrc140pNiq/E1IUjKNnIbgOB5/UbuzVdpoAoRQj3g1lDqQHoDYOYtGbKYap48OaknyFPiBSd2u9NgSwL9xSCyhVZ2FdnrfH97LrFCE6+KCG8pxDZYN4GFcBS2BS7orIGs+5Zays/FY27uwmt2xmaFwYox3dzY9A8luvIPX756fxDTVsrbE0vTn+en2kpSi10HaSMAfGTTLRGUVfWOAJuT5arudV7n3edhh4RODSolqq059vPh9SHBvcjNTZfiWwPYfBcmIqgHRUakO6Tqi3C9f47nBiG2Us6Ae9LB8H3/gB/v8u97sOOKcBT4qM1nx3UXSLZ2cmWy2p3Vh5Fph2K+IaYyXxbjHEgNShntzOHO7Zz80awOoCCAfNPsepiw821fU+NMH7V6RPkf7XmWNB1IYaBcBVdD2S2pPIbWj1UXaaA0/b1kbiRM5qsnIXnGvtw8UDE4Y2UwbUEwREJgaHM/Qxtv7v11axIv7kfV7Q3bF6JXy2UJGJOSTVcWDyvKzPy/E6y6gkSLMLRIoLiwpHO0Of0s=]]></content>
      <categories>
        <category>test</category>
      </categories>
      <tags>
        <tag>test</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-8]]></title>
    <url>%2F2019%2F07%2F25%2FWebScraping8%2F</url>
    <content type="text"><![CDATA[到目前为止，在本书中，您已经忽略了格式糟糕的数据的问题，而是使用了一般格式良好的数据源，如果数据偏离了您的预期，则完全删除数据。但通常，在web抓取中，您不能对数据的来源或外观过于挑剔。由于标点符号错误、大小写不一致、换行符和拼写错误，脏数据在web上可能是一个大问题。本章将介绍一些工具和技术，通过改变编写代码的方式，以及在数据库中清理数据，帮助您从源头上预防问题。 Cleaning in Code在语言学中，n-gram是在文本或言语中使用的n个单词的序列。在进行自然语言分析时，通过查找常用的n-gram或经常一起使用的重复单词集，通常可以方便地分解一段文本。 本节的重点是获取格式正确的n-gram，而不是使用它们进行任何分析。稍后，在第9章中，您可以看到2-gram和3-gram在执行文本摘要和分析。下面返回了在Wikipedia关于Python编程语言的文章中找到的2克的列表: 12345678910111213141516from urllib.request import urlopenfrom bs4 import BeautifulSoupdef getNgrams(content, n): content = content.split(' ') output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputhtml = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')bs = BeautifulSoup(html, 'html.parser')content = bs.find('div', &#123;'id':'mw-content-text'&#125;).get_text()ngrams = getNgrams(content, 2)print(ngrams)print('2-grams count is: '+str(len(ngrams))) getNgrams函数接受一个输入字符串，并将其分解为一系列单词(假设所有单词都用空格分隔)，并将每个单词开头的n-gram(在本例中为2- gram)添加到数组中。 这将从文本中返回一些真正有趣和有用的2- gram： 1['of', 'free'], ['free', 'and'], ['and', 'open-source'], ['open-source', 'software'] 但它也会返回很多垃圾： 1['software\nOutline\nSPDX\n\n\n\n\n\n\n\n\nOperating', 'system\nfamilies\n\n\n\nAROS\nBSD\nDarwin\neCos\nFreeDOS\nGNU\nHaiku\nInferno\nLinux\nMach\nMINIX\nOpenSolaris\nPlan'], ['system\nfamilies\n\n\n\nAROS\nBSD\nDarwin\neCos\nFreeDOS\nGNU\nHaiku\nInferno\nLinux\nMach\nMINIX\nOpenSolaris\nPlan', '9\nReactOS\nTUD:OS\n\n\n\n\n\n\n\n\nDevelopment\n\n\n\nBasic'], ['9\nReactOS\nTUD:OS\n\n\n\n\n\n\n\n\nDevelopment\n\n\n\nBasic', 'For'] 此外，由于为遇到的每个单词都创建了2-gram(除了最后一个单词)，所以在撰写本文时，本文中有7,411个2-gram。不是一个非常容易管理的数据集！ 使用正则表达式删除转义字符(比如\n)和过滤删除任何Unicode字符，您可以稍微清理一下输出： 123456789101112131415161718import redef getNgrams(content, n): content = re.sub('\n|[[\d+\]]', ' ', content) content = bytes(content, 'UTF-8') content = content.decode('ascii', 'ignore') content = content.split(' ') content = [word for word in content if word != ''] output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputhtml = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')bs = BeautifulSoup(html, 'html.parser')content = bs.find('div', &#123;'id':'mw-content-text'&#125;).get_text()ngrams = getNgrams(content, 2)print(ngrams)print('2-grams count is: '+str(len(ngrams))) 这将用空格替换换行符的所有实例，删除诸如[123]之类的引用，并过滤行中多个空格导致的所有空字符串。然后，使用UTF-8编码内容，消除转义字符。 需要解释以下 1content = re.sub('\n|[[\d+\]]', ' ', content) 其中，\n|[[\d+\]]或前面是换行字符，后面的[[\d+\]]是说的是\[|\d+|\]这三个相或，为什么只有]前面加一个\这个是规定，表示对这个字符进行操作，但为什么[没有这样的讲究，只能说规定如此，而且我觉得[[\d+\]\n]也是等价的。 这些步骤大大提高了函数的输出，但仍存在一些问题： 1[&apos;years&apos;, &apos;ago(&apos;], [&apos;ago(&apos;, &apos;-&apos;], [&apos;-&apos;, &apos;-&apos;], [&apos;-&apos;, &apos;)&apos;], [&apos;)&apos;, &apos;Stable&apos;] 您可以通过删除每个单词前后的所有标点符号来改进这一点标点符号(剥离)。这保留了单词中的连字符，但是消除了空字符串之后只包含一个标点符号的字符串。 当然，标点符号本身是有意义的，简单地去掉它可能会丢失一些有价值的信息。例如，句点后面的空格可以被认为是一个完整句子或语句的结尾。您可能想要禁止n-gram在这样的stop上桥接，并且只考虑在句子中创建的那些。 例如，给定文本： 1Python features a dynamic type system and automatic memory management. It supports multiple programming paradigms... 2-gram[‘memory’, ‘management’]是有效的，但是2-gram[‘management’, ‘It’]无效。 现在，您有了一个更长的“清洁任务”列表，您正在介绍“清洁任务”的概念“句子”，你的程序已经变得越来越复杂，最好把它们移到四个不同的函数中: 12345678910111213141516171819202122232425262728293031323334353637from urllib.request import urlopenfrom bs4 import BeautifulSoupimport reimport stringdef cleanSentence(sentence): sentence = sentence.split(' ') sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence] sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == 'a' or word.lower() == 'i')] return sentencedef cleanInput(content): content = content.upper() content = re.sub('\n|[[\d+\]]', ' ', content) content = bytes(content, "UTF-8") content = content.decode("ascii", "ignore") sentences = content.split('. ') return [cleanSentence(sentence) for sentence in sentences]def getNgramsFromSentence(content, n): output = [] for i in range(len(content)-n+1): output.append(content[i:i+n]) return outputdef getNgrams(content, n): content = cleanInput(content) ngrams = [] for sentence in content: ngrams.extend(getNgramsFromSentence(sentence, n)) return(ngrams)html = urlopen('http://en.wikipedia.org/wiki/Python_(programming_language)')bs = BeautifulSoup(html, 'html.parser')content = bs.find('div', &#123;'id':'mw-content-text'&#125;).get_text()print(getNgrams(content, 2))print(len(getNgrams(content, 2))) 其中，.strip()，是只能删除开头或是结尾的字符，不能删除中间部分的字符。 getNgrams仍然是您进入程序的基本入口点。cleanInput和前面一样删除了换行和引用，但也根据句点和空格的位置将文本分割为“句子”。它还调用了cleanSentence，它将句子分成单词，去掉标点符号和空格，并删除除I和a之外的单个字符。 创建n个gram的关键行被移动到getNgramsFromSentence中，getngram对每个句子调用这个函数。这确保不会创建跨多个句子的n-gram。注意string.punctuation和string.whitespace的使用。获取Python中所有标点符号的列表。您可以查看字符串的输出。Python终端中的标点符号: 123&gt;&gt;&gt; import string&gt;&gt;&gt; print(string.punctuation)!"#$%&amp;'()*+,-./:;&lt;=&gt;?@[\]^_`&#123;|&#125;~ 通过在循环中遍历内容中的所有单词时使用item.strip(string. +string.whitespace)，单词两边的任何标点符号都将被删除，而连字符(标点符号被两边的字母包围)将保持不变。 这种努力的结果是更干净的2-gram： 1[['Python', 'Paradigm'], ['Paradigm', 'Object-oriented'], ['Object-oriented','imperative'], ['imperative', 'functional'], ['functional', 'procedural'],['procedural', 'reflective'],... Data Normalization每个人都遇到过设计糟糕的web表单:“输入您的电话号码。您的电话号码必须是‘xxx- xxxx -xxxx’格式。” 作为一名优秀的程序员，您可能会想，为什么他们不去掉我输入的非数字字符，然后自己来做呢?数据规范化是确保在语言或逻辑上彼此等价的字符串(如电话号码(555)123-4567和555.123.4567)显示为等价字符串，或至少将其进行比较的过程。 使用上一节中的n-gram代码，您可以添加数据规范化特性。这段代码的一个明显问题是它包含许多重复的2-gram。它每遇到2-gram就会被添加到列表中，没有记录它的频率。记录这些2-gram的频率(而不仅仅是它们的存在)不仅很有趣，而且在绘制清洗和数据归一化算法更改的效果时也很有用。如果数据被成功归一化，那么唯一n-gram的总数将减少，而找到的n-gram总数(即，确定为n—gram的唯一或非唯一项的数目)不会减少。换句话说，对于相同数量的n克，“木桶”会更少。 您可以通过修改收集n个g的代码来实现这一点，将它们添加到计数器对象而不是列表中： 123456789101112from collections import Counterdef getNgrams(content, n): content = cleanInput(content) ngrams = Counter() ngrams_list = [] for sentence in content: newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)] ngrams_list.extend(newNgrams) ngrams.update(newNgrams) return(ngrams)print(getNgrams(content, 2)) 还有许多其他方法可以做到这一点，比如向dictionary对象添加n-grams，其中list的值按其被看到的次数计数。这有一个缺点，它需要更多的管理，并使排序变得棘手。但是，使用计数器对象也有一个缺点：它不能存储列表(列表是不可缓存的)，所以您需要首先在每个n-gram的列表理解中使用’ ‘.join(n-gram)将它们转换为字符串。 这里是结果： 1Counter(&#123;&apos;Python Software&apos;: 37, &apos;Software Foundation&apos;: 37, &apos;of the&apos;: 34,&apos;of Python&apos;: 28, &apos;in Python&apos;: 24, &apos;in the&apos;: 23, &apos;van Rossum&apos;: 20, &apos;to the&apos;:20, &apos;such as&apos;: 19, &apos;Retrieved February&apos;: 19, &apos;is a&apos;: 16, &apos;from the&apos;: 16,&apos;Python Enhancement&apos;: 15,... 在撰写本文时，总共有7275个2克，5628个独特的2克，其中最流行的2克是“软件基础”，其次是“Python Software.””。然而，对结果的分析表明，“Python Software.””以“Python软件”另外两次。同样的，” vanRossum “和” vanRossum”单独出现在列表中。 添加一行: 1content = content.upper() 对于cleanInput函数，保持找到的2-gram的总数稳定在7,275，同时将惟一的2-gram的数量减少到5,479。 除此之外，通常最好停下来，考虑一下需要花费多少计算能力来规范化数据。在许多情况下，单词的不同拼写是等价的，但是为了解决这种等价性，您需要检查每个单词，看看它是否匹配任何预编程的等价。 例如，Python first和Python 1st都出现在2-gram的列表中。然而，如果要制定一个总括规则，即all first、second、third等等都将被解析为1st、2nd、3rd等等(反之亦然)，那么每个单词将会被额外检查10次左右。同样，不一致地使用连字符(coordination对co-ordination)、拼写错误和其他自然语言的不一致也会影响n-gram的分组，如果这些不一致足够常见，可能会混淆输出的结果。对于带连字符的单词，一种解决方案可能是完全删除连字符，将单词视为一个字符串，这只需要一个操作。然而，这也意味着连字符短语(非常常见的现象)将被视为一个单词。走另一条路，将连字符作为空格可能是更好的选择。只是为偶尔的配合和配合攻击做好准备。 Cleaning After the Fact在代码中，您只能(或想)做这么多。此外，您可能正在处理一个没有创建的数据集，或者一个甚至在没有先查看数据集的情况下就知道如何清理的数据集。许多程序员在这种情况下的下意识反应是编写一个脚本，这可能是一个很好的解决方案。然而，第三方工具，如OpenRefine，不仅能够快速轻松地清理数据，而且允许非程序员轻松地查看和使用数据。 OpenRefineOpenRefine是一个开源项目，由Metaweb公司于2009年启动。谷歌于2010年收购Metaweb，将项目名称从Freebase Gridworks改为谷歌Refine。2012年，谷歌放弃了对Refine的支持，再次将名称更改为OpenRefine，欢迎任何人为项目的开发做出贡献。 要使用OpenRefine，需要将数据保存为CSV文件，或者，如果您将数据存储在数据库中，则可以将其导出到CSV文件。 Using OpenRefine在下面的例子中，您将使用从Wikipedia的“比较”中提取的数据文本编辑器”表;见图8 - 1。虽然这个表的格式相对较好，但是它包含了人们在很长一段时间内进行的许多编辑，因此它有一些格式上的小矛盾。此外，由于它的数据是由人而不是机器读取的，所以一些格式选择(例如，使用“Free”而不是“$0.00”)不适合编程输入。 关于OpenRefine要注意的第一件事是，每个列标签旁边都有一个箭头。此箭头提供了一个工具菜单，可与该列一起用于筛选、排序、转换或删除数据。 Filtering 数据过滤可以使用两种方法执行:Filters和facets。过滤器适用于使用正则表达式对数据进行过滤;例如，在“编程语言”列中，只显示包含三种或更多逗号分隔的编程语言的数据，如图8-2所示。可以通过操作右栏中的块轻松地组合、编辑和添加过滤器。它们还可以与facets组合。 可以通过操作右栏中的块轻松地组合、编辑和添加过滤器。它们还可以与facet组合。 facest非常适合包含或排除基于列的整个内容的数据。(例如，“显示所有使用GPL或MIT许可的行，这些行在2005年后首次发布”，如图8-3所示)。他们有内置的过滤工具。例如，对数值进行筛选可以使用幻灯片条选择要包含的值范围。 无论您如何过滤数据，它都可以在任何时候导出到OpenRefine支持的几种格式之一。这包括CSV, HTML(一个HTML表)，Excel和其他几种格式。 cleaning 只有在开始时数据相对干净时，才能成功地进行数据过滤。例如，在前一节的facet示例中，“First public release”facet中不会选择发布日期为01-01-2006的文本编辑器，该文本编辑器正在寻找一个值为2006的值，并忽略了看起来不像这个值的值。 使用OpenRefine表达式在OpenRefine中执行数据转换语言，称为GREL (G是OpenRefine的前一个名称遗留下来的，谷歌优化)。此语言用于创建基于简单规则转换单元格中的值的短lambda函数。例如: 1if(value.length() != 4, &quot;invalid&quot;, value) 当这个函数应用于“第一个稳定版本”列时，它将保存日期为YYYY格式的单元格的值，并将所有其他列标记为无效(图8-4)。 可以通过单击任何列标签旁边的下箭头并选择Edit cells$\rightarrow$Transform来应用任意的GREL语句。然而，将所有不太理想的值标记为无效，同时使它们易于发现，这对您没有多大好处。如果可能，最好尝试从格式糟糕的值中回收信息。这可以通过使用GREL的match函数来实现： 1value.match(&quot;.*([0-9]&#123;4&#125;).*&quot;).get(0) 这将尝试将字符串值与给定的正则表达式匹配。如果正则表达式与字符串匹配，则返回一个数组。任何与正则表达式中的捕获组匹配的子字符串(由表达式中的括号分隔，在本例中为[0-9]{4})都作为数组值返回。实际上，这段代码将在一行中找到四个小数的所有实例，并返回第一个实例。这通常足以从文本或格式糟糕的日期中提取年份。它还具有为不存在的日期返回null的优点。(在对空变量执行操作时，GREL不会抛出空指针异常。)使用单元格编辑和GREL可以进行许多其他数据转换。在OpenRefine的GitHub页面上可以找到该语言的完整指南。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-7]]></title>
    <url>%2F2019%2F07%2F24%2FWebScraping7%2F</url>
    <content type="text"><![CDATA[本章将介绍如何处理文档，无论您是将它们下载到本地文件夹，还是阅读它们并提取数据。您还将了解如何处理各种类型的文本编码，这甚至可以使阅读外语成为可能HTML页面。 人们很容易认为互联网主要是一个基于文本的网站集合，其中点缀着新颖的web 2.0多媒体内容，这些内容在web抓取的目的中基本上可以忽略。然而，这忽略了internet最基本的特性:传输文件的内容无关工具。 尽管互联网自20世纪60年代末以来就以某种形式存在，HTML直到1992年才首次出现。在那之前，互联网主要由电子邮件和文件传输组成;我们今天所知道的web页面的概念并不存在。换句话说，internet不是HTML文件的集合。它是许多类型的文档的集合，HTML文件通常用作展示它们的框架。由于无法读取各种文档类型，包括文本、PDF、图像、视频、电子邮件等，我们将丢失大量可用数据。 Document Encoding文档的编码告诉应用程序——无论它们是您计算机的操作系统还是您自己的Python代码——如何读取它。这种编码通常可以从它的文件扩展名推断出来，尽管这个文件扩展名不是由它的编码强制的。例如，我可以毫无问题地将myImage.jpg保存为myImage.txt—至少在我的文本编辑器尝试打开它之前是这样。幸运的是，这种情况很少见，为了正确读取文档，通常只需要知道文档的文件扩展名。 在基本级别上，所有文档都是用0和1编码的。最重要的是，编码算法定义了诸如“每个字符有多少位”或“每个像素有多少位代表颜色”之类的东西(对于图像文件)。除此之外，您可能还有一层压缩，或者一些空间缩减算法，就像PNG文件的情况一样。 尽管一开始处理非html文件可能看起来有些吓人，但是请放心，使用正确的库，Python将能够正确地处理任何格式的信息。文本文件、视频文件和图像文件之间的唯一区别是如何解释它们的0和1。本章介绍几种常见的文件类型:文本、CSV、pdf和Word文档。 注意，这些基本上都是存储文本的文件。有关处理图像的信息，我建议您通读本章，以便习惯处理和存储不同类型的文件，然后阅读第13章，了解关于图像处理的更多信息! Text将文件以纯文本形式存储在网上有些不同寻常，但在一些基本的网站或老式网站中，拥有大型文本文件存储库很受欢迎。例如，Internet Engineering Task Force (IETF)将其所有已发布的文档存储为HTML、PDF和文本文件(参见https://www.ietf.org/rfc/rfc1149.txt)。大多数浏览器都会很好地显示这些文本文件，您应该能够毫无问题地抓取它们。 1234from bs4 import BeautifulSoupfrom urllib.request import urlopentextPage = urlopen('http://www.pythonscraping.com/pages/warandpeace/chapter1.txt')print(textPage.read()) 通常，当您使用urlopen检索页面时，您将它转换为一个BeautifulSoup对象来解析HTML。在这种情况下，您可以直接读取页面。虽然完全有可能将其转换为一个BeautifulSoup对象，但这只会适得其反——因为没有HTML要解析，所以库将毫无用处。一旦文本文件以字符串的形式读入，您只需像将任何其他字符串读入Python那样分析它。当然，这里的缺点是您没有能力使用HTML标记作为上下文线索，指向实际文本的方向 Text Encoding and the Global Internet还记得我之前说过，正确读取文件只需要一个文件扩展名吗？奇怪的是，这个规则并不适用于所有最基本的文档:.txt文件。10次中有9次，使用前面描述的方法阅读文本会很好。然而，处理互联网上的文本可能是一件棘手的事情。接下来，我们将介绍英语和外语编码的基础知识，从ASCII到Unicode到ISO，以及如何处理它们。 a history of text encoding ASCII最早出现在20世纪60年代，当时比特非常昂贵，除了拉丁字母和一些标点符号外，没有理由对任何东西进行编码。由于这个原因，总共只有7位被用于编码128个大写字母、小写字母和标点符号。即使有这么多的创意，他们仍然留下了33个非打印字符，其中一些被使用，取代，和/或成为过时的技术变化多年。每个人都有足够的空间，对吧? 任何程序员都知道，7是一个奇怪的数字。它不是一个很好的2次方，但是它非常接近。在20世纪60年代，计算机科学家们就是否应该增加额外的位进行了争论，到底是为了方便得到一个漂亮的整数，还是为了实际的文件需要更少的存储空间。最后，7位赢了。然而，在现代计算中，每个7位序列的开头都加了一个额外的0，这就给我们留下了两个世界中最糟糕的情况——14%的文件变大了，而且只缺少128个字符的灵活性。 在20世纪90年代初，人们意识到存在着比英语更多的语言，如果电脑能显示这些语言那就太好了。一个名为Unicode联盟试图通过为任何文本文档、任何语言中需要使用的每个字符建立编码来实现一个通用的文本编码器。目标是包括从拉丁字母编写的这本书是,西里尔,中国象形图,数学和逻辑符号，甚至表情符号和各种各样的符号,如生物危害和标志。 您可能已经知道，生成的编码器被命名为UTF-8，它的意思是“通用字符集转换格式8位”。这里的8位并不是指每个字符的大小，而是指一个字符需要显示的最小大小。 UTF-8字符的实际大小是灵活的。它们的范围从1字节到4字节，这取决于它们在可能的字符列表中的位置(更流行的字符用更少的字节编码，更不常见的字符需要更多的字节)。 如何实现这种灵活的编码？使用7位加上最终无用的前导0起初看起来像是ASCII中的一个设计缺陷，但事实证明UTF-8具有巨大的优势。由于ASCII非常流行，Unicode决定利用这个领先的0位，以0开头声明所有字节，以表示字符中只使用一个字节，并使ASCII和UTF-8的两种编码方案相同。因此，以下字符在UTF-8和ASCII中都是有效的。 12301000001 - A01000010 - B01000011 - C 以下字符仅在UTF-8中有效，如果将文档解释为ASCII文档，则将呈现为不可打印： 除了UTF-8之外，还存在其他UTF标准，如UTF-16、UTF-24和UTF-32，尽管以这些格式编码的文档很少出现，除非在不寻常的情况下，这超出了本书的范围。 虽然ASCII的这个原始设计缺陷对UTF-8有一个主要的优势，但是这个劣势并没有完全消失。每个字符的前8位信息仍然只能编码128($2^7$)个字符，而不能编码完整的256个字符。在需要多个字节的UTF-8字符中，额外的前导位不是用于字符编码，而是用于防止损坏的检查位。在4字节字符中的32位(8 $\times$ 4)中，只有21位用于字符编码，总共有2,097,152个可能的字符，其中当前分配了1,114,112个。当然，所有通用语言编码标准的问题在于，任何用一种外语编写的文档都可能比必须的大得多。虽然您的语言可能只有100个左右的字符，但是每个字符需要16位，而不是像特定于英语的ASCII那样只需要8位。这使得UTF-8中的外语文本文档的大小大约是英语文本文档的两倍，至少对于不使用拉丁字符集的外语来说是这样。 ISO通过为每种语言创建特定的编码来解决这个问题。与Unicode一样，它具有与ASCII相同的编码，但是在每个字符的开头使用填充0位，以便为所有需要它们的语言创建128个特殊字符。这对于严重依赖拉丁字母(在编码中仍处于0 127的位置)，但需要额外特殊字符的欧洲语言最有效。这允许ISO-8859-1**** Encodings in action 在上一节中，您使用了urlopen的默认设置来读取可能在internet上遇到的文本文档。这对大多数英语文本都很有用。然而，当你遇到俄语、阿拉伯语，甚至像“resume”这样的单词时，你可能会遇到问题。 以下面的代码为例: 123from urllib.request import urlopentextPage = urlopen('http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt')print(textPage.read()) 这读在第一章的原始战争与和平(写在俄罗斯和并将其打印到屏幕上。这个屏幕文本的部分内容如下: 1b&quot;\xd0\xa7\xd0\x90\xd0\xa1\xd0\xa2\xd0\xac \xd0\x9f\xd0\x95\xd0\xa0\xd0\x92\xd0\x90\xd0\xaf\n\nI\n\n\xe2\x80\x94 Eh bien, mon prince. 此外，在大多数浏览器中访问这个页面会导致胡言乱语。 即使对母语为俄语的人来说，这也可能有点难以理解。问题是Python试图将文档读取为ASCII文档，而浏览器则试图将其读取为ISO-8859-1编码的文档。当然，双方都没有意识到这是一个UTF-8文档。您可以显式地将字符串定义为UTF-8，它可以正确地将输出格式化为Cyrillic字符: 12345from urllib.request import urlopentextPage = urlopen( 'http://www.pythonscraping.com/pages/warandpeace/chapter1-ru.txt')print(str(textPage.read(), 'utf-8')) 在BeautifulSoup和Python 3.x中使用这个概念是这样的: 1234567from urllib.request import urlopenhtml = urlopen("http://en.wikipedia.org/wiki/Python_(programming_language)")bs = BeautifulSoup(html, "html.parser")content = bs.find("div", &#123;"id":"mw-content-text"&#125;).get_text()content = bytes(content, "UTF-8")content = content.decode("UTF-8")print(content) Python 3.默认情况下，将所有字符编码为UTF-8。您可能会忍不住不去管它，而是为您编写的每个web scraper使用UTF-8编码。毕竟，UTF-8还可以流畅地处理ASCII字符和外语。然而，重要的是要记住9%的网站使用的是ISO编码也一样，所以你永远无法完全避免这个问题。 幸运的是，对于HTML页面，编码通常包含在站点的部分中找到的标记中。大多数网站，尤其是英语网站，都有这样的标签： 1&lt;meta charset="utf-8" /&gt; 而ECMA国际的网站上有这个标签： 1&lt;META HTTP-EQUIV="Content-Type" CONTENT="text/html; charset=iso-8859-1"&gt; 如果您计划进行大量的web抓取，特别是国际站点，那么在阅读页面内容时，最好查找这个元标记并使用它推荐的编码。 CSV当web抓取时，您可能会遇到CSV文件或喜欢这种格式的数据的同事。幸运的是，Python有一个非常棒的库，可以读取和写入CSV文件。尽管这个库能够处理CSV的许多变体，但本节主要关注标准格式。如果您有特殊情况需要处理，请参考文档! Reading CSV Files Python的csv库主要用于处理本地文件，假设您需要的csv数据存储在您的机器上。不幸的是，情况并非总是如此，尤其是在web抓取时。有几种方法可以解决这个问题: 手工下载文件并将Python指向本地文件位置 编写一个Python脚本来下载文件、读取文件，并(可选地)在检索后删除文件。 从web中以字符串的形式检索该文件，并将该字符串包装在StringIO对象中，这样它的行为就像一个文件。 虽然前两个选项是可行的，但是当您可以轻松地将文件保存在内存中时，占用硬盘空间是一种不好的做法。更好的方法是将文件读入字符串并将其封装在一个对象中，该对象允许Python将其视为文件，而无需保存文件。下面的脚本从internet检索CSV文件(在本例中，Monty Python相册列表位于http://pythonscraping.com/files/Monty‐PythonAlbums.csv然后一行一行地打印到终端: 1234567891011from urllib.request import urlopenfrom io import StringIOimport csvdata = urlopen('http://pythonscraping.com/files/MontyPythonAlbums.csv').read().decode('ascii', 'ignore')dataFile = StringIO(data)csvReader = csv.reader(dataFile)for row in csvReader: print(row) print("The album \""+row[0]+"\" was released in "+str(row[1])) 输出类似如下： 12345678[&apos;Name&apos;, &apos;Year&apos;]The album &quot;Name&quot; was released in Year[&quot;Monty Python&apos;s Flying Circus&quot;, &apos;1970&apos;]The album &quot;Monty Python&apos;s Flying Circus&quot; was released in 1970[&apos;Another Monty Python Record&apos;, &apos;1971&apos;]The album &quot;Another Monty Python Record&quot; was released in 1971[&quot;Monty Python&apos;s Previous Record&quot;, &apos;1972&apos;]... 注意第一行:专辑“Name”是在那年发行的。虽然在编写示例代码时，这可能是一个容易忽略的结果，但您不希望在现实世界中将其放入数据中。级别较低的程序员可能只是跳过csvReader对象中的第一行，或者用特殊的情况编写来处理它。幸运的是，csv的替代品。reader函数会自动为您处理所有这些。输入DictReader: 123456789101112from urllib.request import urlopenfrom io import StringIOimport csvdata = urlopen("http://pythonscraping.com/files/MontyPythonAlbums.csv").read().decode('ascii', 'ignore')dataFile = StringIO(data)dictReader = csv.DictReader(dataFile)print(dictReader.fieldnames)for row in dictReader: print(row) 输出类似如下： 1234567[&apos;Name&apos;, &apos;Year&apos;]OrderedDict([(&apos;Name&apos;, &quot;Monty Python&apos;s Flying Circus&quot;), (&apos;Year&apos;, &apos;1970&apos;)])OrderedDict([(&apos;Name&apos;, &apos;Another Monty Python Record&apos;), (&apos;Year&apos;, &apos;1971&apos;)])OrderedDict([(&apos;Name&apos;, &quot;Monty Python&apos;s Previous Record&quot;), (&apos;Year&apos;, &apos;1972&apos;)])OrderedDict([(&apos;Name&apos;, &apos;The Monty Python Matching Tie and Handkerchief&apos;), (&apos;Year&apos;, &apos;1973&apos;)])OrderedDict([(&apos;Name&apos;, &apos;Monty Python Live at Drury Lane&apos;), (&apos;Year&apos;, &apos;1974&apos;)])... csv.DictReader返回CSV文件中每一行的值作为dictionary对象而不是list对象，字段名存储在变量DictReader中。字段名和键在每个dictionary对象: 当然，与csvReader相比，它的缺点是创建、处理和打印这些DictReader对象所需的时间稍微长一些，但是其方便性和可用性通常值得额外的开销。也要记住,当涉及到web抓取请求所需的开销和检索网站数据从外部服务器几乎总是会不可避免的限制因素在任何程序编写,所以担心技术可能刮微秒你总运行时通常是一个有争议的问题！ PDF作为一名Linux用户，我知道被发送一个.docx文件的痛苦，我的非微软软件把这个文件弄得一团糟，而且我还在努力寻找代码编解码器来解释一些新的苹果媒体格式。在某种程度上，Adobe在1993年创建其可移植文档格式方面是革命性的。pdf允许不同平台上的用户以完全相同的方式查看图像和文本文档，而不管他们是在哪个平台上查看的。尽管在web上存储pdf有点过时(当您可以将其编写为HTML时，为什么要以静态、慢加载的格式存储内容?)，但是pdf仍然无处不在，尤其是在处理官方表单和归档时。2009年，一位名叫尼克·英尼斯(Nick Innes)的英国人因为要求白金汉郡市议会提供公开的学生考试成绩信息而上了新闻。根据英国版的《信息自由法》(Freedom of information Act)，白金汉郡市议会可以获得学生的考试成绩信息。在多次请求和拒绝之后，他终于收到了184份PDF文件，这是他要找的资料。尽管Innes坚持了下来，并最终获得了一个格式更合适的数据库，但如果他是一个专业的web scraper，他很可能会在法庭上节省很多时间，直接使用PDF文档，使用Python的许多PDF解析模块之一。 不幸的是，许多pdf解析库都是为python2构建的。没有随着Python 3.x的发布而升级。不过，因为PDF是相对而言的简单和开放源码的文档格式，许多体面的Python库，甚至在Python 3.x，可以读出来。 PDFMiner3K就是这样一个相对容易使用的库。它是灵活的，允许命令行使用或集成到现有代码中。它还可以处理各种语言编码——同样，这在web上经常派上用场。 这里是一个基本的实现，允许你读取任意的pdf到一个字符串，给定一个本地文件对象： 这个PDF阅读器的好处是，如果你在本地处理文件，你可以用一个普通的Python文件对象替换urlopen返回的文件对象，并使用下面这行代码: 1pdfFile = open('../pages/warandpeace/chapter1.pdf', 'rb') 输出可能并不完美，特别是对于带有图像、格式奇怪的文本或表或图表中的文本的pdf。但是，对于大多数纯文本PDF，输出应该与PDF是文本文件时的输出没有什么不同。 Microsoft Word and .docx冒着冒犯微软朋友的风险:我不喜欢微软Word。不是因为它一定是一个坏软件，而是因为它的用户滥用它的方式。它有一种特殊的天赋，可以将原本应该是简单的文本文档或pdf文件转换成大型、缓慢、难以打开的文件，这些文件在不同机器之间常常会失去所有格式，而且由于某种原因，当内容通常是静态的时候，这些文件是可以编辑的。Word文件是为内容创建而设计的，而不是为内容共享而设计的。然而，它们在某些网站上是无处不在的，包括重要的文件、信息，甚至图表和多媒体;简而言之，所有可以而且应该用HTML创建的东西。大约在2008年以前，Microsoft Office产品使用专有的.doc文件格式。这种二进制文件格式很难阅读，而且其他文字处理程序也不支持这种格式。为了顺应时代潮流，采用许多其他软件都使用的标准，微软决定使用基于xml的Open Office标准，该标准使文件与开源和其他软件兼容。不幸的是，Python对这种文件格式的支持(谷歌Docs、Open Office和Microsoft Office都使用这种格式)仍然不是很好。有python-docx库，但这只允许用户创建文档并只读取基本的文件数据，比如文件的大小和标题，而不是实际内容。要读取Microsoft Office文件的内容，您需要滚动自己的解决方案。 第一步是从文件中读取XML: 123456789101112131415from zipfile import ZipFilefrom urllib.request import urlopenfrom io import BytesIOfrom bs4 import BeautifulSoupwordFile = urlopen('http://pythonscraping.com/pages/AWordDocument.docx').read()wordFile = BytesIO(wordFile)document = ZipFile(wordFile)xml_content = document.read('word/document.xml')wordObj = BeautifulSoup(xml_content.decode('utf-8'), 'xml')textStrings = wordObj.find_all('w:t')for textElem in textStrings: print(textElem.text) 它将远程Word文档读取为二进制文件对象(BytesIO类似于使用Python的核心zipfile库解压缩它(所有.docx文件都被压缩以节省空间)，然后读取未压缩的文件XML。 读取我的简单Word文档的Python脚本输出如下: 1234567891011121314151617181920212223242526272829303132333435&lt;!--?xml version="1.0" encoding="UTF-8" standalone="yes"?--&gt;&lt;w:document mc:ignorable="w14 w15 wp14" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math" xmlns:mc="http://schemas.openxmlformats.org/markup-compatibility/2006" xmlns:o="urn:schemas-microsoft-com:office:office" xmlns:r="http://schemas.openxmlformats.org/officeDocument/2006/relationships" xmlns:v="urn:schemas-microsoft-com:vml" xmlns:w="http://schemas.openxmlformats.org/wordprocessingml/2006/main" xmlns:w10="urn:schemas-microsoft-com:office:word" xmlns:w14="http://schemas.microsoft.com/office/word/2010/wordml" xmlns:w15="http://schemas.microsoft.com/office/word/2012/wordml" xmlns:wne="http://schemas.microsoft.com/office/word/2006/wordml" xmlns:wp="http://schemas.openxmlformats.org/drawingml/2006/wordprocessingDrawing" xmlns:wp14="http://schemas.microsoft.com/office/word/2010/wordprocessingDrawing" xmlns:wpc="http://schemas.microsoft.com/office/word/2010/wordprocessingCanvas" xmlns:wpg="http://schemas.microsoft.com/office/word/2010/wordprocessingGroup" xmlns:wpi="http://schemas.microsoft.com/office/word/2010/wordprocessingInk" xmlns:wps="http://schemas.microsoft.com/office/word/2010/wordprocessingShape"&gt;&lt;w:body&gt;&lt;w:p w:rsidp="00764658" w:rsidr="00764658" w:rsidrdefault="00764658"&gt;&lt;w:ppr&gt;&lt;w:pstyle w:val="Title"&gt;&lt;/w:pstyle&gt;&lt;/w:ppr&gt;&lt;w:r&gt;&lt;w:t&gt;A Word Document on a Website&lt;/w:t&gt;&lt;/w:r&gt;&lt;w:bookmarkstart w:id="0" w:name="_GoBack"&gt;&lt;/w:bookmarkstart&gt;&lt;w:bookmarkend w:id="0"&gt;&lt;/w:bookmarkend&gt;&lt;/w:p&gt;&lt;w:p w:rsidp="00764658" w:rsidr="00764658" w:rsidrdefault="00764658"&gt;&lt;/w:p&gt;&lt;w:p w:rsidp="00764658" w:rsidr="00764658" w:rsidrdefault="00764658" w:rsidrpr="00764658"&gt;&lt;w: r&gt; &lt;w:t&gt;This is a Word document, full of content that you want very much. Unfortunately, it’s difficult to access because I’m puttingit on my website as a .&lt;/w:t&gt;&lt;/w:r&gt;&lt;w:prooferr w:type="spellStart"&gt;&lt;/w:prooferr&gt;&lt;w:r&gt;&lt;w:t&gt;docx&lt;/w:t&gt;&lt;/w:r&gt;&lt;w:prooferr w:type="spellEnd"&gt;&lt;/w:prooferr&gt; &lt;w:r&gt; &lt;w:t xml:space="preserve"&gt; file, rather than just publishing it as HTML&lt;/w:t&gt; &lt;/w:r&gt; &lt;/w:p&gt; &lt;w:sectpr w:rsidr="00764658"w:rsidrpr="00764658"&gt; &lt;w:pgszw:h="15840" w:w="12240"&gt;&lt;/w:pgsz&gt;&lt;w:pgmar w:bottom="1440" w:footer="720" w:gutter="0" w:header="720" w:left="1440" w:right="1440" w:top="1440"&gt;&lt;/w:pgmar&gt; &lt;w:cols w:space="720"&gt;&lt;/w:cols&amp;g; &lt;w:docgrid w:linepitch="360"&gt;&lt;/w:docgrid&gt; &lt;/w:sectpr&gt; &lt;/w:body&gt; &lt;/w:document&gt; 这里显然有很多元数据，但是您想要的实际文本内容被隐藏了。幸运的是，文档中的所有文本，包括顶部的标题，都包含在w:t标签中，便于抓取: 123456789101112131415from zipfile import ZipFilefrom urllib.request import urlopenfrom io import BytesIOfrom bs4 import BeautifulSoupwordFile = urlopen('http://pythonscraping.com/pages/AWordDocument.docx').read()wordFile = BytesIO(wordFile)document = ZipFile(wordFile)xml_content = document.read('word/document.xml')wordObj = BeautifulSoup(xml_content.decode('utf-8'), 'xml')textStrings = wordObj.find_all('w:t')for textElem in textStrings: print(textElem.text) 注意这里不是html。解析器解析器通常与BeautifulSoup一起使用，您将把xml解析器传递给它。这是因为冒号在HTML标记名中是不标准的，比如w:t和HTML。解析器不能识别它们。输出并不完美，但它已经达到了这个目标，将每个w:t标记打印在新行上可以很容易地看到单词是如何分割文本的 12345A Word Document on a WebsiteThis is a Word document, full of content that you want very much. Unfortunately,it’s difficult to access because I’m putting it on my website as a .docxfile, rather than just publishing it as HTML 请注意，单词“docx”位于它自己的行上。在原始XML中，它被标记&lt;w:proofErrw:type=&quot;spellStart&quot;/&gt;包围。这是Word的突出显示方式“docx”下划线为红色，表示它认为自己的文件格式的名称存在拼写错误。 文档的标题前面有样式描述符标签&lt;w:pstyle w:val=&quot; title &quot;&gt;。尽管这并不能让我们非常容易地识别标题(或其他样式的文本)，使用BeautifulSoup的导航功能可以很有用: 12345678textStrings = wordObj.find_all('w:t')for textElem in textStrings: style = textElem.parent.parent.find('w:pStyle') if style is not None and style['w:val'] == 'Title': print('Title is: &#123;&#125;'.format(textElem.text)) else: print(textElem.text) 可以很容易地扩展此函数，以便围绕各种文本样式打印标记，或者以其他方式标记它们。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-6]]></title>
    <url>%2F2019%2F07%2F23%2FWebScraping6%2F</url>
    <content type="text"><![CDATA[尽管将数据打印到终端非常有趣，但在数据聚合和分析方面，它并不是非常有用。要使大多数远程有用，您需要能够保存它们所抓取的信息。本章介绍了三种主要的数据管理方法，这些方法几乎适用于任何可以想象的应用程序。您是否需要为网站的后端供电或创建自己的API?您可能希望写入数据库。需要一个快速和简单的方法来收集文件从互联网上，并把他们放在您的硬盘驱动器?您可能想为此创建一个文件流。需要偶尔的提醒，或者每天一次聚合数据?给自己发一封电子邮件!除了web抓取之外，存储和与大量数据交互的能力对于任何现代编程应用程序都非常重要。事实上，本章中的信息对于实现本书后面章节中的许多示例是必要的。如果您不熟悉自动数据存储，我强烈建议您至少浏览一下这一章。 Media Files您可以通过两种主要方式存储媒体文件:引用和下载文件本身。通过存储文件所在的URL，可以通过引用存储文件。这有几个优点： 运行得更快，当他们不需要下载文件时，需要更少的带宽。 通过只存储url，您可以在自己的机器上节省空间。 编写只存储url且不需要处理额外文件下载的代码会更容易。 您可以通过避免大型文件下载来减轻主机服务器上的负载。 缺点如下: 将这些url嵌入到您自己的网站或应用程序中称为盗链，这样做是让您在internet上陷入困境的一种快速方法。 您不希望使用其他人的服务器周期为自己的应用程序托管媒体。 位于任何特定URL的文件都可能发生更改。这可能会导致尴尬的效果，例如，如果你在一个公共博客中嵌入一个热链接图像。如果您存储url的目的是为了稍后存储该文件，以便进行进一步的研究，那么它可能最终会丢失，或者在稍后被更改为完全不相关的内容。 真正的web浏览器不只是请求页面的HTML然后继续前进。它们还下载页面所需的所有资产。下载文件可以看起来像一个人在浏览网站，这可能是一个优势。 如果你辩论是否存储文件或URL到一个文件,你应该问问你自己你是否可能视图或读到文件不止一次或两次,或如果该数据库文件是围坐在收集电子尘埃的大部分生活。如果答案是后者，那么最好只存储URL。如果是前者，请继续阅读： 用于检索网页内容的urllib库还包含检索文件内容的函数。下面的程序使用urlib .request.urlretrieve下载图片从一个远程URL： 12345678from urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com')bs = BeautifulSoup(html, 'html.parser')imageLocation = bs.find('a', &#123;'id': 'logo'&#125;).find('img')['src']urlretrieve (imageLocation, 'logo.jpg') 这将从http://pythonscraping.com下载徽标，并将其作为logo.jpg存储在运行脚本的同一目录中。如果您只需要下载一个文件，并且知道如何调用它，以及文件扩展名是什么，那么这种方法非常有效。但大多数抓取器不会下载一个文件，然后就挂掉。下面从http://pythonscraping.com的主页下载所有内部文件，这些文件由任何标记的src属性链接到: 12345678910111213141516171819202122232425262728293031323334353637383940414243import osfrom urllib.request import urlretrievefrom urllib.request import urlopenfrom bs4 import BeautifulSoupdownloadDirectory = 'downloaded'baseUrl = 'http://pythonscraping.com'def getAbsoluteURL(baseUrl, source): if source.startswith('http://www.'): url = 'http://&#123;&#125;'.format(source[11:]) elif source.startswith('http://'): url = source elif source.startswith('www.'): url = source[4:] url = 'http://&#123;&#125;'.format(source) else: url = '&#123;&#125;/&#123;&#125;'.format(baseUrl, source) if baseUrl not in url: return None return urldef getDownloadPath(baseUrl, absoluteUrl, downloadDirectory): path = absoluteUrl.replace('www.', '') path = path.replace(baseUrl, '') path = downloadDirectory+path directory = os.path.dirname(path) if not os.path.exists(directory): os.makedirs(directory) return pathhtml = urlopen('http://www.pythonscraping.com')bs = BeautifulSoup(html, 'html.parser')downloadList = bs.findAll(src=True)for download in downloadList: fileUrl = getAbsoluteURL(baseUrl, download['src']) if fileUrl is not None: print(fileUrl)urlretrieve(fileUrl, getDownloadPath(baseUrl, fileUrl, downloadDirectory)) 你知道所有那些关于从网上下载未知文件的警告吗?这个脚本将它遇到的所有内容下载到您的计算机硬盘上。这包括随机的bash脚本、.exe文件和其他潜在的恶意软件。认为你是安全的，因为你从来没有真正执行任何发送到你的下载文件夹?尤其是如果你以管理员的身份运行这个程序，你就是在自找麻烦。如果你在网站上偶然看到一个文件，它把自己发送到../../../../usr/bin/ python吗?下一次从命令行运行Python脚本时，您可能正在您的机器上部署恶意软件!本程序仅为说明目的而编写;它不应该在没有更广泛的文件名检查的情况下随机部署，而且它应该只在具有有限权限的帐户中运行。像往常一样，备份文件、不在硬盘上存储敏感信息，以及使用一点常识，这些都大有帮助。 这个脚本使用一个lambda函数(在第2章中介绍)来选择首页上所有具有src属性的标记，然后清理和规范url，以获得每个下载的绝对路径(确保丢弃外部链接)。然后，将每个文件下载到您自己机器上下载的本地文件夹中的自己的路径。注意，Python s os模块被简单地用于检索每次下载的目标目录，并在需要时沿着路径创建丢失的目录。os模块充当Python和操作系统之间的接口，允许它操作文件路径、创建目录、获取有关运行进程和环境变量的信息，以及许多其他有用的东西。 Storing Data to CSVCSV(comma-separated values逗号分隔值)是存储电子表格数据的最流行的文件格式之一。由于其简单性，它受到Microsoft Excel和许多其他应用程序的支持。下面是一个完全有效的CSV文件示例: 1234fruit,costapple,1.00banana,0.30pear,1.25 与Python一样，这里的空格也很重要:每一行由换行符分隔，而行中的列由逗号分隔(因此得名csv)。其他形式的CSV文件(有时称为字符分隔值文件)使用制表符或其他字符分隔行，但是这些文件格式不太常见，也不太受广泛支持。 如果您希望直接从web下载CSV文件并将其存储在本地，而不需要进行任何解析或修改，则不需要本节。像下载其他文件一样下载它们，并使用上一节描述的方法以CSV文件格式保存它们。 使用Python的CSV库，修改CSV文件，甚至完全从零开始创建CSV文件都非常容易: 12345678910import csvcsvFile = open('test.csv', 'w+')try: writer = csv.writer(csvFile) writer.writerow(('number', 'number plus 2', 'number times 2')) for i in range(10): writer.writerow( (i, i+2, i*2))finally: csvFile.close() 警告:用Python创建文件是非常可靠的。如果test.csv还不存在，Python将自动创建文件(而不是目录)。如果已经存在，Python将使用新数据覆盖test.csv。 运行后，你应该看到一个CSV文件: 12345number,number plus 2,number times 20,2,01,3,22,4,4... 个常见的web抓取任务是检索HTML表并将其编写为CSV文件。Wikipedia对文本编辑器的比较提供了一个相当复杂的HTML表，其中包含颜色编码、链接、排序和其他HTML垃圾，在将其写入CSV之前需要丢弃这些垃圾。使用BeautifulSoup和get_text()函数，你可以在20行之内完成: 1234567891011121314151617181920import csvfrom urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://en.wikipedia.org/wiki/Comparison_of_text_editors')bs = BeautifulSoup(html, 'html.parser')# The main comparison table is currently the first table on the pagetable = bs.findAll('table',&#123;'class':'wikitable'&#125;)[0]rows = table.findAll('tr')csvFile = open('editors.csv', 'wt+',encoding='utf-8')writer = csv.writer(csvFile)try: for row in rows: csvRow = [] for cell in row.findAll(['td', 'th']): csvRow.append(cell.get_text()) writer.writerow(csvRow)finally: csvFile.close() 如果遇到许多HTML表需要转换为CSV文件，或者许多HTML表需要收集到一个CSV文件中，那么这个脚本非常适合集成到scraper中。然而，如果你只需要做一次，有一个更好的工具:复制和粘贴。选择并复制HTML表的所有内容，并将其粘贴到Excel或谷歌文档中，就可以得到您正在寻找的CSV文件，而无需运行脚本。 MySQL这个就偷懒了，觉得小项目不太会用到数据库，如果后期用到我再不上。 Email这个就偷懒了，觉得小项目不太会用到数据库，如果后期用到我再不上。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-5]]></title>
    <url>%2F2019%2F07%2F21%2FWebScraping5%2F</url>
    <content type="text"><![CDATA[前一章介绍了构建大型、可伸缩和(最重要的是)可维护的web爬虫程序的一些技术和模式。虽然这很容易手工完成，但是许多库、框架，甚至基于GUI的工具都可以为您完成这一任务，或者至少尝试让您的生活变得更简单一些。 本章介绍了开发爬行器的最佳框架之一：Scrapy。编写web爬虫程序的一个挑战是，您经常一次又一次地执行相同的任务：查找页面上的所有链接，评估内部和外部链接之间的差异，转到新页面。了解这些基本模式并能够从头开始编写非常有用，但是Scrapy库为您处理了其中的许多细节。 当然，Scrapy并不是一个读心者。您仍然需要定义页面模板，给它提供开始抓取的位置，并为您要查找的页面定义URL模式。但是在这些情况下，它提供了一个干净的框架来保持代码的组织性。 我是用anaconda来安装的，很简单，在库里找到，并且下载就好。可能是网络问题，我也是下载了好几次才成功。其中最新版的文档在此。 Initializing a New Spider安装了Scrapy框架之后，需要为每个爬行器进行少量的设置。蜘蛛是一个杂乱的项目，就像它的同名蛛形纲动物一样，它的设计目的是爬网。在本章中，我特别用“spider”来描述一个杂乱的项目，用“crawler”来表示“任何爬行web的通用程序，不管使用与否”。 要在当前目录中创建一个新的爬行器，可以从命令行运行以下命令： 1$ scrapy startproject wikiSpider 这将在创建项目的目录中创建一个新的子目录，标题为wikiSpider。在这个目录中是以下文件结构： scrapy.cfg wikiSpider spiders __init.py__ items.py middlewares.py settings.py __init.py__ Writing a Simple Scraper要创建爬行器，您将在spider目录中的wikiSpider/ wikiSpider/article.py中添加一个新文件。在您新创建的article.py文件中，编写以下内容： 1234567891011121314151617import scrapyclass ArticleSpider(scrapy.Spider): name='article' def start_requests(self): urls = [ "http://en.wikipedia.org/wiki/Python_%28programming_language%29", "https://en.wikipedia.org/wiki/Functional_programming", "https://en.wikipedia.org/wiki/Monty_Python"] return [scrapy.Request(url=url, callback=self.parse) for url in urls] def parse(self, response): url = response.url title = response.css('h1::text').extract_first() print('URL is: &#123;&#125;'.format(url)) print('Title is: &#123;&#125;'.format(title)) 这个类的名称(ArticleSpider)与目录的名称不同(wikiSpider)，表示这个类特别负责在wikiSpider这个更广泛的类别下搜索文章页面，您稍后可能希望使用它搜索其他页面类型。 对于包含多种类型内容的大型站点，您可能为每种类型(博客文章、新闻稿、文章等)都有单独的剪贴项目，每个项目都有不同的字段，但是都运行在相同的剪贴项目下。每个爬行器的名称在项目中必须是惟一的。 关于这个爬行器需要注意的另一个关键问题是两个函数start_requests和parse。 start_requests是用于生成程序的入口点请求Scrapy用于抓取网站的对象。 parse是由用户定义的回调函数，并通过callback=self.parse传递给请求对象。稍后，您将看到使用parse函数可以完成的更强大的功能，但是现在它打印页面的标题。 您可以通过导航到wikiSpider/wikiSpider目录运行本文spider，并运行： 1scrapy runspider article.py 默认的杂乱输出相当冗长。除了调试信息外，还应该打印如下行： 12345678910112019-07-21 20:47:55 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/robots.txt&gt; (referer: None)2019-07-21 20:47:56 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/wiki/Functional_programming&gt; (referer: None)URL is: https://en.wikipedia.org/wiki/Functional_programmingTitle is: Functional programming2019-07-21 20:47:56 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/wiki/Monty_Python&gt; (referer: None)URL is: https://en.wikipedia.org/wiki/Monty_PythonTitle is: Monty Python2019-07-21 20:47:58 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (301) to &lt;GET https://en.wikipedia.org/wiki/Python_%28programming_language%29&gt; from &lt;GET http://en.wikipedia.org/wiki/Python_%28programming_language%29&gt;2019-07-21 20:47:58 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://en.wikipedia.org/wiki/Python_%28programming_language%29&gt; (referer: None)URL is: https://en.wikipedia.org/wiki/Python_%28programming_language%29Title is: Python (programming language) scraper转到作为start_urls列出的三个页面，收集信息，然后终止。 Spidering with Rules前一节中的爬行器并不是爬行器，仅限于抓取它提供的url列表。它没有能力自己寻找新的页面。把它要成为一个成熟的爬虫程序，您需要使用由Scrapy。 不幸的是，这种杂乱的框架不能很容易地在jupyter notebook中运行，这使得代码的线性进展难以捕捉。为了在文本中显示所有代码示例，本文存储了上一节中的scraper.py文件，而下面的示例创建了一个遍历多个页面的杂乱爬行器，存储在article .py中(注意复数形式的使用)。 后面的示例也将存储在单独的文件中，每个部分都给出了新的文件名。运行这些示例时，请确保使用了正确的文件名。 123456789# 注意：以下两行已经不支持###############from scrapy.contrib.linkextractors import LinkExtractorfrom scrapy.contrib.spiders import CrawlSpider, Rule# 注意：以上两行已经不支持################改成:from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Rule 创建文件名articles.py，并写入： 12345678910111213141516171819from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Ruleclass ArticleSpider(CrawlSpider): name = 'articles' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [Rule(LxmlLinkExtractor(allow=r'.*'), callback='parse_items', follow=True)] def parse_items(self, response): url = response.url title = response.css('h1::text').extract_first() text = response.xpath('//div[@id="mw-content-text"]//text()').extract() lastUpdated = response.css('li#footer-info-lastmod::text').extract_first() lastUpdated = lastUpdated.replace('This page was last edited on ', '') print('URL is: &#123;&#125;'.format(url)) print('title is: &#123;&#125; '.format(title)) print('text is: &#123;&#125;'.format(text)) print('Last updated: &#123;&#125;'.format(lastUpdated)) 这个新的ArticleSpider扩展了CrawlSpider类。它没有提供start_requests函数，而是提供了start_urls和allowed_domains的列表。这将告诉爬虫从哪里开始爬行，以及它应该遵循还是忽略基于域的链接。 还提供了rules列表。这进一步说明了哪些链接可以遵循或忽略(在本例中，您允许使用正则表达式. *的所有URLs)。 除了提取每个页面上的标题和URL外，还添加了几个新项。使用XPath选择器提取每个页面的文本内容。XPath通常用于检索文本内容，包括子标记中的文本(例如，文本块中的&lt;a&gt;标记)。如果使用CSS选择器来实现这一点，子标记中的所有文本都将被忽略。 最后更新的日期字符串也从页脚解析并存储在lastUpdated变量中。 让我们使用Scrapy ‘s Rule和LinkExtractor仔细看看这行代码: 1rules = [Rule(LinkExtractor(allow=r'.*'), callback='parse_items',follow=True)] 这一行提供了一个杂乱Rule对象列表，这些对象定义了所有找到的链接都要经过过滤的规则。当有多个规则时，按顺序根据规则检查每个链接。第一个匹配的规则用于确定如何处理链接。如果链接不匹配任何规则，则忽略它。它有六个参数： link_extractor 唯一的强制参数是LxmlLinkExtractor对象 callback 用于解析页面内容的函数 cb_kwargs 要传递给callback函数的参数的字典。该字典的格式为{arg_name1: arg_value1, arg_name2: arg_value2}，可以作为一个方便的工具，重用相同的解析函数，用于稍微不同的任务。 follow 指示是否希望在将来的爬行中包含在该页面中找到的链接。如果没有提供回调函数，则默认值为True(毕竟，如果您没有对页面做任何操作，那么您至少应该使用它来继续在站点中爬行)。如果提供回调函数，则默认为False。 LxmlLinkExtractorr是一个简单的类，专门用于根据提供给它的规则识别和返回HTML内容页面中的链接。它有许多参数，可用于接受或拒绝基于CSS和XPath选择器的链接、标记(您可以在不仅仅是锚标记中寻找链接!)、域等等。 LxmlLinkExtractor类甚至可以扩展，并且可以创建自定义参数。有关更多信息，请参阅有关链接提取器的Scrapy文档。 allow 允许与提供的正则表达式匹配的所有链接 deny 拒绝与提供的正则表达式匹配的所有链接 使用两个单独的规则和一个解析函数的LinkExtractor类，您可以创建一个爬行Wikipedia的爬行器，识别所有文章页面并标记非文章页面(articlesmorerulls .py)： 12345678910111213141516171819202122232425from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Ruleclass ArticleSpider(CrawlSpider): name = 'articles' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [ Rule(LxmlLinkExtractor(allow='^(/wiki/)((?!:).)*$'), callback='parse_items', follow=True, cb_kwargs=&#123;'is_article': True&#125;), Rule(LxmlLinkExtractor(allow='.*'), callback='parse_items', cb_kwargs=&#123;'is_article': False&#125;) ] def parse_items(self, response, is_article): print(response.url) title = response.css('h1::text').extract_first() if is_article: url = response.url text = response.xpath('//div[@id="mw-content-text"]//text()').extract() lastUpdated = response.css('li#footer-info-lastmod::text').extract_first() lastUpdated = lastUpdated.replace('This page was last edited on ', '') print('Title is: &#123;&#125; '.format(title)) print('title is: &#123;&#125; '.format(title)) print('text is: &#123;&#125;'.format(text)) else: print('This is not an article: &#123;&#125;'.format(title)) 回想一下，这些规则按照它们在列表中显示的顺序应用于每个链接。所有文章页面(以/wiki/开头且不包含冒号的页面)首先传递给parse_items函数，默认参数为is_arti cle=True。然后，将所有其他的非article链接传递给parse_items函数，参数为is_article=False。 当然，如果您希望只收集文章类型的页面而忽略其他所有页面，那么这种方法是不切实际的。如果忽略与文章URL模式不匹配的页面，并完全忽略第二条规则(以及is_arti cle变量)，则会容易得多。然而，这种方法在URL信息或爬行过程中收集的信息影响页面解析方式的奇怪情况下可能很有用。 Creating Items到目前为止，您已经了解了许多查找、解析和爬行网站的方法。但是，Scrapy还提供了一些有用的工具，用于将收集到的项组织起来，并将其存储在具有定义良好字段的自定义对象中。 要帮助组织正在收集的所有信息，您需要创建一个Article对象。在items.py文件中定义一个名为Article的新项目。当您打开items.py文件时，它应该是这样的: 12345678910# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# http://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass WikispiderItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() pass 用一个扩展了Scrapy.Item的新Article类替换这个默认的Item存根。 123456import scrapyclass Article(scrapy.Item): url = scrapy.Field() title = scrapy.Field() text = scrapy.Field() lastUpdated = scrapy.Field() 您正在定义将从每个页面收集的三个字段：标题、URL和页面最后一次编辑的日期。 如果要为多个页面类型收集数据，应该在items.py中将每个单独的类型定义为它自己的类。如果您的项比较大，或者您开始将更多的解析功能转移到项对象中，您还可能希望将每个项提取到自己的文件中。然而，尽管这些项目很小，但我喜欢将它们保存在一个文件中。 在文件ArticleSpider .py中，注意为了创建新的文章项，对ArticleSpider类所做的更改: 1234567891011121314151617181920from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Rulefrom wikiSpider.items import Articleclass ArticleSpider(CrawlSpider): name = 'articleItems' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [ Rule(LxmlLinkExtractor(allow='(/wiki/)((?!:).)*$'), callback='parse_items', follow=True), ] def parse_items(self, response): article = Article() article['url'] = response.url article['title'] = response.css('h1::text').extract_first() article['text'] = response.xpath('//div[@id="mw-content-text"]//text()').extract() lastUpdated = response.css('li#footer-info-lastmod::text').extract_first() article['lastUpdated'] = lastUpdated.replace('This page was last edited on ', '') return article 它将输出通常杂乱的调试数据以及每个项目Python字典: 12 用杂乱的条目不仅仅是为了促进良好的代码组织，或者以一种可读的方式进行布局。项目提供了许多用于输出和处理数据的工具，这些工具将在下一节中介绍。 Outputting ItemsScrapy使用Item对象来确定应该从它访问的页面中保存哪些信息。这些信息可以通过多种方式保存，如CSV、JSON或XML文件，使用以下命令: 123$ scrapy runspider articleItems.py -o articles.csv -t csv$ scrapy runspider articleItems.py -o articles.json -t json$ scrapy runspider articleItems.py -o articles.xml -t xml 每一个都运行scraper articleItems项，并将指定格式的输出写入提供的文件。如果该文件不存在，将创建该文件。 您可能已经注意到，在前面的示例中创建的文章spider中，文本变量是字符串列表，而不是单个字符串。这个列表中的每个字符串表示单个HTML元素中的文本，而&lt;div id=&quot;mwcontent- text&quot;&gt;中的内容(您从其中收集文本数据)由许多子元素组成。Scrapy很好地管理了这些更复杂的值。例如，在CSV格式中，它将列表转换为字符串并转义所有逗号，以便在单个CSV单元格中显示文本列表。 在XML中，这个列表的每个元素都保存在子值标签中： 123456789101112&lt;items&gt;&lt;item&gt; &lt;url&gt;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life&lt;/url&gt; &lt;title&gt;Benevolent dictator for life&lt;/title&gt; &lt;text&gt; &lt;value&gt;For the political term, see &lt;/value&gt; &lt;value&gt;Benevolent dictatorship&lt;/value&gt; ... &lt;/text&gt; &lt;lastUpdated&gt; 13 December 2017, at 09:26.&lt;/lastUpdated&gt;&lt;/item&gt;.... 在JSON格式中，列表保存为列表。当然，您可以自己使用Item对象，并以您想要的任何方式将它们写入文件或数据库，只需在爬虫程序的解析函数中添加适当的代码即可。 The Item Pipeline虽然Scrapy是单线程的，但它能够异步地发出和处理许多请求。这使得它比本书目前所写的scraper更快，尽管我一直坚信，对于web抓取来说，速度并不总是越快越好。 使用item pipeline可以在等待请求返回的同时执行所有数据处理，而不是在发出另一个请求之前等待数据处理，从而进一步提高web scraper的速度。当数据处理需要大量时间或必须执行处理器密集型计算时，有时甚至需要这种类型的优化。要创建项目管道，请重新访问本章开头创建的settings.py文件。您应该看到以下注释行 12345# Configure item pipelines# See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html#ITEM_PIPELINES = &#123;# &apos;wikiSpider.pipelines.WikispiderPipeline&apos;: 300,#&#125; 取消最后三行注释，代之以： 123ITEM_PIPELINES = &#123; &apos;wikiSpider.pipelines.WikispiderPipeline&apos;: 300,&#125; 这提供了一个Python类 wikispider .pipeline。WikispiderPipeline将用于处理数据，如果有多个处理类，则使用一个整数表示运行管道的顺序。虽然这里可以使用任何整数，但通常使用0-1000，并且将按升序运行。 现在，您需要添加管道类并重写原始爬行器，以便爬行器收集数据，而管道将执行繁重的数据处理工作。在原始爬行器中编写parse_items方法来返回响应并让管道创建Article对象，这可能很有吸引力： 12def parse_items(self, response): return response 但是，Scrapy框架不允许这样做，并且不允许一个Item对象(例如必须返回Article，它扩展Item)。所以parse_items现在的目标是提取原始数据，尽可能少的处理，这样它就可以传递给管道: 12345678910111213141516171819from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractorfrom scrapy.spiders import CrawlSpider,Rulefrom wikiSpider.items import Articleclass ArticleSpider(CrawlSpider): name = 'articlePipelines' allowed_domains = ['wikipedia.org'] start_urls = ['https://en.wikipedia.org/wiki/Benevolent_dictator_for_life'] rules = [ Rule(LxmlLinkExtractor(allow='(/wiki/)((?!:).)*$'), callback='parse_items', follow=True), ] def parse_items(self, response): article = Article() article['url'] = response.url article['title'] = response.css('h1::text').extract_first() article['text'] = response.xpath('//div[@id="mw-content-text"]//text()').extract() article['lastUpdated'] = response.css('li#footer-info-lastmod::text').extract_first() return article 当然，现在需要通过添加管道将settings.py文件和更新后的爬行器绑定在一起。当Scrapy项目第一次初始化时，在wikiSpider/wikiSpider/pipelines.py文件中创建了一个文件: 12345678# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlclass WikispiderPipeline(object): def process_item(self, item, spider): return item 应该用新的管道代码替换这个存根类。在前面的部分中，您已经以原始格式收集了两个字段，这些字段可以使用额外的处理：lastUpdated(这是一个格式很差的字符串对象，表示一个日期)和text(一组杂乱的字符串片段)。 应该使用以下代码替换wikiSpider/wikiSpider/ pipelines.py中的存根代码: 123456789101112from datetime import datetimefrom wikiSpider.items import Articlefrom string import whitespaceclass WikispiderPipeline(object): def process_item(self, article, spider): article['lastUpdated'] = article['lastUpdated'].replace('This page was last edited on', '') article['lastUpdated'] = article['lastUpdated'].strip() article['lastUpdated'] = datetime.strptime(article['lastUpdated'], '%d %B %Y, at %H:%M.') article['text'] = [line for line in article['text'] if line not in whitespace] article['text'] = ''.join(article['text']) return article 类WikispiderPipeline有一个process_item方法，该方法接收Article对象，将最后更新的字符串解析为Python datetime对象，并将文本从字符串列表中清理并连接到单个字符串中。 process_item是每个管道类的强制方法。Scrapy使用此方法异步传递爬行器收集的Item。例如，如果您像上一节那样将条目输出到JSON或CSV，那么这里返回的已解析的Article对象将被Scrapy记录或打印。 现在，在决定在何处进行数据处理时，您有两种选择:爬行器中的parse_items方法，或者管道中的process_items方法。 可以在settings.py文件中声明具有不同任务的多个管道。然而，Scrapy按顺序将所有项目(无论项目类型如何)传递到每个管道。在数据到达管道之前，项目特定的解析可能在爬行器中得到更好的处理。但是，如果这种解析需要很长时间，您可能需要考虑将其移动到管道(在管道中可以异步处理它)，并添加对项类型的检查: 123def process_item(self, item, spider): if isinstance(item, Article): # Article-specific processing here 在编写杂乱的项目，尤其是大型项目时，要考虑哪些处理和在哪里处理。 Logging with Scrapy由Scrapy生成的调试信息可能很有用，但是，正如您可能已经注意到的，它通常过于冗长。你可以很容易地调整日志记录的水平，在你的Scrapy项目的settings.py文件中添加一行: 1LOG_LEVEL = 'ERROR' Scrapy使用了日志级别的标准层次结构，如下所示: CRITICAL ERROR WARNING DEBUG INFO 如果日志设置为ERROR，则只显示CRITICAL和ERROR日志。如果日志设置为INFO，那么将显示所有日志，依此类推。 除了通过settings.py文件控制日志记录之外，还可以从命令行控制日志的位置。若要将日志输出到单独的日志文件而不是终端，请在从命令行运行时定义一个日志文件: 1$ scrapy crawl articles -s LOG_FILE=wiki.log 这将在当前目录中创建一个新的日志文件(如果不存在)，并将所有日志输出到该文件中，从而使终端只显示手动添加的Python print语句。 More ResourcesScrapy是一个强大的工具，可以处理与web爬行相关的许多问题。它自动收集所有url并将它们与预定义的规则进行比较，确保所有url都是惟一的，在需要的地方对相对url进行规范化，并递归更深入地进入页面。 尽管本章几乎没有触及到Scrapy功能的表面，但我鼓励您查看Dimitrios编写的Scrapy文档，并学习一下它库兹-劳卡斯(O ‘Reilly)编写的Learning Scrapy，提供了关于该框架的全面论述。 Scrapy是一个非常大且不断扩展的库，具有许多特性。它的功能可以无缝地协同工作，但有许多重叠的领域，使用户可以轻松地在其中开发自己的特定样式。如果有什么你想做的这里没有提到，可能有一种(或几种)方法可以做到！]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-4]]></title>
    <url>%2F2019%2F07%2F21%2FWebScraping4%2F</url>
    <content type="text"><![CDATA[当您能够控制数据和输入时，编写干净且可伸缩的代码就已经非常困难了。为web爬虫程序编写代码通常会带来独特的组织挑战，因为web爬虫程序可能需要从程序员无法控制的不同站点集合中抓取和存储各种数据。 可能被要求从各种各样的网站收集新闻文章或博客文章，每个网站都有不同的模板和布局。一个网站的h1标签包含文章的标题，另一个网站的h1标签包含网站本身的标题，文章标题在&lt;span id=&quot;title&quot;&gt;中。 可能需要灵活地控制哪些网站被擦除，以及它们如何被擦除，以及一种快速添加新网站或修改现有网站的方法，尽可能快，而不需要编写多行代码。 可能会被要求从不同的网站上搜集产品的价格，最终目的是比较相同产品的价格。也许这些价格以不同的货币表示，也许还需要将其与来自其他非web源的外部数据相结合。 尽管web爬虫程序的应用程序几乎是无穷无尽的，但是大型可伸缩爬虫程序往往会陷入以下几种模式之一。通过学习这些模式并识别它们适用的情况，可以极大地提高web爬行器的可维护性和健壮性。 本章主要关注web爬虫程序，这些爬虫程序从各种网站收集有限数量的数据类型(如餐馆评论、新闻文章、公司简介)，并将这些数据类型存储为Python对象，从数据库中读写数据。 Planning and Defining Objectsweb抓取的一个常见陷阱是完全根据眼前可用的内容定义要收集的数据。例如，如果你想收集产品数据，你可以先去服装店看看，然后决定你刮下来的每一件产品都需要有以下几个字段： 产品名字 价格 描述 尺寸 颜色 材质 客户评价 查看另一个网站，您会发现页面上列出了sku(用于跟踪和订购商品的库存单位)。想必肯定也想收集这些数据，即使它没有出现在第一个站点上!添加这个字段： SKU 虽然服装可能是一个很好的开始，但您也希望确保可以将这个爬虫扩展到其他类型的产品。你开始浏览其他网站的产品部分，并决定你也需要收集这些信息： 显然，这是一种不可持续的做法。每次在网站上看到新信息时，只要向产品类型添加属性，就会导致太多字段无法跟踪。不仅如此，每次抓取一个新网站时，都将被迫对该网站的字段和到目前为止积累的字段进行详细分析，并可能添加新字段(修改的Python对象类型和数据库结构)。这将导致混乱和难以阅读的数据集，可能导致使用它的问题。 在决定收集哪些数据时，最好的方法之一就是完全忽略这些网站。你不会通过浏览一个网站，然后说:“存在什么?但是通过说，“我需要什么?”然后想办法从那里找到你需要的信息。 也许您真正想做的是比较多个商店之间的产品价格，并随着时间的推移跟踪这些产品的价格。在这种情况下，你需要足够的信息来唯一地识别产品，就是这样： 商品名称 制造商 产品编号(如适用/相关) 需要注意的是，这些信息都不是特定于特定存储的。例如，产品评论、评级、价格，甚至描述都是特定于特定商店中该产品的实例的。可以单独存储。 其他信息(产品的颜色、材质)是特定于产品的，但可能很少——它并不适用于所有产品。重要的是退后一步，为你考虑的每一项都列一个清单，然后问自己以下问题： 这些信息对项目目标有帮助吗？如果我没有它，它会成为一个障碍吗？或者它只是“拥有它很好”，但最终不会影响任何东西？ 如果这在将来可能有帮助，但我不确定，在以后的时间返回并收集数据会有多难？ 这些数据与我已经收集到的数据是否冗余？ 将数据存储在这个特定对象中合乎逻辑吗？(如前所述，如果同一产品的描述在不同站点之间发生变化，那么在产品中存储描述就没有意义了。) 如果你决定你需要收集数据，重要的是问几个问题，然后决定如何存储和处理它的代码： 这些数据是稀疏的还是密集的?它是相关的，并在每个列表中填充，还是只包含少数列表？ 数据有多大？ 特别是在大数据的情况下，我是否需要在每次运行分析时定期检索它，或者只是偶尔检索？ 这类数据的变量有多大?我是否需要定期添加新属性、修改类型(比如可能经常添加的织物图案)，或者设置为石头(鞋码)？ 假设计划围绕产品属性和价格进行一些元分析：例如，一本书的页数，或者一件衣服的面料类型，以及将来可能与价格相关的其他属性。您将遍历这些问题并意识到这些数据是稀疏的(很少有产品具有这些属性)，并且您可能决定频繁地添加或删除这些属性。在这种情况下，创建这样的产品类型可能是有意义的： 商品名字 制造商 产品编号(如适用/相关) 属性(可选列表或字典) 属性类型是这样的： 属性名字 属性值 这允许操作者随着时间的推移灵活地添加新产品属性，而不需要重新设计数据模式或重写代码。在决定如何在数据库中存储这些属性时，可以将JSON写入属性字段，或者将每个属性存储在一个单独的表中，并使用产品ID。有关实现这些类型的数据库模型的更多信息，请参见第6章。 您也可以将上述问题应用于您需要存储的其他信息。为了跟踪每种产品的价格，您可能需要以下工具： 产品编号 商店编号 价钱 日期/时间戳价格见 但是，如果产品的属性实际上改变了产品的价格，情况又会怎样呢?例如，商店可能会对一件大衬衫比一件小衬衫要价更高，因为大衬衫需要更多的劳动力或材料。在这种情况下，您可以考虑将单个衬衫产品拆分为每个尺寸的单独产品列表(以便每个衬衫产品可以独立定价)，或者创建一个新的项目类型来存储关于产品实例的信息，其中包含以下字段： 产品编号 实例类型(本例中为衬衫的大小) 每个价格是这样的： 产品实例ID 商店编号 价钱 日期/时间戳价格见 虽然“产品和价格”的主题似乎过于具体，但是您需要问自己的基本问题，以及在设计Python对象时使用的逻辑，几乎适用于所有情况。 如果您正在抓取新闻文章，您可能需要以下基本信息： 题目 作者 日期 内容 但是说一些文章包含修改日期，或者相关文章，或者一些社交媒体分享。你需要这些吗？它们与项目相关吗？如果不是所有的新闻网站都使用所有形式的社交媒体，而且社交媒体网站的受欢迎程度可能会随着时间的推移而上升或下降，你如何有效和灵活地存储社交媒体分享的数量？ 当面对一个新项目时，立即开始编写Python来抓取网站是很有诱惑力的。数据模型(放在后面考虑)常常会受到第一个站点上数据的可用性和格式的强烈影响。 然而，数据模型是使用它的所有代码的基础。模型中的错误决策很容易导致编写和维护代码的问题，或者难以提取和有效地使用结果数据。特别是在处理各种已知和未知的网站时——认真考虑和计划你到底需要收集什么，以及如何储存，变得至关重要。 Dealing with Different Websites Layouts像谷歌这样的搜索引擎最令人印象深刻的功绩之一是，它能够从各种各样的网站中提取相关和有用的数据，而不需要预先了解网站结构本身。尽管我们人类能够立即识别页面的标题和主要内容(除非web设计非常糟糕)，但是让机器人做同样的事情要困难得多。 幸运的是，在大多数web爬行的情况下，不是要从从未见过的站点收集数据，而是要从几个或几十个由人类预先选择的站点收集数据。这意味着不需要使用复杂的算法或机器学习来检测页面上哪些文本看起来最像标题，或者哪些可能是主要内容。 可以手动确定这些元素是什么。最明显的方法是为每个网站编写一个单独的web爬虫程序或页面解析器。每个对象都可以接受URL、字符串或BeautifulSoup对象，并为被抓取的对象返回一个Python对象。 下面是内容类(表示网站上的一段内容，比如一篇新闻文章)和两个scraper函数的示例，它们接收一个BeautifulSoup对象并返回一个内容实例 123456789101112131415161718192021222324252627282930313233343536373839import requestsclass Content: def __init__(self, url, title, body): self.url = url self.title = title self.body = bodydef getPage(url): req = requests.get(url) return BeautifulSoup(req.text, 'html.parser')def scrapeNYTimes(url): bs = getPage(url) title = bs.find('h1').text lines = bs.select('div.StoryBodyCompanionColumn div p') body = '\n'.join([line.text for line in lines]) return Content(url, title, body)def scrapeBrookings(url): bs = getPage(url) title = bs.find('h1').text body = bs.find('div', &#123;'class', 'post-body'&#125;).text return Content(url, title, body)url = 'https://www.brookings.edu/blog/future-development/2018/01/26/delivering-inclusive-urban-access-3-uncomfortable-truths/'content = scrapeBrookings(url)print('Title: &#123;&#125;'.format(content.title))print('URL: &#123;&#125;\n'.format(content.url))print(content.body)url = 'https://www.nytimes.com/2018/01/25/opinion/sunday/silicon-valley-immortality.html'content = scrapeNYTimes(url)print('Title: &#123;&#125;'.format(content.title))print('URL: &#123;&#125;\n'.format(content.url))print(content.body) 当您开始为其他新闻站点添加scraper功能时，您可能会注意到模式正在形成。每个网站的解析功能本质上都是一样的： 选择title元素并提取标题的文本 选择文章的主要内容 根据需要选择其他内容项 返回使用前面找到的字符串实例化的Content对象 这里唯一真正的站点相关变量是用于获取每条信息的CSS选择器。BeautifulSoup的find和find_all函数接受两个参数—一个标记字符串和一个键/值属性字典—因此您可以将这些参数作为参数传递进来，这些参数定义站点本身的结构和目标数据的位置。 解释以下requests的用法。 1r=requests.get(url,params,**kwargs) url: 需要爬取的网站地址。 params: 翻译过来就是参数， url中的额外参数，字典或者字节流格式，可选。 **kwargs : 12个控制访问的参数 而对于输出r，它的用法如下： 属性 说明 r.status_code http请求的返回状态，若为200则表示请求成功。 r.text http响应内容的字符串形式，即返回的页面内容 r.encoding 从http header 中猜测的相应内容编码方式 r.apparent_encoding 从内容中分析出的响应内容编码方式（备选编码方式） r.content http响应内容的二进制形式 具体例子：(其中有的输出太多，已删除部分) 123456789101112131415161718import requestsr=requests.get("http://www.baidu.com")r.status_code200r.encoding'ISO-8859-1'r.apparent_encoding'utf-8'r.text'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;ipt&gt; &lt;a href=//www.baidu.com/more/ name=tj_briicon class=bri style="display: block;"&gt;æ\x9b´å¤\x9aäº§å\x93\x81&lt;/a&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div id=ftCon&gt; &lt;div id=ftConw&gt; &lt;p id=lh&gt; &lt;a com/ class=cp-feedback&gt;æ\x84\x8fè§\x81å\x8f\x8dé¦\x88&lt;/a&gt;&amp;nbsp;äº¬ICPè¯\x81030173å\x8f·&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n'r.encoding='utf-8'r.text'&lt;!DOCTYPE html&gt;\r\n&lt;!--STATUS OK--&gt;&lt;html&gt; &lt;head&gt;&lt;meta http-equiv=content-type content=text/html;charset=utf-8&gt;&lt;meta http-equiv=X-UA-Compatible content=IE=Edge&gt;&lt;meta chref=http://s1.bdstatic.com/r/www/cache/bdorz/baidu.min.css="h读&lt;/a&gt;&amp;nbsp; &lt;a href=http://jianyi.baidu.com/ class=cp-feedback&gt;意见反馈&lt;/a&gt;&amp;nbsp;京ICP证030173号&amp;nbsp; &lt;img src=//www.baidu.com/img/gs.gif&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt;\r\n' 还有，join(iterable)函数，用于以指定分隔符将可迭代对象【成员必须为str类型】连接为一个新的字符串,分隔符可以为空，返回值位字符串 123456789101112131415161718192021222324252627282930313233343536import osstring = "test"lis = ['w', 'e', 'q']tpl = ('w', 'e', 'q')dic = &#123;"55": 5, "44": 4, "22": 2, "33": 3, "11": 1&#125;print("11".join(string))t11e11s11tprint("".join(tpl))weqprint(" ".join(lis))w e qprint("key is : [%s] " % (",".join(dic)))key is : [55,44,22,33,11] # 字符串去重并按从大到小排列words = "wsasdeddcewtttwssa"words_set = set(words) # 集合特性实现去重 字符串集合化words_list = list(words_set) # 集合列表化words_list.sort(reverse=True) # 设置排序为从大到小new_words = "".join(words_list) # join方法以空位分隔符拼接列表元素位新字符串print(words_list)['w', 't', 's', 'e', 'd', 'c', 'a']print(new_words)wtsedcaprint(os.path.join("/home/", "test/", "python")) /home/test/pythonprint(os.path.join("/home", "/test", "/python"), end="")/python 为了让事情变得更方便，你可以使用BeautifulSoup select函数，为你想要收集的每条信息添加一个CSS选择器字符串，然后把所有这些选择器都放到dictionary对象中，而不是处理所有这些标签参数和键/值对： 123456789101112131415161718192021222324252627class Content: """ Common base class for all articles/pages """ def __init__(self, url, title, body): self.url = url self.title = title self.body = body def print(self): """ Flexible printing function controls output """ print('URL: &#123;&#125;'.format(self.url)) print('TITLE: &#123;&#125;'.format(self.title)) print('BODY:\n&#123;&#125;'.format(self.body))class Website: """ Contains information about website structure """ def __init__(self, name, url, titleTag, bodyTag): self.name = name self.url = url self.titleTag = titleTag self.bodyTag = bodyTag 请注意，网站类并不存储从各个页面本身收集的信息，而是存储关于如何收集数据的说明。它不存储标题“My Page title”。它只存储字符串标签h1，表示可以在哪里找到标题。这就是为什么这个类被称为网站(这里的信息属于整个网站)，而不是内容(仅包含来自单个页面的信息)。 1234567891011121314151617181920212223242526272829303132333435import requestsfrom bs4 import BeautifulSoupclass Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, 'html.parser') def safeGet(self, pageObj, selector): """ Utilty function used to get a content string from a Beautiful Soup object and a selector. Returns an empty string if no object is found for the given selector """ selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &gt; 0: return '\n'.join([elem.get_text() for elem in selectedElems]) return '' def parse(self, site, url): """ Extract content from a given page URL """ bs = self.getPage(url) if bs is not None: title = self.safeGet(bs, site.titleTag) body = self.safeGet(bs, site.bodyTag) if title != '' and body != '': content = Content(url, title, body) content.print() 下面的代码定义了网站对象，并开始了这个过程： 123456789101112131415161718192021crawler = Crawler()siteData = [ ['O\'Reilly Media', 'http://oreilly.com', 'h1', 'section#product-description'], ['Reuters', 'http://reuters.com', 'h1', 'div.StandardArticleBody_body_1gnLA'], ['Brookings', 'http://www.brookings.edu', 'h1', 'div.post-body'], ['New York Times', 'http://nytimes.com', 'h1', 'div.StoryBodyCompanionColumn div p']]websites = []for row in siteData: websites.append(Website(row[0], row[1], row[2], row[3]))crawler.parse(websites[0], 'http://shop.oreilly.com/product/0636920028154.do')crawler.parse( websites[1], 'http://www.reuters.com/article/us-usa-epa-pruitt-idUSKBN19W2D0')crawler.parse( websites[2], 'https://www.brookings.edu/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/')crawler.parse( websites[3], 'https://www.nytimes.com/2018/01/28/business/energy-environment/oil-boom.html') 虽然这个新方法乍一看似乎并不比为每个新站点编写一个新的Python函数简单多少，但是想象一下，当从一个有4个站点源的系统切换到一个有20或200个源的系统时会发生什么。每个字符串列表相对容易编写。它不占太多空间。它可以从数据库或CSV文件加载。它可以从远程源代码导入，也可以交给有一些前端经验的非程序员填写和添加新网站，而且他们永远不必看一行代码。当然，缺点是正在放弃一定程度的灵活性。在第一个例子中，每个网站都有自己的自由形式函数来选择和解析HTML，以获得最终结果。在第二个例子中，每个网站都需要有一个特定的结构，保证字段的存在，字段中的数据必须是干净的，每个目标字段必须有一个唯一的和可靠的CSS选择器。 然而，我认为这种方法的威力和相对灵活性超过了弥补其真实的或察觉到的缺点。下一节将详细介绍这个基本模板的应用程序和扩展，例如，可以处理缺少的字段、收集不同类型的数据、仅遍历网站的特定部分以及存储关于页面的更复杂的信息。 Structuring Crawlers如果你仍然需要手工查找每个链接，那么创建灵活的、可修改的网站布局类型并没有多大用处。前一章展示了各种各样的方法，可以自动地在网站上爬行并找到新的页面。本节将展示如何将这些方法合并到一个结构良好且可扩展的网站爬虫程序中，该爬虫程序可以以自动化的方式收集链接和发现数据。我在这里只介绍了三种基本的web爬虫结构，尽管我相信它们适用于您在野外爬行站点时可能需要的大多数情况，只是在这里和那里做了一些修改。如果你遇到一个不寻常的情况与你自己的爬行问题，我也希望你将使用这些结构作为灵感，以创造一个优雅和强大的爬虫设计。 Crawling Sites Through Search抓取网站最简单的方法之一就是使用与人类相同的方法:使用搜索栏。虽然搜索一个网站的关键字或主题，并收集搜索结果列表的过程可能看起来像一项任务，在不同的网站之间有很大的可变性，但有几个关键点使这一点变得非常简单： 大多数站点检索特定主题的搜索结果列表，方法是通过URL中的参数将该主题作为字符串传递。例如:http://example.com?search=myTopic。URL的第一部分可以保存为网站对象的属性，主题可以简单地附加到它。 搜索之后，大多数站点将结果页面呈现为一个易于识别的链接列表，通常带有一个方便的周围标记，如&lt;span class=&quot;result&quot;&gt;，其确切格式也可以作为网站对象的属性存储。 每个结果链接要么是相对URL(例如/articles/page.html)，要么是绝对URLURL(例如,http://example.com/articles/page.html)。无论期望的是绝对URL还是相对URL，都可以存储为网站对象的属性。 让我们看看这个算法在代码中的实现。Content类与前面的示例非常相似。正在添加URL属性，以跟踪内容是在哪里找到的： 12345678910111213141516class Content: """Common base class for all articles/pages""" def __init__(self, topic, url, title, body): self.topic = topic self.title = title self.body = body self.url = url def print(self): """ Flexible printing function controls output """ print("New article found for topic: &#123;&#125;".format(self.topic)) print("TITLE: &#123;&#125;".format(self.title)) print("BODY:\n&#123;&#125;".format(self.body)) print("URL: &#123;&#125;".format(self.url)) 网站类添加了一些新属性。searchUrl定义了如果您附加了要查找的主题，应该到哪里获取搜索结果。resultListing定义了一个“框”，其中包含关于每个结果的信息，resultUrl定义了这个框中的标记，它将为您提供结果的确切URL。absoluteUrl属性是一个布尔值，它告诉您这些搜索结果是绝对链接还是相对链接。 123456789101112class Website: """Contains information about website structure""" def __init__(self, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag): self.name = name self.url = url self.searchUrl = searchUrl self.resultListing = resultListing self.resultUrl = resultUrl self.absoluteUrl=absoluteUrl self.titleTag = titleTag self.bodyTag = bodyTag 已经扩展了一些，包含了我们的网站数据、要搜索的主题列表和一个循环，该循环遍历所有主题和所有网站。它还包含一个搜索功能，可以导航到特定网站和主题的搜索页面，并提取该页面上列出的所有结果链接。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import requestsfrom bs4 import BeautifulSoupclass Crawler: def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, 'html.parser') def safeGet(self, pageObj, selector): childObj = pageObj.select(selector) if childObj is not None and len(childObj) &gt; 0: return childObj[0].get_text() return '' def search(self, topic, site): """ Searches a given website for a given topic and records all pages found """ bs = self.getPage(site.searchUrl + topic) searchResults = bs.select(site.resultListing) for result in searchResults: url = result.select(site.resultUrl)[0].attrs['href'] # Check to see whether it's a relative or an absolute URL if(site.absoluteUrl): bs = self.getPage(url) else: bs = self.getPage(site.url + url) if bs is None: print('Something was wrong with that page or URL. Skipping!') return title = self.safeGet(bs, site.titleTag) body = self.safeGet(bs, site.bodyTag) if title != '' and body != '': content = Content(topic, title, body, url) content.print()crawler = Crawler()siteData = [ ['O\'Reilly Media', 'http://oreilly.com', 'https://ssearch.oreilly.com/?q=', 'article.product-result', 'p.title a', True, 'h1', 'section#product-description'], ['Reuters', 'http://reuters.com', 'http://www.reuters.com/search/news?blob=', 'div.search-result-content', 'h3.search-result-title a', False, 'h1', 'div.StandardArticleBody_body_1gnLA'], ['Brookings', 'http://www.brookings.edu', 'https://www.brookings.edu/search/?s=', 'div.list-content article', 'h4.title a', True, 'h1', 'div.post-body']]sites = []for row in siteData: sites.append(Website(row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]))topics = ['python', 'data science']for topic in topics: print('GETTING INFO ABOUT: ' + topic) for targetSite in sites: crawler.search(topic, targetSite) 这个脚本循环遍历主题列表中的所有主题，并在开始抓取主题之前声明: 1GETTING INFO ABOUT python 然后它循环遍历站点列表中的所有站点，并为每个特定的主题抓取每个特定的站点。每当它成功地抓取到关于页面的信息时，就会将其打印到控制台: 1234New article found for topic: pythonURL: http://example.com/examplepage.htmlTITLE: Page Title HereBODY: Body content is here 注意，它循环遍历所有主题，然后在内部循环遍历所有网站。为什么不反过来，从一个网站收集所有的主题，然后从下一个网站收集所有的主题?首先遍历所有主题是一种更均匀地分配任何一台web服务器上的负载的方法。如果有一个包含数百个主题和数十个网站的列表，这一点尤其重要。你不会一次对一个网站发出成千上万的请求;你发出10个请求，等待几分钟，再发出10个请求，等待几分钟，等等。 尽管请求的数量最终是相同的，但通常情况下，在合理的时间内尽可能多地分发这些请求会更好。注意循环的结构是一种简单的方法。 其中函数解释如下： Crawling Sites Through Links前一章介绍了一些方法来识别web页面上的内部和外部链接，然后使用这些链接在站点上爬行。在本节中，您将把这些相同的基本方法组合成一个更灵活的网站爬虫程序，它可以遵循任何匹配特定URL模式的链接。 当希望从站点收集所有数据，而不仅仅是从特定的搜索结果或页面列表中收集数据时，这种类型的爬虫非常适合于项目。当站点的页面可能杂乱无章或广泛分散时，它也可以很好地工作。这些类型的爬虫程序不需要结构化的方法来定位链接，就像前一节在搜索页面中爬行一样，因此描述搜索页面的属性在Website对象中不需要。但是，因为爬虫没有为它正在寻找的链接的位置提供特定的指令，所以您确实需要一些规则来告诉它应该选择什么样的页面。您提供targetPattern(目标URLs的正则表达式)，并留下布尔值 absoluteUrl变量来完成此操作： 123456789101112131415161718class Website: def __init__(self, name, url, targetPattern, absoluteUrl, titleTag, bodyTag): self.name = name self.url = url self.targetPattern = targetPattern self.absoluteUrl=absoluteUrl self.titleTag = titleTag self.bodyTag = bodyTagclass Content: def __init__(self, url, title, body): self.url = url self.title = title self.body = body def print(self): print("URL: &#123;&#125;".format(self.url)) print("TITLE: &#123;&#125;".format(self.title)) print("BODY:\n&#123;&#125;".format(self.body)) Content类与第一个爬虫程序示例中使用的是同一个类。爬虫类从每个站点的主页开始，定位内部链接，解析找到的每个内部链接的内容： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import reclass Crawler: def __init__(self, site): self.site = site self.visited = [] def getPage(self, url): try: req = requests.get(url) except requests.exceptions.RequestException: return None return BeautifulSoup(req.text, 'html.parser') def safeGet(self, pageObj, selector): selectedElems = pageObj.select(selector) if selectedElems is not None and len(selectedElems) &gt; 0: return '\n'.join([elem.get_text() for elem in selectedElems]) return '' def parse(self, url): bs = self.getPage(url) if bs is not None: title = self.safeGet(bs, self.site.titleTag) body = self.safeGet(bs, self.site.bodyTag) if title != '' and body != '': content = Content(url, title, body) content.print() def crawl(self): """ Get pages from website home page """ bs = self.getPage(self.site.url) targetPages = bs.findAll('a', href=re.compile(self.site.targetPattern)) for targetPage in targetPages: targetPage = targetPage.attrs['href'] if targetPage not in self.visited: self.visited.append(targetPage) if not self.site.absoluteUrl: targetPage = '&#123;&#125;&#123;&#125;'.format(self.site.url, targetPage) self.parse(targetPage)reuters = Website('Reuters', 'https://www.reuters.com', '^(/article/)', False, 'h1', 'div.StandardArticleBody_body_1gnLA')crawler = Crawler(reuters)crawler.crawl() 这里还有一个以前示例中没有使用的更改：Website对象(在本例中是变量reuters)是Crawler程序对象本身的属性。这对于在爬虫程序中存储已访问页面(已访问visited)很有效，但是这意味着必须为每个网站实例化一个新的爬虫程序，而不是重用同一个爬虫程序来抓取一个网站列表。不管您是选择使爬行器网站不可知，还是选择使网站成为爬行器的一个属性，这都是一个设计决策，您必须根据自己的特定需求来权衡。两种方法都可以。 另一件需要注意的事情是，这个爬虫程序将从主页获取页面，但是在所有这些页面都被记录之后，它将不会继续爬行。您可能想编写一个包含第3章中的模式之一的爬行器，并让它在访问的每个页面上查找更多的目标。您甚至可以跟踪每个页面上的所有url(不仅仅是匹配目标模式的url)来查找包含目标模式的URLs。 Crawling Multiple Page Types不像爬行通过预定的一组页面，爬行通过一个网站的所有内部链接可以提出一个挑战，因为你永远不知道你到底得到了什么。幸运的是，有一些基本的方法可以识别页面类型： By the URL 网站上的所有博客文章都可能包含一个URL(例如http://example.com/blog/titl-of-post)。 By the presence or lack of certain fields on a site 如果一个页面有日期，但是没有作者的名字，您可以将其归类为新闻稿。如果它有标题、主图像、价格，但没有主内容，那么它可能是一个产品页面。 By the presence of certain tags on the page to identify the page 即使没有在标记中收集数据，也可以利用标记。您的爬虫程序可能会查找诸如&lt;div id=&quot;relatedproducts&quot;&gt;这样的元素来将页面标识为产品页面，即使爬虫程序对相关产品的内容不感兴趣。 要跟踪多个页面类型，您需要在Python中拥有多个类型的页面对象。这可以通过两种方式实现： 如果所有页面都是相似的(它们的内容类型基本相同)，您可能想要向现有的网站页面对象添加pageType属性： 123456789class Website: """Common base class for all articles/pages""" def __init__(self, type, name, url, searchUrl, resultListing, resultUrl, absoluteUrl, titleTag, bodyTag): self.name = name self.url = url self.titleTag = titleTag self.bodyTag = bodyTag self.pageType = pageType 如果您将这些页面存储在类似sql的数据库中，这种类型的模式表明所有这些页面可能都存储在同一个表中，并且会添加一个额外的pageType列。 如果您正在抓取的页面/内容彼此之间有足够的差异(它们包含不同类型的字段)，这可能需要为每种页面类型创建新的对象。当然，有些东西对所有web页面都是通用的，它们都有一个URL，并且很可能还有一个名称或页面标题。这是使用子类的理想情况： 123456class Webpage: """Common base class for all articles/pages""" def __init__(self, name, url, titleTag): self.name = name self.url = url self.titleTag = titleTag 这不是一个对象，您的爬虫程序将直接使用，但对象将被您的页面类型引用： 12345678910111213class Product(Website): """Contains information for scraping a product page""" def __init__(self, name, url, titleTag, productNumber, price): Website.__init__(self, name, url, TitleTag) self.productNumberTag = productNumberTag self.priceTag = priceTag class Article(Website): """Contains information for scraping an article page""" def __init__(self, name, url, titleTag, bodyTag, dateTag): Website.__init__(self, name, url, titleTag) self.bodyTag = bodyTag self.dateTag = dateTag 这个产品页面扩展了网站基类，并添加了只适用于产品的属性productNumber和price，而Article类添加了不适用于产品的属性body和date。您可以使用这两个类来抓取，例如，商店网站除了产品外，可能还包含博客文章或新闻稿。 Thinking About Web Crawler Models从互联网上收集信息就像从消防水管里喝水一样。外面有很多东西，你需要什么或者如何需要并不总是很清楚。任何大型web抓取项目(甚至一些小型web抓取项目)的第一步都应该是回答这些问题。 当跨多个域或从多个源收集类似数据时，您的目标几乎总是要将其标准化。处理具有相同和可比字段的数据要比处理完全依赖于原始源格式的数据容易得多。 在许多情况下，您应该构建scraper，并假定将来会向其中添加更多的数据源，并以最小化添加这些新数据源所需的编程开销为目标。即使一个网站乍一看不符合你的模型，它也可能以更微妙的方式符合你的模型。能够看到这些潜在的模式可以节省您的时间、金钱和很多头痛。 数据块之间的连接也不应该被忽略。你希望对于具有“类型”、“大小”或“主题”等跨属性的信息数据来源？如何存储、检索和概念化这些属性软件架构是一个广泛而重要的主题，可能需要整个职业生涯的主人。幸运的是，用于web抓取的软件架构要有限得多一组相对容易掌握的技能。当你继续刮除数据后，您可能会发现相同的基本模式反复出现。创建一个结构良好的web刮刀不需要很多神秘的知识，但是它确实需要花点时间退后一步想想你的项目。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-3]]></title>
    <url>%2F2019%2F07%2F14%2FWebScraping3%2F</url>
    <content type="text"><![CDATA[到目前为止，已经看到了带有一些静态页面。在本章中，将开始研究实际的问题，抓取会遍历多个页面，甚至多个站点。 Web爬虫之所以这样命名，是因为它们在Web上爬行。它们的核心是递归的一个元素。它们必须为URL检索页面内容，为另一个URL检查该页面，并检索该页面，一直到无穷。 但是要注意：仅仅因为你能在网上爬行并不意味着你总是应该这么做。在前面的示例中使用的爬虫在所有需要的数据都在一个页面上的情况下工作得非常好。使用爬虫程序时，必须非常注意所使用的带宽，并尽一切努力确定是否有一种方法可以使目标服务器的加载更容易。 Traversing a Single Domain应该已经知道如何编写检索任意值的Python脚本Wikipedia页面，并在该页面上生成链接列表： 1234567from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')bs = BeautifulSoup(html, 'html.parser')for link in bs.find_all('a'): if 'href' in link.attrs: print(link.attrs['href']) 其中，会出现一些你并不像出现的输出，例如： 12//wikimediafoundation.org/wiki/Privacy_policy//en.wikipedia.org/wiki/Wikipedia:Contact_us 事实上，Wikipedia充满了出现在每个页面上的边栏、页脚和页眉链接，以及指向类别页面、谈话页面和其他不包含不同文章的页面的链接： 12/wiki/Category:Articles_with_unsourced_statements_from_April_2014/wiki/Talk:Kevin_Bacon 最近，我的一个朋友在做一个类似的维基百科抓取项目时，提到他写了一个很大的过滤函数，有100多行代码，用来判断一个内部的维基百科链接是否是一个文章页面。不幸的是，他并没有花太多的时间去寻找两者之间的规律“文章链接”和“其他链接”，或者他可能已经发现了这个技巧。如果你查看指向文章页面的链接(相对于其他内部页面)，你会发现它们都有三个共同点: 它们驻留在div中，id设置为bodyContent 链接中不包含冒号 链接以/wiki/开头 可以使用正则表达式^(/wiki/)((?!:).)*$&quot;):使用这些规则稍微修改代码，只检索所需的文章链接： 12345678from urllib.request import urlopenfrom bs4 import BeautifulSoupimport rehtml = urlopen('http://en.wikipedia.org/wiki/Kevin_Bacon')bs = BeautifulSoup(html, 'html.parser')for link in bs.find('div', &#123;'id':'bodyContent'&#125;).find_all('a', href=re.compile('^(/wiki/)((?!:).)*$')): if 'href' in link.attrs: print(link.attrs['href']) 当然，一个脚本可以在一篇硬编码的Wikipedia文章中找到所有的文章链接，虽然很有趣，但在实践中却毫无用处。需要能够采取这段代码，并将其转换成类似于以下内容： 12345678910111213141516from urllib.request import urlopenfrom bs4 import BeautifulSoupimport datetimeimport randomimport rerandom.seed(datetime.datetime.now())def getLinks(articleUrl): html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(articleUrl)) bs = BeautifulSoup(html, 'html.parser') return bs.find('div', &#123;'id':'bodyContent'&#125;).find_all('a',href=re.compile('^(/wiki/)((?!:).)*$'))links = getLinks('/wiki/Kevin_Bacon')while len(links) &gt; 0: newArticle = links[random.randint(0, len(links)-1)].attrs['href'] print(newArticle) links = getLinks(newArticle) 在导入所需的库之后，程序要做的第一件事是用当前系统时间设置随机数生成器种子。这实际上确保了每次运行程序时，都能在Wikipedia文章中找到一个新的、有趣的随机路径。 接下来，程序定义getLinks函数，该函数接受表单/wiki/…，然后加上Wikipedia域名http://en.wikipedia.org，并在该域中检索HTML的BeautifulSoup对象。然后根据前面讨论的参数提取文章链接标记列表，并返回它们。 程序的主体从设置文章链接标签列表开始，然后进入一个循环，在页面中找到一个随机的文章链接标记，从中提取href属性，打印页面，并从提取的URL中获得一个新的链接列表。 Crawling an Entire Site在上一节中，随意浏览了一个网站，从一个链接到另一个链接。但是，如果需要系统地编目或搜索站点上的每个页面，该怎么办呢？爬行整个站点，特别是大型站点，是一个内存密集型的过程，最适合存储爬行结果的数据库随时可用的应用程序。但是，可以在不全面运行这些类型的应用程序的情况下研究它们的行为。 什么时候爬行整个网站是有用的，什么时候是有害的？遍历整个站点的抓取有很多好处，包括以下内容： 生成站点地图 几年前，我遇到了一个问题:一个重要的客户希望对网站的重新设计进行评估，但他不想让我的公司访问他们当前内容管理系统的内部结构，也没有公开可用的网站地图。我能够使用爬行器覆盖整个站点，收集所有内部链接，并将页面组织到站点上使用的实际文件夹结构中。这使我能够快速找到我甚至不知道存在的站点部分，并准确地计算需要多少页面设计和需要迁移多少内容。(不是我，是这个书的作者。。) 数据采集 我的另一个客户想收集文章(故事、博客文章、新闻文章等)，以便创建一个专门搜索平台的工作原型。虽然这些网站抓取不需要是详尽的，但它们确实需要相当广泛的(我们只对从几个站点获取数据感兴趣)。我能够创建爬虫程序，递归遍历每个站点，只收集在文章页面上找到的数据。 彻底搜索站点的一般方法是从顶级页面(如主页)开始，搜索该页面上所有内部链接的列表。然后爬行其中的每个链接，并在每个链接上找到附加的链接列表，从而触发另一轮爬行。但是这便是一个指数级的工作量！ 为了避免在同一个页面上爬行两次，非常重要的一点是，发现的所有内部链接都要保持一致的格式，并在程序运行时保存在一个运行集中，以便于查找。集合类似于列表，但是元素没有特定的顺序，只存储惟一的元素，这非常适合我们的需要。只有新的链接才应该被抓取和搜索额外的链接： 1234567891011121314151617from urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(pageUrl)) bs = BeautifulSoup(html, 'html.parser') for link in bs.find_all('a', href=re.compile('^(/wiki/)')): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print(newPage) pages.add(newPage) getLinks(newPage)getLinks('') 为了展示爬虫业务如何工作的全部效果，我放松了构成内部链接的标准(来自前面的示例)。它不是将scraper限制为文章页面，而是查找以/wiki/开头的所有链接，而不管它们在页面的什么位置，也不管它们是否包含冒号。记住：文章页面不包含冒号，但是文件上传页面、谈话页面等在URL中包含冒号。 最初，使用空URL调用getLinks。只要空URL前面加上http://en.wikipedia，这就被翻译为“Wikipedia的首页。然后，遍历第一个页面上的每个链接，并检查它是否在全局页面集中(脚本已经遇到的一组页面)。如果没有，则将其添加到列表中，并将其打印到屏幕上，然后在其上递归调用getLinks函数。 Collecting Data Across an Entire Site如果Web爬行器所做的只是从一个页面跳转到另一个页面，那么它们将非常无聊。为了使它们有用，需要能够在页面上做一些事情。让我们看看如何构建一个scraper，它收集标题、内容的第一段和编辑页面的链接(如果有的话)。 像往常一样，确定如何最好地做到这一点的第一步是查看站点的几个页面并确定一个模式。看看维基百科上的一些页面(包括文章和非文章页面，如隐私政策页面)，以下内容应该清楚: 所有标题(在所有页面上，无论它们作为文章页面、编辑历史页面或任何其他页面的状态如何)都有标题位于h1-&gt;span标记之下，这些是页面上惟一的h1标记。 如前所述，所有正文都位于div#bodyContent标记之下。但是，如果希望获得更具体的信息，并且只访问文本的第一段，那么最好使用div#mw-content-text→p(只选择第一段标记)。对于除文件页面(例如，https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg)之外的所有内容页面都是如此，这些页面没有内容文本的部分。 编辑链接只出现在文章页面上。如果它们发生了，它们将在li#ca-edit标签下的li#ca-edit→span→a中找到。 修改后，如下： 123456789101112131415161718192021222324from urllib.request import urlopenfrom bs4 import BeautifulSoupimport repages = set()def getLinks(pageUrl): global pages html = urlopen('http://en.wikipedia.org&#123;&#125;'.format(pageUrl)) bs = BeautifulSoup(html, 'html.parser') try: print(bs.h1.get_text()) print(bs.find(id ='mw-content-text').find_all('p')[0]) print(bs.find(id='ca-edit').find('span').find('a').attrs['href']) except AttributeError: print('This page is missing something! Continuing.') for link in bs.find_all('a', href=re.compile('^(/wiki/)')): if 'href' in link.attrs: if link.attrs['href'] not in pages: #We have encountered a new page newPage = link.attrs['href'] print('-'*20) print(newPage) pages.add(newPage) getLinks(newPage)getLinks('') 这个程序中的for循环本质上与原始爬行程序中的for循环相同(为了清晰起见，添加了打印破折号，将打印的内容分隔开)。 因为您永远无法完全确定所有的数据都在每个页面上，所以每个print语句都按照最有可能出现在站点上的顺序排列。也就是说，h1标题标签出现在每个页面上(至少就我所知)，所以您首先尝试获取数据。文本内容出现在大多数页面上(文件页面除外)，因此这是检索到的第二段数据。Edit按钮只出现在标题和文本内容都已经存在的页面上，但不会出现在所有这些页面上。 显然，在异常处理程序中封装多行涉及一些危险。首先，您不知道哪一行抛出了异常。此外，如果由于某种原因，一个页面包含一个Edit按钮，但是没有标题，那么Edit按钮将永远不会被记录。然而，在许多情况下，如果站点上出现了按顺序排列的条目，并且无意中丢失了一些数据点或保留了详细的日志，那么它就足够了。 重定向允许web服务器将一个域名或URL指向位于不同位置的内容块。有两种类型的重定向： server-side 服务器端重定向，即在加载页面之前更改URL client-side 客户端重定向，有时会出现“您将在10秒内重定向”类型的消息，在重定向到新消息之前，页面将加载到该消息 使用服务器端重定向，您通常不必担心。如果您在python3.x中使用urllib库，它自动处理重定向!如果您正在使用请求库，请确保将allow- redirects标志设置为True: 1r = requests.get('http://github.com', allow_redirects=True) 只是要注意，有时候，您正在爬行的页面的URL可能不是您输入页面的URL。 Crawling across the Internet在你开始编写一个爬虫程序，跟踪所有出站链接，不管你是否愿意，你应该问自己几个问题： 我要收集什么数据？这可以通过抓取几个预定义的网站来实现吗(几乎总是更容易的选项)，或者我的爬虫程序需要能够发现我可能不知道的新网站吗？ 当我的爬虫到达一个特定的网站，它会立即跟随下一个出站链接到一个新的网站，还是会停留一段时间，并深入到当前的网站？ 在什么情况下，我不想刮一个特定的网站?我对非英语内容感兴趣吗？ 如果我的网络爬虫在某个网站上引起了站长的注意，我该如何保护自己免受法律诉讼？ 因此，代码更新如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152from urllib.request import urlopenfrom urllib.parse import urlparsefrom bs4 import BeautifulSoupimport reimport datetimeimport randompages = set()random.seed(datetime.datetime.now())#Retrieves a list of all Internal links found on a pagedef getInternalLinks(bs, includeUrl): includeUrl = '&#123;&#125;://&#123;&#125;'.format(urlparse(includeUrl).scheme,urlparse(includeUrl).netloc) internalLinks = [] #Finds all links that begin with a "/" for link in bs.find_all('a', href=re.compile('^(/|.*'+includeUrl+')')): if link.attrs['href'] is not None: if link.attrs['href'] not in internalLinks: if(link.attrs['href'].startswith('/')): internalLinks.append(includeUrl+link.attrs['href']) else: internalLinks.append(link.attrs['href']) return internalLinks#Retrieves a list of all external links found on a pagedef getExternalLinks(bs, excludeUrl): externalLinks = [] #Finds all links that start with "http" that do #not contain the current URL for link in bs.find_all('a', href=re.compile('^(http|www)((?!'+excludeUrl+').)*$')): if link.attrs['href'] is not None: if link.attrs['href'] not in externalLinks: externalLinks.append(link.attrs['href']) return externalLinksdef getRandomExternalLink(startingPage): html = urlopen(startingPage) bs = BeautifulSoup(html, 'html.parser') externalLinks = getExternalLinks(bs,urlparse(startingPage).netloc) if len(externalLinks) == 0: print('No external links, looking around the site for one') domain = '&#123;&#125;://&#123;&#125;'.format(urlparse(startingPage).scheme,urlparse(startingPage).netloc) internalLinks = getInternalLinks(bs, domain) return getRandomExternalLink(internalLinks[random.randint(0,len(internalLinks)-1)]) else: return externalLinks[random.randint(0, len(externalLinks)-1)]def followExternalOnly(startingSite): externalLink = getRandomExternalLink(startingSite) print('Random external link is: &#123;&#125;'.format(externalLink))followExternalOnly(externalLink) followExternalOnly('http://oreilly.com') followExternalOnly('http://oreilly.com') 前面的程序从http://oreilly.com开始，从外部链接随机跳转到外部链接。下面是它产生的输出示例: 1234http://igniteshow.com/http://feeds.feedburner.com/oreilly/newshttp://hire.jobvite.com/CompanyJobs/Careers.aspx?c=q319http://makerfaire.com/ 外部链接并不总是保证能在网站的首页找到。在本例中，为了找到外部链接，使用了一种类似于前一个爬行示例中使用的方法来递归地深入到一个网站，直到找到一个外部链接。 具体的流程图，如下： 其中，有几个函数的意义需要解释一下： urlparse(link).scheme与 (urlparse(link).netloc两个解析网址的函数： 1234567link = "https://www.baidu.com/baidu?isource=infinity&amp;iname=baidu&amp;itype=web&amp;tn=02003390_42_hao_pg&amp;ie=utf-8&amp;wd=%E7%88%AC%E8%99%AB"print(urlparse(link).scheme)print(urlparse(link).netloc)output:httpswww.baidu.com 我一直提到这一点，但是为了空间和可读性，本书中的示例程序并不总是包含代码所需的必要检查和异常处理。例如，如果在爬行器遇到的站点的任何地方都没有找到外部链接(不太可能，但是如果运行足够长的时间，它一定会在某个时刻发生)，这个程序将继续运行，直到达到Python的递归限制。增加这个爬虫程序健壮性的一个简单方法是将它与第1章中的连接异常处理代码结合起来。这将允许代码在检索页面时遇到HTTP错误或服务器异常时选择不同的URL。 增加这个爬虫程序健壮性的一个简单方法是将它与章节中的连接异常处理代码结合起来这将允许代码在检索页面时遇到HTTP错误或服务器异常时选择不同的URL。 在为任何重要目的运行这段代码之前，请确保您进行了检查，以处理潜在的陷阱。 将任务分解为简单的函数(如“查找此页面上的所有外部链接”)的好处是，稍后可以轻松重构代码来执行不同的爬行任务。例如，如果你的目标是抓取整个网站的外部链接，并记录下每一个链接，你可以添加以下功能： 1234567891011121314151617# Collects a list of all external URLs found on the siteallExtLinks = set()allIntLinks = set()def getAllExternalLinks(siteUrl): html = urlopen(siteUrl) domain = '&#123;&#125;://&#123;&#125;'.format(urlparse(siteUrl).scheme,urlparse(siteUrl).netloc) bs = BeautifulSoup(html, 'html.parser') internalLinks = getInternalLinks(bs, domain) externalLinks = getExternalLinks(bs, domain) for link in externalLinks: if link not in allExtLinks:allExtLinks.add(link) print(link) for link in internalLinks: if link not in allIntLinks:allIntLinks.add(link) getAllExternalLinks(link)allIntLinks.add('http://oreilly.com')getAllExternalLinks('http://oreilly.com') 这段代码可以被看作是两个循环—一个收集内部链接，一个收集外部链接—彼此协同工作。流程图如下： 在编写代码之前，将代码应该做的事情记录下来或绘制图表，这是一种非常好的习惯，可以在爬行器变得越来越复杂时为您节省大量时间和挫折。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WebScraping-2]]></title>
    <url>%2F2019%2F07%2F13%2FWebScraping2%2F</url>
    <content type="text"><![CDATA[没什么好说的，觉得学的太基础，想快快学，但是在家就很懈怠，一直看剧看剧。。。 对了，忘记声明：此系列参照了《Web Scraping with Python, 2nd Edition》———Ryan Mitchell 假设你有一些目标内容。可能是一个名称、统计数据或文本块。也许它将20层标签在HTML代码中，没有任何有用的标签或HTML属性。假设您决定将警告抛到九霄云外，并编写类似下面这样的代码来尝试提取: 1bs.find_all('table')[4].find_all('tr')[2].find('td').find_all('div')[1].find('a') 看起来不太好。除了线条的美观之外，站点管理员对网站的任何微小更改都可能彻底破坏web scraper。 对于已下这种文本，可以用find_all()来提取绿色的文本： 12&lt;span class="red"&gt;Heavens! what a virulent attack!&lt;/span&gt; replied&lt;span class="green"&gt;the prince&lt;/span&gt;, not in the least disconcerted by this reception. 具体代码： 1234567from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page1.html')bs = BeautifulSoup(html.read(), 'html.parser')nameList = bs.findAll('span', &#123;'class':'green'&#125;)for name in nameList: print(name.get_text()) 其中，.get_text()从正在处理的文档中删除所有标记，并返回只包含文本的Unicode字符串。例如，如果您正在处理一个包含许多超链接、段落和其他标记的大文本块，那么所有这些都将被删除，只剩下一个无标记的文本块。请记住，在一个BeautifulSoup对象中查找要比在一个文本块中查找容易得多。在打印、存储或操作最终数据之前，调用.get_text()应该是您做的最后一件事。一般来说，应该尽可能长地保存文档的标记结构。 Another serving of BeautifulSoupfind/find_all() with BeautifulSoupfind()与find_all()非常相似。它们的细节如下： 12find_all(tag, attributes, recursive, text, limit, keywords)find(tag, attributes, recursive, text, keywords) tag是之前见过的，可以传递标记的字符串名称，甚至是字符串标记名称的Python列表。例如 1.find_all(['h1','h2','h3','h4','h5','h6']) attributes参数接受一个Python属性字典，并匹配包含其中任何一个属性的标记。例如，下面的函数将返回HTML文档中的绿色和红色span标记: 1.find_all('span', &#123;'class':&#123;'green', 'red'&#125;&#125;) recursive是一个 布尔值。您想要深入到文档的哪个部分?如果将recursive设置为True, find_all函数将查看children、children‘s children…用于匹配参数的标记。如果为False，它将只查看文档中的顶级标记。默认情况下，find_all是递归工作的(recursive设置为True)；一般来说，保持现状是一个好主意，除非真正知道需要做什么，并且性能是个问题。 text参数的不同寻常之处在于，它是基于标记的文本内容而不是标记本身的属性进行匹配的。例如，如果想在示例页面上找到“prince”被标记包围的次数，可以用以下行替换前面示例中的.find_all()函数： 12nameList = bs.find_all(text='the prince')print(len(nameList)) limit参数只在find_all方法中使用；find等价于相同的find_all调用，限制为1。如果只对从页面中检索前x项感兴趣，可以设置此选项。但是，请注意，这将按出现的顺序显示页面上的第一项，而不一定是您想要的第一项。 keyword参数允许您选择包含特定属性或一组属性的标记。例如： 1title = bs.find_all(id='title', class_='text') 这将返回class_属性中带有单词“text”和id属性中带有单词“title”的第一个标记。注意，按照惯例，id的每个值在页面上只能使用一次。因此，在实际中，这样一行字可能不是特别有用，应相当于下列案文: 1title = bs.find(id='title') keyword参数在某些情况下是有用的。然而，作为一个BeautifulSoup特性，它在技术上是多余的。请记住，任何可以使用关键字完成的事情也可以使用本章后面介绍的技术来完成(参见regular_express和lambda_express)。例如，以下是相同的： 12bs.find_all(id='text')bs.find_all('', &#123;'id':'text'&#125;) 此外，您可能偶尔会遇到使用keyword的问题，最明显的是当根据类属性搜索元素时，因为class在Python中是受保护的关键字。也就是说，class是Python中不能使用的保留字作为变量或参数名(与前面讨论的beautiful .find_all()关键字参数没有关系)。例如，如果您尝试以下调用，由于类的非标准使用，您将得到一个语法错误: 123456bs.find_all(class='green') File "&lt;ipython-input-1-6de1a859337f&gt;", line 7 bs.find_all(class = 'green') ^SyntaxError: invalid syntax 而以下便可以，加入一个下划线： 1bs.find_all(class_='green') 当然，以下也是可以的： 1bs.find_all('', &#123;'class':'green'&#125;) 回想一下，通过属性列表将标记列表传递给.find_all()就像一个“或”过滤器(它选择包含tag1、tag2或tag3…的所有标记的列表)。如果你有一个很长的标签列表，你可能会得到很多你不想要的东西。keyword参数允许您为此添加一个额外的“和”过滤器。 other BeautifulSoup objects到目前为止，已经在BeautifulSoup库中看到了两种类型的对象: BeautifulSoup objects 在前面的代码示例中看到的实例作为变量bs 在列表中检索，或通过在BeautifulSoup上调用find和find_all分别检索或向下搜索，如下: 1bs.div.h1 然而，库中还有另外两个对象，虽然不太常用，但仍然需要了解: NavigableString objects 用于表示标记内的文本，而不是标记本身(一些函数操作并生成navigablestring，而不是标记对象)。 Comment object 用于在注释标签中查找HTML注释，如： 1&lt;!----like this one----&gt; 这四个对象是在BeautifulSoup库中(此版本时)将遇到的惟一对象。 navigating treesfind_all函数负责根据标签的名称和属性查找标签。但是，如果需要根据文档中的位置查找标记，该怎么办？这就是树导航(navigating trees)派上用场的地方。之前我们的老师方法如下： 1bs.tag.subTag.anotherSubTag 现在，让我们看看如何向上、跨界和对角地导航HTML树。您将使用我们非常可疑的在线购物网站http://www.pythonscraping.com/pages/page3.html ，作为一个用于抓取的示例页面，如图下图所示： 具体的html树是这样的： children and descendants此时要区分children与 descendants。 children：是一个标签的孩子，例如，在上图中，tr是table的孩子。 descendants：是一个标签的后代，例如，在上图中，tr，th，td，img和span都是table的后代。 因此，对于一个标签，它的所有的children都是decendants，但反之未必。 通常，BeautifulSoup函数总是处理所选当前标记的descendants。例如，bs.body.h1选择主体标记的后代的第一个h1标记。它不会找到位于主体之外的标记。 同样的，bs.div.find_all(&#39;img&#39;)也是查找文档中的第一个div，并且返回此div的 descendants中的的所有img的标签。 如果你只想输出只是children的标签： 123456from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')for child in bs.find('table',&#123;'id':'giftList'&#125;).children: print(child) 如果是children，那么输出的，如下： 同样的，如果输出后代，只要把children换成descendants即可。 如果，是descendants，输入，类似如下 1234567891011121314151617&lt;tr class="gift" id="gift1"&gt; &lt;td&gt; Vegetable Basket &lt;/td&gt; &lt;td&gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! &lt;span class="excitingNote"&gt;Now with super-colorful bell peppers! &lt;/span&gt; &lt;/td&gt; &lt;td&gt; $15.00 &lt;/td&gt; &lt;td&gt; &lt;img src="../img/gifts/img1.jpg"/&gt; &lt;/td&gt;&lt;/tr&gt; 那么输出如下： 123456789101112131415161718192021222324252627&lt;td&gt; Vegetable Basket&lt;/td&gt;Vegetable Basket&lt;td&gt; This vegetable basket is the perfect gift for your health conscious (or overweight) friends! &lt;span class="excitingNote"&gt;Now with super-colorful bell peppers!&lt;/span&gt;&lt;/td&gt;This vegetable basket is the perfect gift for your health conscious (or overweight) friends!&lt;span class="excitingNote"&gt;Now with super-colorful bell peppers!&lt;/span&gt;Now with super-colorful bell peppers!&lt;td&gt;$15.00&lt;/td&gt;$15.00&lt;td&gt;&lt;img src="../img/gifts/img1.jpg"/&gt;&lt;/td&gt;&lt;img src="../img/gifts/img1.jpg"/&gt; next_siblings()有了子孙后代，当然还有同辈之间函数：next_siblings()，具体实例如下： 123456from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')for sibling in bs.find('table', &#123;'id':'giftList'&#125;).tr.next_siblings: print(sibling) 这段代码的输出是打印商品表中的所有产品行，除了第一行标题行。为什么标题行被跳过？对象不能是它们自己的兄弟姐妹。任何时候，只要对象有兄弟姐妹，该对象本身就不会包含在列表中。正如函数的名称所示，它只调用next sibling()。例如，如果您要选择列表中间的一行，并在其上调用next_sibling()，那么只会返回后面的兄弟姐妹。因此，通过选择标题行并调用next_sibling()，可以选择表中的所有行，而不需要选择标题行本身。 作为next_sibling()的一个补充，如果希望获得的同级标记列表的末尾有一个易于选择的标记，那么previous_sibling()函数通常会很有帮助。 parents在抓取页面时，您可能会发现，与查找标记的孩子或兄弟姐妹相比，查找标记的父母的频率要低一些。通常，当您以爬行为目标查看HTML页面时，首先要查看标签的顶层，然后找出如何深入到所需的确切数据块。然而，偶尔会发现自己处于一些奇怪的情况，需要BeautifulSoup的父类查找功能.parent和.parents。例如： 12345from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')print(bs.find('img',&#123;'src':'../img/gifts/img1.jpg'&#125;).parent.previous_sibling.get_text()) 这段代码将打印由该位置的图像表示的对象的价格../img/gifts/img1.jpg(本例中价格为15.00美元)。 具体过程如下： 1234567&lt;tr&gt; - td - td - td 《3》 - "$15.00" 《4》 - td 《2》 - &lt;img src="../img/gifts/img1.jpg"&gt; 《1》 《1》图像 ../img/gifts/img1.jpg首先被选择 《2》选择它的父母，此例中为td 《3》选择td的previous_sibling，此例中是价钱的那个td 《4》选择这个标签的文本”$15.00” Regular expressions这是一个很重要的模块！ 我用了正则字符串这个短语。什么是常规字符串?它是任何可以由一系列线性规则生成的字符串，例如: 至少出现一个字母a 再加上准确的5次字母b 再加上任何偶数个的字母c 以d或者e为结尾 用正则表达式写出：aa*bbbbb(cc)*(d|e) 符号 含义 举例 实例 * 匹配前面的字符，子表达式或括起来的字符，0或者更多次 a*b* aaaaa, aabbb, bbbb + 匹配前面的字符，子表达式或括起来的字符，1或者更多次 a+b+ aaaab, aaabb, abbb [] 匹配括号里的任何一个字符， [A-Z]* APPLE, HELLO () 变成一个子表达式(优先按照正则表达式的“操作顺序”计算这些值) (a*b)* aaabaab, abaaab, ababaaab {m,n} 匹配前面的字符，(优先按照正则表达式的“操作顺序”计算这些值) a{2,3}b{2,3} aabbb, aaabbb, aabb [^] 匹配任何一个不在括号的字符 [^A-Z]* apple, nihao I 匹配由I分隔的任何字符、字符串或子表达式 b(aIiIe)d bad, bid, bed . 匹配任何一个字符(包括符号，数字，空格等) b.d bad, b d, b$d ^ 指示字符或子表达式出现在字符串的开头 ^a apple, asdf, a \ 转义字符(这允许您使用特殊字符作为其字面含义) \.\I\\ .I\ $ 通常在正则表达式的末尾使用，它的意思是“将这个匹配到字符串的末尾”。没有它，每个正则表达式都有一个事实，”.*“在它的末尾，接受只有字符串的第一部分匹配的字符串。这可以看作类似于^符号。 [A-Z]*[a-z]*$ ABCabc, zzzyx, Bob ?! ”不包含。这是个奇怪的符号，紧接在一个字符(或正则表达式)前面，表示这个字符不应该在字符串的特定位置找到。这可能很难使用；毕竟，字符可能在字符串的不同部分找到。如果试图完全消除一个字符，请与连用^和$在两端。 ^((?![A-Z]).)*$ no-caps-here, $ymb0ls a4e f!ne 当然，这是简略版，还有很多的符号，在此就先不解释了。 Regular expressions and BeautifulSoup因此，如果想应用在BeautifulSoup函数中，已上例中，想要输出所有的 1&lt;img src="../img/gifts/img3.jpg"&gt; 在使用find_all(&quot;img&quot;)的时候，也会把其他的多余项加进来，代码如下： 123456789101112131415from urllib.request import urlopenfrom bs4 import BeautifulSoupimport rehtml = urlopen('http://www.pythonscraping.com/pages/page3.html')bs = BeautifulSoup(html, 'html.parser')images = bs.find_all('img',&#123;'src':re.compile('\.\.\/img\/gifts/img.*\.jpg')&#125;)for image in images: print(image['src']) output:../img/gifts/img1.jpg../img/gifts/img2.jpg../img/gifts/img3.jpg../img/gifts/img4.jpg../img/gifts/img6.jpg Accessing attributes标题翻译为：访问属性 到目前为止，已经了解了如何访问和过滤标签以及访问其中的内容。然而，通常在web抓取中，并不寻找标记的内容；你在寻找它的属性。这对于a之类的标记尤其有用，其中它所指向的URL包含在href属性中；或者img标记，其中目标图像包含在src属性中。 使用标签对象，可以通过调用以下命令自动访问Python属性列表: 1myTag.attrs 请记住，这实际上返回了一个Python dictionary对象，这使得检索和操作这些属性变得非常简单。例如，可以使用以下行找到图像的源位置： 1myImgTag.attrs['src'] Lambda expressionslambda表达式是作为变量传递给另一个函数的函数;您可以将函数定义为f(g(x) y)，甚至f(g(x),h(x))，而不是f(x, y)。 BeautifulSoup允许您将某些类型的函数作为参数传递给find_all函数。 唯一的限制是这些函数必须接受一个标记对象作为参数并返回一个布尔值。在这个函数中，BeautifulSoup遇到的每个标记对象都被求值，求值为True的标记被返回，其余的则被丢弃。 例如，下面的代码检索所有恰好具有两个属性的标签： 1bs.find_all(lambda tag: len(tag.attrs) == 2) 这里，作为参数传递的函数是len(tag.attrs) == 2。如果为真，find_all函数将返回标记。也就是说，它会找到具有两个属性的标签，例如： 12&lt;div class="body" id="content"&gt;&lt;/div&gt;&lt;span style="color:red" class="title"&gt;&lt;/span&gt; Lambda函数非常有用，你甚至可以用它们来替换现有的BeautifulSoup函数: 1bs.find_all(lambda tag: tag.get_text() =='Or maybe he\'s only resting?') 这也可以在没有lambda函数的情况下完成: 1bs.find_all('', text='Or maybe he\'s only resting?') 但是，如果您记住lambda函数的语法，以及如何访问标记属性，那么您可能再也不需要记住任何其他BeautifulSoup语法了! 因为所提供的lambda函数可以是返回True或的任何函数False值，您甚至可以将它们与正则表达式组合起来，以查找具有匹配特定字符串模式的属性的标记。]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Webscraping-1]]></title>
    <url>%2F2019%2F07%2F12%2Fwebscraping1%2F</url>
    <content type="text"><![CDATA[创建爬虫专题，完全出于自己的爱好兴趣，可能与学业无关。因此，所记录的会尽量精简，只作为我的学习笔记。 建议环境：python3，jupyter notebook 初入最初的样例如下，函数urlopen(url)：打开网站 123from urllib.request import urlopenhtml = urlopen('http://pythonscraping.com/pages/page1.html')print(html.read()) 输出是这样的： 1b'&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;A Useful Page&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;An Interesting Title&lt;/h1&gt;\n&lt;div&gt;\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n&lt;/div&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n 可以再通过BeautifulSoup对网站进行解析 12345678from urllib.request import urlopenfrom bs4 import BeautifulSouphtml = urlopen('http://www.pythonscraping.com/pages/page1.html')bs = BeautifulSoup(html.read(), 'html.parser')print(bs.h1)output:&lt;h1&gt;An Interesting Title&lt;/h1&gt; 其中，bs为： 1234567891011&lt;html&gt;&lt;head&gt;&lt;title&gt;A Useful Page&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;An Interesting Title&lt;/h1&gt;&lt;div&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.&lt;/div&gt;&lt;/body&gt;&lt;/html&gt; 此网页的结构如下： 123456- html -&gt; &lt;html&gt;&lt;head&gt;...&lt;/head&gt;&lt;body&gt;...&lt;/body&gt;&lt;/html&gt; - head -&gt; &lt;head&gt;&lt;title&gt;A Useful Page&lt;title&gt;&lt;/head&gt; - title -&gt; &lt;title&gt;A Useful Page&lt;/title&gt;- body -&gt; &lt;body&gt;&lt;h1&gt;An Int...&lt;/h1&gt;&lt;div&gt;Lorem ip...&lt;/div&gt;&lt;/body&gt; - h1 -&gt; &lt;h1&gt;An Interesting Title&lt;/h1&gt; - div -&gt; &lt;div&gt;Lorem Ipsum dolor...&lt;/div&gt; 因此对于这个实例，bs.h1、bs.html.body.h1、bs.body.h1、bs.html.h1，这四个的结果是相同的。 对于BeautifulSoup函数，第一个参数是指定的HTML文本，第二个参数是为了选择不同的解析器。其他的还有lxml、html5lib等，可以自行查询其利弊。 连接的可靠性与处理异常网络是混乱的。数据格式很差，网站崩溃，关闭标签丢失。最令人沮丧的经历之一在web抓取运行，在你睡觉的时候来抓取数据，在第二天却发现爬虫因为一些意想不到的错误数据格式而停止执行。在这种情况下，你可能会想咒骂创建网站的开发人员的名字(以及格式奇怪的数据)，但是你真正应该责备的是您自己，因为你一开始就没有预料到异常。 爬虫的第一步，就可能会有异常出现： 1html = urlopen('http://www.pythonscraping.com/pages/page1.html' 在服务器上找不到该页面(或者检索时出错)。 找不到服务器 HTTPError第一种情况，将返回一个HTTPerror，可能是404 Page Not Found，也可能是500 Internal Server Error等等。在所有这些情况下，urlopen函数都会抛出通用异常HTTPError，具体解决办法如下： 12345678910from urllib.request import urlopenfrom urllib.error import HTTPErrortry: html = urlopen('http://www.pythonscraping.com/pages/page1.html')except HTTPError as e: print(e) # return null, break, or do some other "Plan B"else: # program continues. Note: If you return or break in the # exception catch, you do not need to use the "else" statement 如果返回HTTP错误代码，程序现在打印错误，并且不执行else语句下程序的其余部分。 URLErrorno server如果根本找不到服务器(例如，”http://www.pythonscraping.com“ 宕机了，或者URL输入错误)，urlopen将抛出一个URLError。这表明根本无法访问任何服务器，而且由于远程服务器负责返回HTTP状态码，不能抛出HTTPError，必须捕获更严重的URLError。你可以添加一个检查，看看是不是这样: 123456789101112131415161718192021222324252627from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom urllib.error import URLErrortry: html = urlopen('https://pythonscrapingthisurldoesnotexist.com')except HTTPError as e: print(e)except URLError as e: print('The server could not be found!')else: print('It Worked!') output:The server could not be found!# be compared with HTTPError# ”http://www.pythonscraping.com“ works well but no page1000try: html = urlopen('http://www.pythonscraping.com/pages/page1000.html')except HTTPError as e: print(e) # this line runsexcept URLError as e: print('The server could not be found!')else: print('It Worked!')output:HTTP Error 404: Not Found no tag当然，如果从服务器成功检索到页面，仍然存在页面上的内容不完全符合你的预期的问题。每次访问BeautifulSoup对象中的标记时，添加一个检查以确保标记确实存在是明智的。如果你试图访问一个不存在的标签，BeautifulSoup将返回一个没有对象。问题是：试图访问None对象本身上的标记将导致抛出AttributeError。 print(bs.nonExistentTag)，其中，nonExistentTag是一个虚构的标记，而不是BeautifulSoup函数的真实名称。返回一个None对象。这个对象是完全合理的处理和检查。如果不检查它，而是继续尝试在None对象上调用另一个函数，print(bs.nonExistentTag.someTag)就会出现问题，如下所示: 1AttributeError: 'NoneType' object has no attribute 'someTag' 对于no tag的这两种情况，最简单的处理方式： 123456789try: badContent = bs.nonExistingTag.anotherTagexcept AttributeError as e: print('nonExistingTag was not found')else: if badContent == None: print ('anotherTag was not found') else: print(badContent) 对每个错误的检查和处理一开始看起来确实很费力，但是很容易在代码中添加一些重组，从而降低编写的难度(更重要的是，降低阅读的难度)。例如，这段代码是我们用稍微不同的方式编写的相同的scraper: 12345678910111213141516171819from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except HTTPError as e: return None try: bs = BeautifulSoup(html.read(), 'html.parser') title = bs.body.h1 except AttributeError as e: return None return titletitle = getTitle('http://www.pythonscraping.com/pages/page1.html')if title == None: print('Title could not be found')else: print(title) 在本例中，将创建一个getTitle函数，该函数将返回页面的标题，如果检索有问题，则返回一个None对象。在getTitle中，检查HTTPError，就像前面的示例一样，并将两个BeautifulSoup封装在一个try语句中。可以从这两行抛出AttributeError(如果服务器不存在、html将是一个None对象、html.read()将抛出AttributeError)。实际上，可以在一个try语句中包含任意多行，或者完全调用另一个函数，这是可以做到的。 以下为测试 12345678910111213141516171819202122232425262728293031from urllib.request import urlopenfrom urllib.error import HTTPErrorfrom urllib.error import URLErrorfrom bs4 import BeautifulSoupdef getTitle(url): try: html = urlopen(url) except URLError as e: return e except HTTPError as e: return e try: bs = BeautifulSoup(html.read(), 'html.parser') title = bs.body.h1 except AttributeError as e: return e return titletitle = getTitle('http://www.pythonscraping.com/pages/page1.html')print(title)output:&lt;h1&gt;An Interesting Title&lt;/h1&gt;title = getTitle('http://www.pythonscraping11111.com/pages/page1.html')print(title)output:&lt;urlopen error [Errno 11001] getaddrinfo failed&gt;title = getTitle('http://www.pythonscraping.com/pages/page10000.html')print(title)output:HTTP Error 404: Not Found]]></content>
      <categories>
        <category>Web-Scraping</category>
      </categories>
      <tags>
        <tag>Web-Scraping</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MaOEA/IGD]]></title>
    <url>%2F2019%2F03%2F29%2Fmoeaigd%2F</url>
    <content type="text"><![CDATA[这个还是关于IGD-indicator的多目标函数优化问题，多学学，长长见识。 IGD Indicator-based Evolutionary Algorithm for Many-objective Optimization Problems Yanan Sun, Member, IEEE, Gary G. Yen, Fellow, IEEE, and Zhang Yi, Fellow, IEEE Nadir Point可以分为如下三种：usurface-to-nadir, edge-to-nadir, and extremepoint-to-nadir。时间有限，没有细看引用的论文。 论文中定义了nadir point、worst point、ideal point。 对于m维目标函数 $y^{ext}_1,..,y^{ext}_m$ ， 使 $y^{ext}_i = f(x^{ext}_i) \ and \ x^{ext}_i = argmax_xf_i(x)$. nadir point as $z^{nad}_i = f_i(x_i^{ext})$ . worst point as $z^{w}_i = \max f_i(x)$ . ideal point as $z^*_i = \min f_i(x)​$. 这三个点的区别可以用下图形象的表示出来： 并且在本文中，所定义的极端点如下： z^{nad}_i = \min|f_i(x)| + \lambda \sum_{j=1,j \ne i}^m (f_j(x))^2其中，$\lambda​$ 大于 1。具体样子如下： A与B点为极端点extreme points，C与D点为坏点worse ponits。其中当最小化 $|f_1(x)|+\lambda(f_2(x))^2$ 时， 因为 $\lambda$ 大于 1$，所以 (f_2(x))^2$ 具有更高的优先级，解会在线段BC上，又因为要使 $|f_1(x)|$ 最小，因此B点为极端点。另一个维度同理获得。 具体算法： $IGD^+-EMOA$先介绍一下IGD： IGD =\frac{ \sum_{p \in p^*} dist(p,PF)}{|p^*|}其中，$p^*​$为参考点，$dist(p,PF)​$ 为 p 到离 PF 最近的个体 $y​$ 的欧式距离，$d(p,y)=\sqrt{\sum_{j=1}^m(p_j-y_j)^2}​$。 而 $IGD^+$ 则是在 $d(p,y)$做出改变： d(p,y)=\sqrt{\sum_{j=1}^m\max(y_j-p_j,0)^2}翻译一下：在求平方和时，仅考虑个体比参考点数字大(性能差)的维度。 产生一致性点和NSGA-III不同，NSGA-III是归一化了种群的每一个个体，使极端点形成的平面在各个轴上的截距均为1，总结来说，就是一致性点不变，而改变个体。 此算法相反，修改了每一个一致性点的尺度，种群的每一个个体不变，以此来适应个体。 分配等级与近似距离the rank values are used to distinguish the proximity of the solutions to the Utopian PF from the view of reference points the proximity distances are utilized to indicate which individuals are with better convergence and diversity in the sub-population in which the solutions are with the same rank values. assign rank等级分为三种：$r_1,r_2,r_3$.并且都是个体与参考点 $p^*$ 之间的比较关系。 $r_1​$ ：个体 $s​$ 点至少支配 $p^*​$ 中的一个解。 $r_2$ ：个体 $s$ 点与所有的 $p^*$ 都是非支配关系。 $r_3​$ ：个体 $s​$ 点受 $p^*​$ 部分支配，另一部分的 $p^*​$ 与 $s​$ 非支配。 其中，$p^*$ 类似于下图这种，但要经过各个维度的变换来适应的当前种群。那么在这个空间中，任意取出一点，一定可以满足上面三种中的一个，并且可以直观的看到，优先级：$r_1 &gt; r_2 &gt; r_3$。 具体的说，如果一个最小问题的PF是一个超平面，那么Utopian PF显然等于PF。结果，位于PF的pareto-optimal解全都非支配于从Utopian PF抽样取出的参考点。又由上面所定义的那样，可知pareto-optimal都是 $r_2$。同理，对于convex PF 都是 $r_1$。对于concave PF 都是 $r_3$。 assign proximity distance就像上面所说那样，proximity distance是在 rank 相同的前提下来区分个体的好坏(收敛性)。并定义： $d^i_j$ 是第 $i$ 个个体，对第 $j$ 个参考点的距离。因此 $d$ 矩阵的尺寸为: q x k，(q个种群，k个参考点) 对于第 $i$ 个个体，对第 $j$ 个参考点: r_1 \rightarrow d^i_j = -\sqrt{\sum_{l=1}^m(f_l(x^i)-(p^*)^j_l)^2}\\ r_2 \rightarrow d^i_j = \sqrt{\sum_{l=1}^m\max (f_l(x^i)-(p^*)^j_l,0)^2}\\ r_3 \rightarrow d^i_j = \sqrt{\sum_{l=1}^m(f_l(x^i)-(p^*)^j_l)^2}等级1、3很好理解，解释一下为何2是这样的，如下图： $x^1,x^2,x^3$ 都是 $r_2$(非支配于那三个参考点)，如果单纯的计算欧式距离，那么可以看出 $x^1$ 是最好的解，但是直观上来看，$x^2$ 更有前景收敛于PF。但如果 $r_2$ 那种计算方式，便可得到 $x^2$ 更优。 总结来说，越小的proximity distance表现了更好的估计当待优化的PF未知时。具体算法流程如下： 后代产生Step 1：从当前种群上填充基因池直至满 Step 2：从基因池中选择两个父代，并且从基因池中把他们删除 Step 3：用所选择的父代通过 SBX 操作去产生后代 Step 4：在所产生的后代中变异 Step 5：重复Steps 2-4直至基因池满 具体算法如下： 注意到，其中SBX与多项式变异也有一定的要求。 Generally, two ways can be employed to solve this problem. One is the mating restrictionmethod to limit the offspring to be generated by the neighbor solutions. The other one is to use SBX with a large distribution index. 环境选择算法流程如下： 其中11行：找到A个个体，这些个体满足整体对于参考点 $r$ 有最小的proximity distance，需要一个 linear assignment problem(LAP)。 讨论选择压力的损失是传统的MOEAs解决MaOPs的主要问题，因为传统的个体间的支配比较会给出很大部分的非支配解集。在所提出的算法中，所有个体间的支配关系与通过IGD来计算的所需的参考点相比较。可是，在PF中均匀分布的参考点很难获得。基于此，就在Utopian PF中均与采样出一些点来。而已，为了解决因为参考点不准确的问题，基于它们对所估计出的参考点的支配关系而设计了三种计算距离的方法。我们希望，越小的proximity distance意味着对应的个体有更好的估计。特别地，如果等级为 $r_2$ 的个体，仍然与 $r_1$ 和 $r_3$ 就算距离的方式相同，那么收敛性就会损失。 代码代码最先不断的循环找到nadir point 12345678910DNPE = Global.ParameterSet(100*Global.N);while Global.NotTermination(Population) &amp;&amp; Global.evaluated &lt; DNPE Offspring = GA(Population(randi(end,1,Global.N)),&#123;0.9,20,1,20&#125;); % Off = GA(P,&#123;proC,disC,proM,disM&#125;) dis = distribution index Population = [Population,Offspring]; [~,rank] = sort(Fitness(Population.objs),1); Population = Population(unique(rank(1:ceil(Global.N/Global.M),:)));end% --------------------------------------------% 可以看到：交叉的概率0.9，交叉的参数20，变异的概率1，变异的参数20，非常大的数值了 种群依次为：]]></content>
      <categories>
        <category>indicator-based</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NSGA-III]]></title>
    <url>%2F2019%2F03%2F28%2Fnsgaiii%2F</url>
    <content type="text"><![CDATA[这个算法是慕名而来，之前学习的两个算法都用到了一致性点作为参考点，而这个算法也是比较经典的算法，就来学习学习。 An Evolutionary Many-Objective Optimization Algorithm Using Reference-point Based Non-dominated Sorting Approach, Part I: Solving Problems with Box Constraints Kalyanmoy Deb, Fellow, IEEE and Himanshu Jain 可以看到还是先一层一层的留下(收敛性)，如果数量有多余的，需要从加入一些特殊的规则，保持住多样性。多样性的话，就要有对应的参考点，然后利用个体与参考点的关系，选择出更有前景的个体，直至个体数到规定个数。 参考点选择 参考点就是在以ideal point为原点，在m维空间上，确定一个m-1的平面，均匀分布的点，个数可参考如下公式，用之前学过的概率论的格式，就是那个 $C$ 的样子，一般的问题是已知 $m$ (目标函数个数)、种群数，我要找到与种群数与相接近的 $H$。此时一下公式的 $p$ 便为未知量， $p$ 在几何中的意义就是，把坐标平均分成 $p$ 份，上图 $p=4$， $m=3$，$H=15$。 H =\left( \begin{matrix} M + p-1 \\ p \end{matrix} \right)=\left( \begin{matrix} M + p-1 \\ M - 1 \end{matrix} \right) = C^{p}_{M+p-1}解释：这个用到了高中学过的小球问题，有 $p$ 个不作区分的小球放到 $m$ 个作区分的篮子里，篮子可以可以有多少种情况，数学公式为： f_1 + f_2+... + f_m = p \\ f_i \geq 0 \ and \ f_i \in Z,\ \forall i \in [1,m]从而也可以转化为： f_1 + f_2+... + f_m = p + m \\ f_i \geq 1 \ and \ f_i \in Z,\ \forall i \in [1,m]可以想象有 $p+m$ 个小球排成一排，因为每个篮子里均要有小球，因此这些小球形成的间隔共有 $p +m -1$ 个，共放 $m-1$个隔板，因此会得出结果。 平面确定这样如果确定了平面，我们可以求得一致性点，那么如何求出这个平面。 以 $m=3$ 为例，也就是说，通过某种规则，找到这三个点，$z^{1,max},z^{2,max},z^{2,max}$ 组成的平面来找一致性点。再求出与坐标轴形成的截距：$a_1,a_2,a_3$ 。 下图为实例：黑色的为 $z^{1,max},z^{2,max},z^{2,max}$ 。 那 $z^{1,max},z^{2,max},z^{2,max}$ ,怎么求呢，先介绍一个函数，在MOMBI-II也用到了，但是策略不同： ASF(x,W) = \max_{i=1}^{M}f'_i(x)/w_i \quad for \ x \in S_t理解：所谓极端点就是找到在一个维度上很大，在另外两个维度上的值很小的个体。 假设我要求出 $z^{1,max}$ ，我就在每一个维度上除以1、1e6、1e6。这样我就可以抽出另两个维度的目标值，并取出最大的那个目标值，为什么要取出最大的呢？因为我要找到除了第一维度另两个维度都很小的值，第一次要取出最大的，再对每一个个体的最大的那个取出最小的，那么这个个体的另一个目标值(除了第一维度的那个目标值)肯定会更小，这样才能保证另外两个维度上的值很小。具体操作如下line-4： 翻译一下就是： 对于每一个维度操作 找到此维度最小的值 此维度上都减掉它 求出$z^{j,max}$ 计算截距 每一个点都要$\frac{f_i(x)-z_i^{min}}{a_i - z_i^{min}}$ 这步相当于把这个超平面的截距都变成成(1,…,1) 那么我知道了这三个极端点，如何求出截距呢？如下： 以三维为例： 平面方程式为：$ax + by + cz + d = 0$ 。 我想求的截距便为： \left[ \begin{matrix} -d/a \\ -d/b \\ -d/c \end{matrix} \right]此时我们已知三个点坐标，也就可以知道，下面的方阵： \left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right] \left[ \begin{matrix} a \\ b \\ c \end{matrix} \right] =\left[ \begin{matrix} -d \\ -d \\ -d \end{matrix} \right]继续化简： \left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right] \left[ \begin{matrix} -a/d \\ -b/d \\ -c/d \end{matrix} \right] =\left[ \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right]因此： \left[ \begin{matrix} -a/d \\ -b/d \\ -c/d \end{matrix} \right] =inv(\left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right])\left[ \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right] =\left[ \begin{matrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{matrix} \right] \backslash \left[ \begin{matrix} 1 \\ 1 \\ 1 \end{matrix} \right]然后每一位均取倒数即可。高维度($m \geq 3$)的依此类推。 注意，如果矩阵E的秩小于m，那么这m个极限点就不能构成一个m维的超平面。甚至即使超平面能够建立，也可能在某些方向上得不到截距或某些截距 $a_i$ ，不满足 $a_i &gt; z^*_i$。在所有上述情形下，对于每一个 $i \in \{ i,…,m\}$ ，$z^{nad}_i$设置维 $S_t$ 中的非支配解在目标 $f_i$ 上的最大值。 在MATLAB代码中，如下： 12345678910Extreme = zeros(1,M);w = zeros(M)+1e-6+eye(M);for i = 1 : M [~,Extreme(i)] = min(max(PopObj./repmat(w(i,:),N,1),[],2));endHyperplane = PopObj(Extreme,:)\ones(M,1);a = 1./Hyperplane;if any(isnan(a)) a = max(PopObj,[],1)';end 个体对参考点链接效果如下： 算法流程： 此时这个超平面的各各截距均为1，每一个个体也都适应性拉伸，此时原点与一致性点连接所形成的射线，对于每一个 $S_t$ 个体，找到离它最近的射线，测量距离，如上上图，并对这些个体记录那个参考点，距离是多少。 并且记录 $P_{t+1} = S_t/F_l$ 在每个参考点周围的个数。这很重要！！ j \in Z^r:\rho_j = \sum_{S \in S_t/F_l}((\pi(s)=j)?1:0)选择机制流程如下： 为看着直观，下图为，$S_t,P_{t+1},F_l$ 之间的关系，但仅仅是为了理解，作图很不严谨！ 记录 $P_{t+1} = S_t/F_l$ 在每个参考点周围的个数。这很重要！！ j \in Z^r:\rho_j = \sum_{S \in S_t/F_l}((\pi(s)=j)?1:0) 找到计数最少的参考点集，并随机选择一个 —— 可能因为少的地方更需要开拓空间 找到离此参考点最近的那些 $F_l$ 中个体 —— 更能保证最后个体的一致性 如果有 $F_l$ 这些个体 —— 存在添加的可能性，不然去哪加新个体 这个参考点周围没有 $P_{t+1}$ 中的个体 —— 原来的个体中没有在这参考点附近的 选择 $F_l$ 中离此参考点最近的个体 —— 当然要找离它最近的 这个参考点周围有 $P_{t+1}$ 中的个体 —— 原来的个体中有在这参考点附近的 随便选一个$F_l$ 中在此参考点周围的个体 —— 那就随便找好啦 更新参数 没有 $F_l$ 这些个体 —— 不存在添加的可能性 以后不会考虑这个参考点了 —— 当然不会管这个参考点 重复以上操作直至选择的数量够了。注意，添加进去的点，就会默认在 $P_{t+1}$ 中了。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOMBI-II]]></title>
    <url>%2F2019%2F03%2F21%2Fmombiii%2F</url>
    <content type="text"><![CDATA[本想在复现的上一个算法AR-MOEA中加上一点小修改，我思来想去觉得每一个步骤都无懈可击。。。于是我意识到可能是论文读的太少，于是又选了一个indicator-based的多目标优化算法学习一下——MOMBI-II，这个算法过程较AR-MOEA比较简单，此算法选择的indicator是R2但效果也不错，学习学习。 Improved Metaheuristic Based on the R2 Indicator for Many-Objective Optimization Raquel Hernandez Gomez, Carlos A. Coello Coello 规定the ideal objective vector：$z^*_i = \min_{\vec{x}}f_i(\vec{x})​$ the nadir objective vector: $z^{nad}_i = \max_{P^*}f_i(\vec{x})$ indicator: R2 R2(A,U) = \frac{1}{U}\sum_{u \in U}u^*(A)其中，$u^*(A) = \min_{\vec{a} \in A} \{ u(\vec{a}) \}$，是在 $A$ 中最尤的效用值(utility value)。 一个测度叫做 achievement scalarizing function (ASF)： u_{asf}(\vec{v}:\vec{r},\vec{w}) = \max_{i \in \{1,...,m\} } \{\frac{|v_i - r_i|}{w_i} \}归一化： f'_i(\vec{x}) = \frac{f_i(\vec{x}) - z_i^{min}}{z_i^{max}-z_i^{min}} \\ \forall i \in \{1,...,m\}算法流程 代码借鉴了PlatEMO 1234567891011121314151617181920212223242526[alpha,epsilon,recordSize] = deal(0.5,0.001,5);%% Generate random population[W,N] = UniformPoint(N,M);Population = Initialization();% Ideal and nadir pointszmin = min(Population_objs,[],1);zmax = max(Population_objs,[],1);% For storing the nadir vectors of a few generationsRecord = repmat(zmax,recordSize,1);Archive= Population_objs;% For storing whether each objective has been marked for a few% generationsMark = false(recordSize,M);% R2 ranking procedure[Rank,Norm] = R2Ranking(Population_objs,W,zmin,zmax);%% Optimizationwhile Global.NotTermination(Population) MatingPool = TournamentSelection(2,N,Rank,Norm); Offspring = GA(Population(MatingPool)); Population = [Population,Offspring]; [Rank,Norm] = R2Ranking(Population_objs,W,zmin,zmax); [~,rank] = sortrows([Rank,Norm]); Population = Population(rank(1:Global.N)); Rank = Rank(rank(1:Global.N)); Norm = Norm(rank(1:Global.N)); [zmin,zmax,Record,Mark] = UpdateReferencePoints(Population_objs,zmin,zmax,Record,Mark,alpha,epsilon); R2Ranking.m 12345678910111213141516function [Rank,Norm] = R2Ranking(PopObj,W,zmin,zmax) N = size(PopObj,1); NW = size(W,1); %% Normalize the population PopObj = (PopObj-repmat(zmin,N,1))./repmat(zmax-zmin,N,1); %% Calculate the L2-norm of each solution Norm = sqrt(sum(PopObj.^2,2)); %% Rank the population Rank = zeros(N,NW); for i = 1 : NW ASF = max(PopObj./repmat(W(i,:),N,1),[],2); [~,rank] = sortrows([ASF,Norm]); [~,Rank(:,i)] = sort(rank); end Rank = min(Rank,[],2);end UpdateReferencePoints.m 12345678910111213141516171819202122232425function [zmin,zmax,Record,Mark] = UpdateReferencePoints(PopObj,zmin,zmax,Record,Mark,alpha,epsilon) z = min(PopObj,[],1); % z* znad = max(PopObj,[],1); zmin = min(zmin,z); Record = [Record(2:end,:);znad]; v = Record(end-1,:) - znad; % 前一轮的zmax减去当前的zmax mark = false(1,length(zmax)); if max(v) &gt; alpha % 如果差值大到一定程度，直接赋值 zmax = znad; else for i = 1 : length(zmax) % 对每一个目标函数上的维度操作 if abs(zmax(i)-zmin(i)) &lt; epsilon zmax(i) = max(zmax); % 如果在i-th维度上 zmax与新的zmin很接近，max(zmax)直接赋值到此维度上 mark(i) = true; elseif znad(i) &gt; zmax(i) zmax(i) = 2*znad(i) - zmax(i);% 如果在i-th维度上，当前种群最大的大于zmax(i) mark(i) = true; elseif v(i)==0 &amp;&amp; ~any(Mark(:,i)) % 如果在i-th维度上,与之前没有变：差值为0，并且 第i列Mark全为0 zmax(i) = (zmax(i)+max(Record(:,i)))/2; % 此维度上Record的最大值与原来的zmax取平均值 mark(i) = true; end end end Mark = [Mark(2:end,:);mark];end 过程理解Ranking其中主要的代码就是这个： 1234567Rank = zeros(N,NW);for i = 1 : NW ASF = max(PopObj./repmat(W(i,:),N,1),[],2); [~,rank] = sortrows([ASF,Norm]); [~,Rank(:,i)] = sort(rank);endRank = min(Rank,[],2); 可以看到max中便是开头介绍的 $u_{asf}$ 算法，公式： u_{asf}(\vec{v}:\vec{r},\vec{w}) = \max_{i \in \{1,...,m\} } \{\frac{|v_i - r_i|}{w_i} \} 首先遍历每一个一致性点W(i,:)，此时因为已经归一化，因此 $\vec{r} = \vec{0}$，用每一个个体点除一致性点向量($\vec{v}$)，因为在一个循环中，W(i,:)是不变的，因此相当于对每一维度的轴进行线性拉伸(点除)。 对得到的矩阵每一行取最大，翻译一下就是找到可以包含住此点的最小边长的立方体(假设三维，高维同理)，此立方体的边长就是 $\max_{i \in \{1,…,m\} } \{\frac{|v_i |}{w_i} \}$，也可以说拉伸后据原点的最大棋盘距离。 紧接着就是sortrows，翻译一下：先对ASF这个向量排序，如果相同的话，再按向量Norm排序。 再对上面的输出索引sort，翻译一下：对[ASF,Norm]进行离散化(ACM中术语-小弘说)，赋值成1~size(ASF,1)的整数值。如果假设数值越小越好，那于对第 i 列的向量，每一个个体都赋予一个等级 最后min(Rank,[],2)翻译一下：查看每一个个体的历史纪录，取它曾经最好(小)的一次。 有必要解释一下主函数里的这两步： 12[Rank,Norm] = R2Ranking(Population_objs,W,zmin,zmax);[~,rank] = sortrows([Rank,Norm]); 根据进化进程，Rank中会有大量的[1 1 1...2 2 2...]这种重复的元素，也就是说，对于[1...1]对应的个体来说，他们都再某个一致性点向量W(i,:)的结果中当过最最小，都“优秀”过。因此他们都是1，对于Rank中都是2的个体，同理。从这个角度来说，同为一个等级的个体都很好，但由于尺寸限制，可能不能都保留，这个时候便可以进一步比较他们到原点的(欧式)距离，对于minimize问题，当然越小越好，因此sortrows([Rank,Norm])。 update reference point这个我理解不了，只能复述一下代码过程，复述内容见上方的代码注释。]]></content>
      <categories>
        <category>indicator-based</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator-based</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AR-MOEA]]></title>
    <url>%2F2019%2F03%2F21%2Farmoea%2F</url>
    <content type="text"><![CDATA[最近想要复现一个关于indicator-based的多目标优化算法，因此，选了一个容易入手的AR-MOEA算法，此算法，是由安徽大学的田野老师在2018年提出，选用的indicator是IGD。 An Indicator-Based Multiobjective Evolutionary Algorithm With Reference Point Adaptation for Better Versatility Ye Tian, Ran Cheng, Xingyi Zhang, Fan Cheng, and Yaochu Jin, Fellow, IEEE 算法规定定义：无贡献解 \nexists y \in Y \ satisfying \ dis(y,x') = \min_{x \in X} dis(y,x)其中，距离为欧式距离。例子如下图： 适应度为： IGD-NS(X,Y) = \sum_{y \in Y} \min_{x \in X} dis(y,x) + \sum_{x \in X^*} \min_{y \in Y} dis(y,x')符号规定： the population P contains the candidate solutions as final output the initial reference point set R is used to guarantee uniform distribution of the candidate solutions in P the archive A reflects the Pareto front and guides the reference point adaptation the adapted reference point set R‘ is used in the IGD-NS-based selection for truncating the population P 具体关系如下图： 算法伪代码 MatingSelection(P,R’)： 个体 p 的适应度，定义为： fitness_p = IGD-NS(P \backslash \{p\},R' ) 细节图示以下为我的个人想法与心得：声明此代码借鉴PlatEMO中的算法 主函数以下为例 1234567891011121314151617N = 100; % 种群个数D = 10; % 变量个数M = 3; % 目标个数name = 'DTLZ3'; % 测试函数选择，目前只有：DTLZ1、DTLZ2、DTLZ3[res,Population,PF] = funfun(); % 生成初始种群与目标值REF = UniformPoint(N,M); % 生成一致性参考解[Archive,RefPoint,Range] = UpdateRefPoint(res,REF,[]);PD_v = [];for i = 1:400 MatingPool = MatingSelection(Population,RefPoint,Range); %已修改 Offspring = GA(Population(MatingPool,:)); %已修改 Offspring_objs = CalObj(Offspring); [Archive,RefPoint,Range] = UpdateRefPoint([Archive;Offspring_objs([all(PopCon&lt;=0,2)],:)],REF,Range); [Population,Range] = EnvironmentalSelection([Population;Offspring],RefPoint,Range,N);endhold onplot3(PF(:,1),PF(:,2),PF(:,3),'g*') UniformPoint.m 此函数是产生一致性点，当参数如上图所示，那么，这些一致性点在目标空间中，分布如下： UpdateRefPoint.m，此算法就是根据目前的Archive，Poupulation和Reference points生成新的Archive与Reference points。 先要介绍一下AdjustLocation.m 图为此时刻下的调整之前的Reference Points：两个图是同一状态，只是视角不同 再加上蓝色的点，即为当前种群： 红色的点即为调整之后的Reference Points： 这么看可能会发现不了什么规律，其实根据公式也可以知道，如果开始时对所有点进行归一化，那么由原点连接每一个黑色的点，对应的调整后的点必定在这条射线上。就像算盘上的珠子一样，滑到与所有蓝色的点(当前种群)中与到射线最短的距离所对应的投影点上，也就是原论文中的$p \leftarrow argmin_{p \in P}||F(P)||sin(\vec{z^*r},F(p))$，如下图： (黄线穿过：原点-黑点-红点 ) 只画出部分射线： AdjustLocation解释完了，就容易理解RefPointAdaption.m了 注意一点，在RefPointAdaption.m中的Reference Points是一致性点，所以，应该是这样的： 先对当前解、Reference点进行归一化，简单地说就是为了让射线从原点出发。 把ReferencePoint根据当前种群Archive进行调整，也就是上图的黑点变红点。 更新 Archive： 删除重复和受支配的解(因为又添加了交叉变异的个体) 滑动每一个Reference point到Archive中离它最近的的个体周围(映射点)，并且把这些新的个体称为 $A^{con}$，这样可以使得大多数个体多多少少周围都有几个Reference point，当然有的个体可能周围一个都没有。 如果个数不够就要从剩下的Archive中来凑，找在离newAchieve夹角最小中所有最大的那个$argmax_{p \in A \backslash A’} \min_{q \in A’} arccos(F(p),F(q))$。我猜测是为了增大多样性。 更新Reference Points： 根据之前找到的$A^{con}$ ，把离这些$A^{con}$ 中最近的Reference Point(调整后的)，作为newReferencePoint。 如果个数不够就要从新产生的的newArchive中来凑。 把newReferencePoint根据当前种群Population进行调整。 结果如下： 蓝色为一致性点 绿色为真实PF 红色为AR-MOEA算法结果 算法分析 首先，它是用了IGD-indicator来计算每一个个体的适应度。其次，选用了Archive与ReferencePoint来一起维护收敛与一致性。期间只要有靠近新的ReferencePoint，就有很大的概率被留下来，R’也会不断的接近R，其中在那个平面的点，说明此参考点周围已经子代存在，而呈拱形的点便是从A’\A中选择出来的，随着种群进化过程，平面上的点会逐渐变多，拱形上的点逐渐减少，最后便会得出均匀且收敛性较好的解集。]]></content>
      <categories>
        <category>indicator-based</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码学习]]></title>
    <url>%2F2019%2F03%2F15%2Flearningofcode%2F</url>
    <content type="text"><![CDATA[最近在复现AR-MOEA算法，虽说是复现，但目前的想法是从PlatEMO中把代码抠出来，我大致的看了一下代码，里面的代码很简洁，其中通过一些函数调用矩阵操作，从而大大的减少了for循环的使用。因此，想要通过这次解构AR-MOEA算法，储备一些常用的Matlab函数。另外，PlatEMO真的是神器！复现的很多很多算法，和常用的一些测试函数等等，很巧的是，AR-MOEA算法和PlatEMO是同一作者，哈哈哈哈。 说明一点：源代码用了很多结构体的东西，我都拆解了一下。。。这个和之前发过的Matlab常用函数可能有所重复，暂时没有想好究竟属于哪一部分，但重要的不是先记录下来吗？ 初始化种群binary：12345678910111213141516171819randi ：生成**均匀分布**的伪随机整数。randi(imax)：产生一个位于(0,imax]之间的整数。randi(imax,n,m)：产生一个位于（0,imax]之间的n*m的矩阵，所有元素都是整数。randi([imin,imax],n,m)：产生一个位于[imin,imax]之间的n*m的矩阵，所有的矩阵元素都是整数。&gt;&gt; randi(10)ans = 8 &gt;&gt; randi(5,2,3)ans = 1 4 3 4 5 1 &gt;&gt; randi([0,1],3,4)ans = 1 1 0 0 1 1 0 0 0 0 1 1 permutation12345678910111213rand：生成均匀分布的伪随机数。分布在（0~1）之间rand(m,n)生成m行n列的均匀分布的伪随机数&gt;&gt; rand(3,4)ans = 0.8328 0.7083 0.7038 0.5311 0.4554 0.3543 0.6873 0.4308 0.8578 0.1437 0.0276 0.8191 &gt;&gt; sort(ans,2)ans = 0.5311 0.7038 0.7083 0.8328 0.3543 0.4308 0.4554 0.6873 0.0276 0.1437 0.8191 0.8578 others1234567891011unifrnd(A,B)：A，B，ans均为同纬度矩阵，以A(i,j)为下界与B(i,j)为上界，均匀分布产生ans(i,j)。&gt;&gt; N = 5;&gt;&gt; lower = zeros(1,4);&gt;&gt; upper = ones(1,4);&gt;&gt; unifrnd(repmat(lower,N,1),repmat(upper,N,1))ans = 0.0145 0.4117 0.6161 0.6673 0.0026 0.5274 0.1487 0.3502 0.5629 0.2023 0.9411 0.8544 0.4124 0.6985 0.0984 0.1186 0.0801 0.0631 0.5249 0.7846 计算函数值此样例为：DTZL1，形式如下： f_1(x)=(1+g(x))x_1x_2 \\f_2(x)=(1+g(x))x_1(1-x_2) \\f_3(x)=(1+g(x))(1-x_1) \\where \ g(x)=100(n-2) +100\sum_{i=3}^{n}{\{(x_i-0.5)^2-cos[20\pi (x_i-0.5)]\}} \\x=(x_1,...,x_n)^T \in [0,1]^n,n=10比较简单的写法： 1234567891011121314ha = [];for ii = 1:N x = Population(ii,:); f=[]; sum=0; for i=3:D sum = sum+((x(i)-0.5)^2-cos(20*pi*(x(i)-0.5))); end g=100*(D-2)+100*sum; f(1)=(1+g)*x(1)*x(2); f(2)=(1+g)*x(1)*(1-x(2)); f(3)=(1+g)*(1-x(1)); ha(ii,:) = f / 2;end 高阶一点的写法： 123456M 为目标函数个数D 为变量维度g = 100*(D-M+1+sum((PopDec(:,M:end)-0.5).^2-cos(20.*pi.*(PopDec(:,M:end)-0.5)),2));PopObj = 0.5*repmat(1+g,1,M).*fliplr(cumprod([ones(N,1),PopDec(:,1:M-1)],2)).*[ones(N,1),1-PopDec(:,M-1:-1:1)];PopDec(:,M:end)-0.5).^2 = (x_i - 0.5)^2 ,i = 3:n 其中，cumprod 这个是第一次见： 123456789101112131415161718192021222324&gt;&gt; A = [1 2 3 4 5];&gt;&gt; B = cumprod(A) % A为向量连乘的形式B = 1 2 6 24 120 &gt;&gt; A = [1 4 7; 2 5 8; 3 6 9]A = 1 4 7 2 5 8 3 6 9&gt;&gt; B = cumprod(A) % 对矩阵A做列累积相乘B = 1 4 7 2 20 56 6 120 504 &gt;&gt; A = [1 3 5; 2 4 6] % 对矩阵A做行累积相乘A = 1 3 5 2 4 6&gt;&gt; B = cumprod(A,2)B = 1 3 15 2 8 48 fliplr 将矩阵A的列绕垂直轴进行左右翻转 matabc如果A是一个行向量，fliplr(A)将A中元素的顺序进行翻转。如果A是一个列向量，fliplr(A)还等于A。 1234567891011&gt;&gt; A =[1 4;2 5;3 6]A = 1 4 2 5 3 6&gt;&gt; fliplr(A)ans = 4 1 5 2 6 3 生成一致性参考点如下图： b = nchoosek(n,k) 返回二项式系数，定义为 n!/((n–k)! k!)。这就是从 n 项中一次取 k 项的组合的数目。说白了就是概率论里的 $C_n^k$ 。 C = nchoosek(v,k) 返回一个矩阵，其中包含了从向量 v 的元素中一次取 k 个元素的所有组合。矩阵 C 有 k 列和 n!/((n–k)! k!) 行，其中 n 为 length(v)。 1234567891011&gt;&gt; nchoosek(5,3)ans = 10&gt;&gt; v = 2:2:10;C = nchoosek(v,4) % 在[2 4 6 8 10] 这5个元素中取出4个的具体例子，列举出来C = 2 4 6 8 2 4 6 10 2 4 8 10 2 6 8 10 4 6 8 10 并且通过几行代码，即可生成均匀的参考点： 123456H1 = 1;while nchoosek(H1+M,M-1) &lt;= N H1 = H1 + 1;endW = nchoosek(1:H1+M-1,M-1) - repmat(0:M-2,nchoosek(H1+M-1,M-1),1) - 1;W = ([W,zeros(size(W,1),1)+H1]-[zeros(size(W,1),1),W])/H1; 交配池pdist2函数是求两个点集的欧式距离(默认) 1Cosine = 1 - pdist2(PopObj,RefPoint,'cosine'); % 此距离为夹角余弦距离,因为函数自带了 1-Cosine，因此要再减回去。 cosine夹角余弦距离： d_{s,t} = 1 - \frac{x_sx_t'}{||x_s||_2 . ||x_t||_2}true 生成logical的数组 12345K&gt;&gt; true(1,5)ans = 1×5 logical 数组 1 1 1 1 1 锦标赛选择varargin 表示用在一个函数中，输入参数不确定的情况，这增强了程序的灵活性。 例如：function g=fun(f,varargin) 然后在程序中使用时，假如在调用函数时，intrans(f,a,b,c)，那么：varargin{1}=a,varargin{2}=b,varargin{3}=c cellfun 是对cell进行操作的函数 12345678910&gt;&gt; C = &#123;1:10, [2; 4; 6], []&#125;&gt;&gt; cellfun(@mean, C)ans = 5.5000 4.0000 NaN &gt;&gt; days = &#123;'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'&#125;;&gt;&gt; cellfun(@(x) x(1:3), days, 'UniformOutput', false)ans = 1×5 cell 数组 'Mon' 'Tue' 'Wed' 'Thu' 'Fri' sortrows 默认依据第一列的数值按升序移动每一行，如果第一列的数值有相同的，依次往右比较。例： 就是说，把每一行看作整体，先从第一列升序排列，遇到相同的，就比较第二列按升序，如果还有相同的就比较第三轮升序,…….依次 123456789101112131415A = 95 45 92 41 13 1 84 95 7 73 89 20 74 52 95 7 73 5 19 44 20 95 7 40 35 60 93 67 76 61 93 81 27 46 83 76 79 91 0 19 41 1&gt;&gt; sortrows(A)ans = 76 61 93 81 27 46 83 76 79 91 0 19 41 1 95 7 40 35 60 93 67 95 7 73 5 19 44 20 95 7 73 89 20 74 52 95 45 92 41 13 1 84 连续用两个sort ，第一次见到这种操作，很有意思，把一组数进行替换，替换成他在这组数中的升序排名。如果原向量是适应度，那么就可以替换成1~N的序号 12345678910&gt;&gt; a = [1.1 3.1 5.1 7.1 4.1 2.1 6.1]&gt;&gt; [~,i] = sort(a)i = 1 6 2 5 3 7 4&gt;&gt; [~,ii] = sort(i)ii = 1 3 5 7 4 2 6% 原代码&gt;&gt; [~,rank] = sortrows([varargin&#123;:&#125;]);&gt;&gt; [~,rank] = sort(rank); 使用randi来进行随机选择 K 元锦标赛的序号 1Parents = randi(length(varargin&#123;1&#125;),K,N) % N 是种群个数 先通过rank(Parents)来通过序号找到对应的适应度值，用min 来找到每一列的最小值(函数输入时对适应度去负了) 1[~,best] = min(rank(Parents),[],1); 最后，best是一系列1、2组合的矩阵，再通过一下变换可求出结果 1index = Parents(best+(0:N-1)*K); 更新参考点unique 对向量去重，并且从小到大排序输出。 ismember 1234567891011121314&gt;&gt; a=[1 2 3 4 5];&gt;&gt; b=[3 4 5 6 7];&gt;&gt; c=[2 4 6 8 10];&gt;&gt; ismember(a,b) % a中的每一个元素是否在b中ans = 1×5 logical 数组 0 0 1 1 1 &gt;&gt; [lia,lib]=ismember(a,c) % a在lib中对应位置在c上的索引，如有多个，取第一个lia = 1×5 logical 数组 0 1 0 1 0lib = 0 1 0 2 0 交叉操作binary： 12345678% One point crossoverk = repmat(1:D,N,1) &gt; repmat(randi(D,N,1),1,D);k(repmat(rand(N,1)&gt;proC,1,D)) = false;Offspring1 = Parent1;Offspring2 = Parent2;Offspring1(k) = Parent2(k);Offspring2(k) = Parent1(k);Offspring = [Offspring1;Offspring2]; permutation： 1234567% Order crossoverOffspring = [Parent1;Parent2];k = randi(D,1,2*N);for i = 1 : N Offspring(i,k(i)+1:end) = setdiff(Parent2(i,:),Parent1(i,1:k(i)),'stable'); Offspring(i+N,k(i)+1:end) = setdiff(Parent1(i,:),Parent2(i,1:k(i)),'stable');end 其他： 12345678910% Simulated binary crossoverbeta = zeros(N,D);mu = rand(N,D);beta(mu&lt;=0.5) = (2*mu(mu&lt;=0.5)).^(1/(disC+1));beta(mu&gt;0.5) = (2-2*mu(mu&gt;0.5)).^(-1/(disC+1));beta = beta.*(-1).^randi([0,1],N,D);beta(rand(N,D)&lt;0.5) = 1;beta(repmat(rand(N,1)&gt;proC,1,D)) = 1;Offspring = [(Parent1+Parent2)/2+beta.*(Parent1-Parent2)/2 (Parent1+Parent2)/2-beta.*(Parent1-Parent2)/2]; 变异操作binary： 123% Bitwise mutationSite = rand(2*N,D) &lt; proM/D;Offspring(Site) = ~Offspring(Site); permutation： 12345678910% Slight mutationk = randi(D,1,2*N);s = randi(D,1,2*N);for i = 1 : 2*N if s(i) &lt; k(i) Offspring(i,:) = Offspring(i,[1:s(i)-1,k(i),s(i):k(i)-1,k(i)+1:end]); elseif s(i) &gt; k(i) Offspring(i,:) = Offspring(i,[1:k(i)-1,k(i)+1:s(i)-1,k(i),s(i):end]); endend 其他： 1234567891011121314 % Polynomial mutationLower = repmat(lower,2*N,1);Upper = repmat(upper,2*N,1);Site = rand(2*N,D) &lt; proM/D;mu = rand(2*N,D);temp = Site &amp; mu&lt;=0.5;Offspring = min(max(Offspring,Lower),Upper);Offspring(temp) = Offspring(temp)+(Upper(temp)-Lower(temp)).*((2.*mu(temp)+(1-... 2.*mu(temp)).*(1-(Offspring(temp)-Lower(temp))./... (Upper(temp)-Lower(temp))).^(disM+1)).^(1/(disM+1))-1);temp = Site &amp; mu&gt;0.5; Offspring(temp) = Offspring(temp)+(Upper(temp)-Lower(temp)).*(1-(2.*(1... -mu(temp))+2.*(mu(temp)-0.5).*(1-(Upper(temp)-... Offspring(temp))./(Upper(temp)-Lower(temp))).^(disM+1)).^(1/(disM+1))); hist函数： hist(X,Y)：X是一个事先给定的区间划分，统计Y在X这个区间划分下的个数，划分规则如下： 12345678910111213141516171819&gt;&gt; a = randi(5,1,10)a = 2 1 4 2 1 2 4 2 3 1&gt;&gt; k = hist(a,1:6)k = 3 4 1 2 0 0 b = [2.6 1 4 2 1 2 4 2 3 1];&gt;&gt; k = hist(b,1:6)k = 3 3 2 2 0 0 &gt;&gt; a = [2.5 -10 4 2 1 2 4 2 3 1];&gt;&gt; k = hist(a,1:6)k = 3 4 1 2 0 0 % ---------------------------------------------------------------- % 可以看到随机产生了1x10的[1~5]的整数向量a，那么函数结果为分别在:% (-inf 1.5],(1.5 2.5],(2.5 3.5],(3.5 4.5],(4.5 5.5],(5.5 inf) 用处： 1234567PopObj = [PopObj1;PopObj2];Distance = pdist2(PopObj,Ref); [d,pi] = min(Distance,[],2);rho = hist(pi(1:N_PopObj1),1:N_Ref);% -----------------------------------------------------------------% pi为PopObj到哪个Ref最近的index，rho即为离PopObj1种群最近的哪个Ref的对于所有Ref的计数量，% 可用于NSGA-III中]]></content>
      <categories>
        <category>matlab</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for All Quality Aspects]]></title>
    <url>%2F2019%2F03%2F06%2Fallaspects%2F</url>
    <content type="text"><![CDATA[一方面一拖再拖，一方面后面的 indicators 很多都是单独的一整篇论文，并非后面的一小部分，阅读并理解起来比较吃力，也有很多地方只看了算法步骤，而没有细看具体推导与证明，其中一篇，记忆很深刻，姚老师的 DoM-indicator 行文很严谨，并且断断续续看了四天才大概理解。 这类中的质量指标在文献中最常用，因为它们涵盖了解决方案集质量的所有四个方面。表2中的75-100项列出了这些QIs。一般可分为两类:基于距离的QIs(项目75-91)和基于容量的QIs(项目92-100)。 distance-based QIs： volume-based QIs： Distance-based QIs基于距离的QIs的基本思想是测量PF到所考虑解集的距离。因此，需要一个能很好地表示PF的参考集(reference set)。只有接近参考集的每个成员的解集才能有一个好的评估值，从而反映所有质量方面的收敛性、扩散性、一致性和基数性。这个想法可以通过平均(或累加)引用集的成员到解集中它们最近的解的距离，或者从这些距离中找到最大值来实现。对于前者，反向代际距离(IGD)是一个典型的例子，它考虑了平均值欧氏距离。其他的例子包括Dist1(D1)和一些IGD的变异。他们使用差异距离度量(如Tchebycheff distance和Hausdorff distance)，或者在评价中引入支配关系或附加点。 测量帕累托前沿到解集的最大差分(距离)可以很容易地识别出它们之间的差距，从而判断解集在前沿是否具有良好的覆盖性。Dist2(D2)和$\epsilon$-indicator就是这样的QIs。Dist2指标考虑Tchebycheff距离，而$\epsilon$-indicator考虑的是在目标上的最大区别：参考集的点优越于所考虑的解。与averaging difference-based QIs不同，maximum difference-based QIs可能具有明确的物理意义；例如，$\epsilon$-indicator是测量最小值添加到任何的解将使它被至少一个参考点集所weakly dominated。然而，他们的结果通常只涉及到一个特定目标的一个特解，因此自然会有大量的信息丢失。 最近提出了一种质量指标，称为优势移动(DoM)，它可以看作是上述两种QIs的组合。具体地说，给定两个解集A和B，A到B的DoM是移动A中一些点以便B中的任意一点都至少由A中的一点所支配。这种直观的指标具有许多可取的性质，如两种解决方案比较的自然延伸，符合帕累托优势，不需要问题知识和参数。然而，它的计算并不简单。虽然提出了一种双目标情况下的高效计算方法，但如何在有三个或三个以上目标的情况下高效计算仍有待探索。值得一提的是，早期的质量指标[126]可以看作是DoM的简化版本。它划分了引用集(即，$A\cup B$)为许多集群,将A对每个簇的最大差分累加。这使得计算变得高效，但是自然地失去了它的物理意义。 Inverted Generational Distance(IGD)它是最常用的指标之一，尽管之前提出了一些类似的观点。顾名思义，IGD是GD指标的反演，即测量从帕累托前沿到解集的距离。 形式上，给定解集A和参考集 $R=\{ r_1,r_2,…,r_M \}$ IGD(A,R) = \frac{1}{M} \sum_{i=1}^M \min_{a \in A} d_2(r_i,a)$d_2(r_i,a)$ 表示 $r_i$ 与 $a$ 的欧式距离。IGD值越低越好，说明该集合具有较好的收敛性、扩散性、均匀性和基数性的组合特性。 然而，IGD评估的准确性在很大程度上取决于参考集对帕累托前沿的逼近质量。不同的参考集可以使指标偏好不同的解集。通常建议对帕累托前沿采用高分辨率的大参考集。如[89,92]所示，集合中点数不足很容易导致反直觉的评价。此外，由优化器生成的所有非支配解决方案组成的引用集也可能导致误导结果，尽管这种做法在实际问题中得到了广泛采用。 Front to set distance看了好几遍，没有发现和IGD有什么不同，，，笨死了 为了度量MOEA的性能，我们只考虑运行MOEA所产生的最终总体中包含的所有非支配解决方案的子集。我们称这样的子集为近似集并用 $S$ 表示。近似集的大小取决于用于运行MOEA的设置。此指标计算离散帕累托最优集中每个解到近似集S中最近解的距离，并取平均值作为指标值： D_{P_F \rightarrow S}(S) = \frac{1}{|P_S|} \sum_{z^1 \in P_S}\min_{z^0 \in S}\{ d(z^0,z^1) \}由于我们感兴趣的是在目标空间中测量性能，两个多目标解 $z^0$ 和 $z^1$ 之间的距离就是它们的目标值 $f(z^0)$ 和 $f(z^1)$ 之间的欧氏距离。$D_{P_F \rightarrow S}(S) $ 指标既表示接近帕累托最优前沿的目标，也表示得到一个diverse、wide-spread 解决方案前沿的目标。这个性能指标的值越小越好。一个与此indicator密切相关的性能指标的hypervolume指标。在hypervolume指示器中，选择目标空间中的一个点，使其由需要测量的近似集中的所有点支配。然后，指示值等于由逼近集和所选参考点包围的多维区域的超体积。这个值是由近似集支配的目标空间中区域的指标。hypervolume 指示器和 $D_{P_F \rightarrow S} $ 指示器之间的主要区别在于，对于 hypervolume 指示器，必须选择一个参考点。不同的参考点导致不同的指示值。此外，不同的参考点可能导致指示值表明对不同的近似集的偏好。由于在 $D_{P_F \rightarrow S} $ 指标中使用了真正的帕累托最优前沿，因此 $D_{P_F \rightarrow S} $ 指标并不适用于此缺点。当然， $D_{P_F \rightarrow S} $ 指标的一个主要缺点是，在实际应用中，真正的帕累托最优前沿是未知的。在这种情况下，所有逼近集的帕累托前缘可以用来代替实际的帕累托最优前缘。 Delineation of Pareto Optimal Front介绍了描述度量 $\Phi$ 评价收敛性和多样性的程度一个已知的帕累托最优。本研究的目标是确定一组能够很好地表示帕累托最优。这个度量背后的思想是在帕累托最优前沿上的每个解能被已得到的非支配解有多好的表示出来。计算描述度量 $\Phi$ ，大量的H等间隔的解必须知道以反映真实帕累托最优前沿的帕累托最优集。用于计算的距离度量 $\gamma$ 同一组 $H$ 解决方案使用。从每个帕累托最优解到以获得的解 $l_i$ 的欧氏距离,这距离的平均值作为描述度量 $\Phi$,也就是说: \Phi(P_T) = \frac{1}{H} \sum_{i=1}^H l_i需要注意的是，在计算这个度量时，要考虑算法获得的所有解，包括那些受支配的解。 Dist1(D1)我们假设 $M$ 是一个好的 $R$ 的近似解，如果它可以给 $R$ 中的所有地区的重要信息，换句话说，如果对于每一个解 $y \in R$ ，这里都有一个比较接近的解 $x \in M$ 。我们建议用以下基于成就尺度函数的度量方法来度量两个解的亲密性： c(x,y) = \max_{j=0,...,J}\{0,w_j(f_j(y)-f(j(x) )\}$J$ 为目标函数的个数。因此，如果在所有目标上x达到解y的值，则测量值为0。否则，它取特定目标相对于y的最大加权偏差值。上述表达式中使用的权重设置为： w_j = 1 / \Delta_j其中，$\Delta_j$ 是在reference set的 $f_j$ 的范围。 Dist1=\frac{1}{card\{R\}} \sum_{y \in R} \{ min_{x \in M} \{c(x,y)\} \}Dist2(D2) Dist2= \max_{y \in R} \{ min_{x \in M} \{c(x,y)\} \}第一个度量给出关于 $y \in R$ 到 $M$ 中最接近解的平均距离的信息，而第二个度量给出关于最坏情况的信息。值越低，集合 $M$ 越接近集合 $R$。而且，$Dist2/Dist1$ 比值越低，从集合 $M$ 到集合 $R$ 的解分布越均匀。 $\epsilon$-indicator$\epsilon$-indicator 考虑sets之间的最大差异，它是受$\epsilon$-approximation所感，著名的测量设计和比较近似优化算法,运筹学和理论计算机科学。给定两个解集，$\epsilon$-indicator是一个集合在目标中被转换(以加法或乘法的方式)以弱支配另一个集合的最小因子。这就产生了两个版本:加法$\epsilon$-indicator和乘法$\epsilon$-indicator。数学上，解集A对于解集B的加法$\epsilon$-indicator定义如下： \epsilon_+(A,B) = \max_{b \in B} \min_{a \in A} \max_{j \in \{1...m\}} a_j - b_j$a_j$ 是 $a$ 的第 j 个目标，$m$ 是目标函数的个数。解集A对于解集B的乘法$\epsilon$-indicator定义如下： \epsilon_\times(A,B) = \max_{b \in B} \min_{a \in A} \max_{j \in \{1...m\}} \frac{a_j}{ b_j}这两个indicators都是越小越好。$\epsilon_+(A,B) \leq 0$ 或者 $\epsilon_\times(A,B) \leq 1$ 意味着 A weakly dominate B。当用代表PF的参考集R替换B时，$\epsilon$-indicator可以用作一元指标。它衡量的是被考虑的集合到帕累托前沿的距离。但是，由于返回的值只涉及两个集合中一个特解的一个特定目标(其中最大的差异)，指示器可能会忽略大量集合的差异。这可能导致不同执行的解决方案集具有相同/类似的评估结果。 (前提每个目标都是越小越好)对于加法拆解理解：设 $k = \min_{a \in A} \max_{j \in \{1…m\}} a_j - b_j$ 就是说对于B中指定一个解 $B_i$ ,把A中所有的解都减 $k$，那么A中至少(意味着min)存在一个解可以 weakly dominate $B_i$ ；如果遍历所有的$B$，那么需要取最大的那个 $k$，才能满足把 $A$ 中所有解都减掉 $k$ ，对于B任意一个解，A中都存在解可以 weakly dominate。对于乘法同理。 具体例子如图： 可知：$\epsilon_+(A_1,A_2) = 1$,$\epsilon_+(A_1,A_3) = 9/10$,$\epsilon_+(A_1,P) = 4$ $A_1$=(4,7),(5,6),(7,5),(8,4),(9,2) $A_3$=(6,8),(7,7)(8,6)(9,5)(10,4) 因为，想求 $\epsilon_+(A_1,A_3) $ 因此，先遍历 $A_3$ 中的元素，定性上说，在 $A_1$ 中里此元素越远，$k = \min_{a \in A_1} \max_{j \in \{1…m\}} a_j / b_j$越难被选上。 最后可以看到，(9,2) 与 (10,4) 的距离为标准，求得的结果，(10,4) 刚好在边缘上，且也可以看到，横轴间的距离差会比纵轴间的间隔会更大，因为横轴的数值就大。 ObjIGDObjective-wise Inverse Generational Distance(ObjIGD) ObjIGD度量评估MaOOA在每个目标上的收敛性和分布性能。ObjIGD的主要思想类似于IGD度量，然而ObjIGD测量的是PF与最接近的解决方案之间基于一个目标的距离。第i个目标的对象定义如下： ObjIGD_i(S,P)=\frac{ \sum_{j=1}^{|P|} \min_{s \in S}|F_i(p_j)-F_i(s)| }{|P|}$P$ 是 reference($PF_{true}$)。$S$ 是 $PF$ 近似集。$F_i(p_j)$ 是第$ i$ 个目标的第 $j$ 个解，$F_i(s)$ 是近似解的第 $i$ 个目标，因此，整体$ObjIGD$为： ObjIGD(S,P)=\frac{\sum_{i=1}^M ObjIGD_i(S,P)}{M}其中，$ObjIGD_i(S,P)$ 是第 $i$ 个目标的 $ObjIGD$ 的值，$M$ 是目标函数的个数。测度值越低，表明目标的收敛性和分布性越好。 IGD-NS在 IGD 计算中，我们经常发现，一些非支配解往往被忽略，因为它们不是均匀地从Pareto optimal front选取的计算 IGD 的任意参考点的最近邻。这意味着这些非支配解集中的解对集合的IGD值没有任何贡献，因此在逼近帕累托最优前沿方面，它们的重要性低于集合中其他非支配解。因此，我们将这些解称为非支配解集中的无贡献解(noncontributing)。具体地说，无贡献解的定义如下。 解 $y’$ 被认为在解集 $P$ 中，对于解 $P^*$是无贡献解，满足： \nexists x \in P^*:dist(x,y')=\min_{y \in P}dist(x,y)其中，$P^*$ 是一组参考点均匀采样的帕累托最优。从上面的方程,它可以学到无贡献解不是 $P^*$ 中任意点的最近邻点。 在考虑无贡献解的情况下，将提出的性能度量，即带无贡献解检测的IGD的度量(IGD-NS)，定义如下： IGD-NS(P,P^*)=\sum_{x \in P^*}\min_{y \in P} dis(x,y) + \sum_{y' \in P'}\min_{x \in P^*} dis(x,y')$P’$ 是population中无贡献解，上式的第一部分与IGD类似，控制了 $P$ 的多样性和收敛性；然而第二部分是对于每一个无贡献解到 $P^*$ 中点的最小距离的总和。因此，当且仅当满足以下两个条件时，可以得到一个较小(良好)的IGD-NS度规值:首先，种群具有良好的收敛性和多样性;第二，总体包含尽可能少的无贡献解。 个人理解：就是遍历 $P^*$ 中的每一个点，找到与此点距离最近的 $P$ 中的点都删掉，$P$ 中剩下的就是 $P’$ $IGD_p$ IGD_p(X,Y) = \left( \frac{1}{M}\sum_{i=1}^M dist(y_i,X)^p \right)^{1/p}这个就没有什么好说的了。。。 $\Delta_p$ \Delta_p(X,Y)=\max(GD_p(X,Y),IGD_p(X,Y))\\ =\max \left( \left( \frac{1}{N}\sum_{i=1}^N dist(x_i,Y)^p \right)^{1/p}, \left( \frac{1}{M}\sum_{i=1}^M dist(y_i,X)^p \right)^{1/p} \right)这个也是。。。。 $\epsilon$-performanceε-dominance是一个概念,用户可以指定他们想要的精度得到帕累托最优解决方案的多目标问题,在本质上给他们的能力来分配每个目标的相对重要性。这是通过应用一个网格(由用户指定大小的值)问题的搜索空间。$\epsilon$ 值较大导致巨大网格(和最终减少解决方案),而较小的 $\epsilon$ 值产生一个更精细的网格。每种解决方案的健身然后映射到一个盒子健身根据指定的 $\epsilon$ 值。 $\epsilon$-dominance适用于一套参考解根据用户指定的ε值 在每一代,匹配算法生成的每个解决方案,其相应的 $\epsilon$-nondominated参考集解。每个参考解只能有一个与之相关的算法解。如果存在多个解在 $\epsilon$-nondominated 参考集解，然后用欧氏距离最小的解来选择。这考虑了在参考解中的 $\epsilon$ 重叠地区，并且腾出额外的解与其他 $\epsilon$-nondominated参考解。 Each $\epsilon$-nondominated reference solution that has a corresponding algorithm solution receives a score of one, while each reference solution that has no corresponding algorithm solution receives a score of zero. ： \epsilon(P) = \sum_{i=1}^n h_i/n$h_i$ 是 对于 $\epsilon$-nondominated reference set 的第 $i$ 个解，并且 n 是reference set的个数。 这个指标测量了收敛性通过考虑,聚集在 $\epsilon$ 的引用集的解。多样性是占每个$\epsilon$-nondominated引用包括只有一个解决方案的解决方案,不管$\epsilon$-block额外的解的存在性。这可以确保集群解决方案不会对度量的计算产生影响。 说实话没太看懂，，，怎么个对应(corresponding algorithm)法子。 $I_{SDE}$ I_{SDE}(x,y) = \sqrt{\sum_{1 \leq i \leq m }sd(f_i(x),f_i(y))^2}其中： sd(f_i(x),f_i(y))=\begin{cases} f_i(y)-f_i(x) & if \ f_i(x) < f_i(y)\\ 0 & otherwise \end{cases}m 为目标函数个数。需要计算所有的 x 与 y 对。 PCI定义1：p 为一个点，Q为一组点$\{ q_1,q_2,..,q_k \}$ 。p 对 Q 的支配距离定义为p在目标空间中满足 p weakly dominate 所有的 Q 的最小距离： D(p,Q) = \sqrt{\sum_{i=1}^m(p^{(i)}-d(p^{(i)},Q))^2}其中： d(p^{(i)},Q) = \begin{cases} min\{ q_1^{(i)},q_2^{(i)},...,q_k^{(i)} \},&if \ p^{(i)} > min\{ q_1^{(i)},q_2^{(i)},...,q_k^{(i)} \}\\ p^{(i)},&otherwise \end{cases}$p^{(i)}$ 是 解 p 的第 i 个目标，m 为目标函数的个数。 $D(p,Q)$ 只考虑了 Q 中优于 p 的解，无关差于 p 的解。这可以使指标不受收敛性差的参考点的影响，如优势抵抗解。$D(p,Q)$ 的范围是0到无穷，越小越好。如果 p 在少数的目标函数中，轻微差于 Q ，$D(p,Q)$ 会很小，只有 p weakly dominate Q ：$D(p,Q)=0$ 易证： \max\{D(p,q_1),...,D(p,q_k)\} \leq D(p,Q) < D(p,q_1)+...+D(p,q_k)定义2： P，Q为两个解集。P 对 Q 的支配解集 $D(P,Q)$ 。定义如下：对于任意点 $q \in Q$ 中 P 的最小的总距离，使得至少有一个点 $p \in P$ weakly dominate q。 在该指标中，由于参考集由所有的近似集组成，因此一个聚类可以包含来自不同近似集的点。让一个聚类 C 包含 P 和 Q。$P = \{ p_1,…,p_i \}$ 与 $Q = \{ q_1,…,q_j \}$，显然 $D(P,C) = D(P,Q)$ 。当 $i=1$ 时，$D(P,C)$ 是 $p_1$ 对 C 的理想点的支配距离。当 $i \geq 2$ 时，$D(P,C)$ 可以小于 $\min\{D(p_1,C),…,D(p_i,C)\}$。 如上图：ideal point 代表了 每个cluster中每个函数的最小值 cluster $C_1$ ：$P_1$ 只有一个点，因此 $D(P_1,C_1) = (0.5^2 + 0.5^2)^{0.5} = 0.707$ cluster $C_2$ ：$P_2$ 有两个点，为 $D(P_2,C_2)= 0.559 &lt; \min\{1.031,1.25\}$ ,其中1.031与1.25是 $P_2$中的点分别到ideal point的点的距离。 cluster $C_3$： $P_3$ 是一个极端情况， $D(P_2,C_2)=0$，但是如果单个计算的话均为 1。 由此可以知道：当 $i \geq 2$ 时，$D(P,C)$ 可以小于 $\min\{D(p_1,C),…,D(p_i,C)\}$。这是因为共有 $i^j$ 种可能性，对于$p_1,p_2,…,p_i$ 去分开 $q_1,q_2,…,q_j$ ，就是对于每一个 $q$ 都有 $i$ 个可能性被 $p$ weakly dominate。因此，粗略计算如下： D'(P,C) = \max\{ \min\{ D(p_1,q_1),...,D(p_i,q_1) \},\\ ...\min\{ D(p_1,q_j),...,D(p_i,q_j) \} \}当 $i \geq 2$ 时，这仅仅有 $i \times j$ 个比较，尽管 $D’(P,C) \leq D(P,C)$ ，但是当 $C$ 的尺寸小时差距是很小的，例如：$D’(P_2,C_2) = 0.5 &lt; D(P_2,C_2)=0.559$ 与 $D’(P_3,C_3) = D(P_3,C_3) = 0$ 。 首先。所有的解集都要归一化， 如果评估的近似解小于两个，PCI考虑 the minimum move of one solution in the approximation set to weakly dominate the cluster (Step 8 否则计算the minimum move of the set’s solutions in the cluster to weakly dominate the cluster (Step 10 在 cluster 算法中：使用贪心的方法来逐步合并点根据他们的优势距离。设为归一化超平面上具有N个点理想分布的两个相邻点的区间(优势距离的意义)，其中N为参考集的大小。在这种情况下，$\sigma = 1/h$ 与 $N=C_{m-1+h}^{m-1}$ 其中，$h$ 为每个目标的分支，$m$ 为目标函数的个数，因为： (h+m-1)\times (h+m-2)\times ...\times (h+1) \approx (h + m/2)^{m-1}因此： \sigma \approx \frac{1}{\sqrt[m-1]{N(m-1)!}-(m/2)}G-Metric规定：$A_1,A_2,…,A_m$ 是 m 个NSs(non-dominated sets)： Scale the values of the vectors in the NSs Group the NSs by levels of complete outperformance For each level of complete outperformance and for every $A_i$ in the level, calculate the zone of infuence $I_{A_i}$ For every $A_i$, combine its convergence and DE to create a number that represents its relative performance respect to the other NSs Scale and normalization Take the union of the m sets, $C = \cup_{i=1}^m A_i$ From C take its non-dominated elements.$C^*=ND(C)$ Find $max_j$ and $min_j$ as the max and min value respectively, for the component j for all points $p \in C$ 暗指在known pareto front 中挑选。 Using $max_j$ and $min_j$ make a linear normalization of all points in all $A_i$ . Convergence Component已知 $D={A_1,A_2,…,A_m}$ ，其中 $A_i$ 是一个NS 令 $j=1$ 令 $L_j=\{\}$ 从 D 中提取出，并放入 $L_j$ 中，这些 $A_i$ 满足 $ \urcorner ((\bigcup_{A_k \in D }A_k) \ O_C \ A_i)$ 如果 D 不空，那么 j = j + 1，返回到第二步 结束 注意：这是以每一个NS作为整体的。 $L_1$ 是不能被 D 中除了 $L_1$ 的解所completely outperform。如果$A\in L_j$ ,$B \in L_k$ 并且 $j &lt; k$ ，我们可以知道 A 要好于 B，如下图，这有5个NSs ：A，B，C，D 和 E。我们分三个层次： $L_1 = \{A\}$，$L_2 = \{B,C\}$，$L_3 = \{D,E\}$ Dispersion–Extension Component定义1：$I_{p_i}$ 是一些据点 $p_i$ 的距离小于或等于一个正实数 $U$ 的一些点集，U 可以当作为半径。 定义2：$I_S$ 是 $I_{p_i}$ 的并集，$for \ all \ p_i \in S $ 一般来说(对于点或集合)，我们将影响区域称为I。 $I$ 的测量 $\mu(I)$ ：它是对一个点或NS的注入带的测量。它是对一个点或NS的测量。对于 2d 它意味着是面积，在 3d中意味着体积，依次类推。 如果 S 有一个较差的 DE(Dispersion–Extension)，那么他们中的许多都挨着很近，并且相互交叉，结果 $\mu(I_S)$ 就会变小。现在假设我们重新定位S的元素以改进DE。我们通过增加元素之间的扩展和距离，以及/或使它们的距离更均匀来实现这一点(如下图)。随着我们提高了 DE，$I_{p_i}$ 会下降，与此同时，DE 与 $\mu(I_S)$ 都会增加，因此，$\mu(I_S)$ 正比于DE，并且 $\mu(I_S)$ 是一个好的 DE indicator。 $I_S$ 也正比于 $I_{p_i}$ 之间的交叠。具有良好DE的NSs比具有不良DE的NSs重叠更少。 Computing the G–Metric已知 m 个 非支配解集，$A_1,A_2,…,A_m$ 归一化所有的解集 把所有的解集分类成$A_k$ for k = 1~Q ，其中，Q 是等级的数量。 对于每一个 $A_i \in L_k$ ，消除所有的点 $p \in A_i$，满足 $p$ 被另一个点 $q$ 所支配，$q \in A_j$ 对于任意 $A_j \in L_k$ 翻译一下就是：在 $L_k$ 中留下非支配解，其他的都删去。 计算基于所有的 $A_i \in L_k$ 的 U (下面会详细说) 计算 $\mu(I_{A_i})$ 对每一个 $A_i \in L_k$ (下面会详细说) 对于每一个 k = 1~Q-1 对于所有的 $A_i \in L_k$ : G(A_i) = \mu(I_{A_i}) + \sum_{j = k + 1} ^Q \mu_{max}(L_j)其中，$\mu_{max}(L_j)$ 是 对于 $A_i \in L_j$ 最大的 $\mu(I_{A_i})$ . 例如下图： 这段有点不会了，索性直接贴图了。。。。 Dominance move(DoM)我看了四天！！！ 好多证明和推导，在这里就不解释了…. 定义：$n_R(q)$ ，为在 $R$ 中最接近点 $q$ 的点。 距离测量： D(P,Q) = \min_{P' \preceq Q} \sum_{i = 1}^n d(p_i,p_i')\\ d(p_i,p_i') = \sum_{j=1}^m |p^j_i - p_i'^j|$p_i^j$ 是 在 $P$ 中第 $i$ 个解的第 $j$ 个目标函数。$p’$ 是 $p$ 转移到 $p$ 试支配 $Q$，使得：$Q$ 中的任意一点都可以被 $P^‘$ 支配。$m$ 是目标函数的个数。 d(p,Q_s) = \sum_{j=1}^m(p^j - \min\{ p^j,q^j_{s1},q^j_{s2},...,q^j_{sk} \})m$ 是目标函数的个数。 假设计算 $D(P,Q)$ 删除 $Q$,$P$ 个子中的被支配的解，再删除 $Q$ 中被 $P$ 支配(存在一个就行)的点。 设 $R = P \bigcup Q$ ，首先把 $Q$ 中的每一个点当作一组，然后对于 $Q$ 中的每一个点，在 $R$ 中寻找它的最近邻点；对 $Q$ 中的每一个点，寻找到一个 $ r \in R$ ，使 $r = n_R(q)$ ，如果 $r \in P$ ，那么把 $r$ 归为此 $q$ 一组；如果 $ r \in Q$ ，如果 $q$ 和 $r$ 已经在一组，什么都不需要做，否则，把这两组归为一组。 如果在任何组中不存在 $q \in Q$ ，满足：$q = n_R(n_R(q))$ ，即这两个点互为最近邻，那么结束； 对于有环(互为最近邻)的组，用这两个点的理想点取代这两个点，产生新的集合名为 $Q’$ ，寻找在 $P \bigcup Q’$ 中此理想点的最近邻点，并归类为一组，转向，Step.3 举例： 以下为 在收敛性，一致性，延展性，基数性，四个方面做出比较，效果都不错，但有一个很大的问题就是，只能比较二元问题，对于二元以上的存在一些漏洞，证明上不是充要条件。 Volume-based QIsHypervolume(HV)HV首次作为空间的大小所展示，然后被用作几个专业术语hyperarea metric，S-metric，Lebesgue measure。由于HV指标具有理想的实际可用性和理论特性，因此可以说是最常用的QIs。计算HV不需要表示帕累托前沿的参考集，这使得它适合于许多实际的优化场景。HV结果对帕累托优势集的任何改进都是敏感的。当一个集合A优于另一个集合B时(即,一个A◁B)，然后HV返回A的质量值高于B。因此，对于给定的问题，达到最大HV值的集合将包含所有帕累托最优解。 HV indicator 的定义如下。已知解集A和参考点r, HV可计算为： HV(A) = \lambda(\cup\{ x | a \prec x \prec r \})$\lambda$ 表示勒贝格测度，简而言之，一个集合的HV值可以看作是由每个解和参考点(分别为左底顶点和右顶顶点)确定的超立方体的并集的体积。 HV的局限性是它的数量关于目标数而指数增加的运行时间(除非P=NP)。HV的另一个问题是其参考点的设置。对于如何为给定的问题选择合适的参考点仍然没有共识，尽管有一些常见的实践，例如帕累托前沿的最低点或比较解集集合的最低点的1.1倍。不同的参考点会导致HV评价结果不一致[110]。除了少数特殊情况外，关于高压参考点的选择缺乏系统的研究/理论指导。Recently,have demonstrated a clear difference of specifying the proper reference point for problems with a simplex-like Pareto front and an inverted simplex-like Pareto front. 他们还通过实验表明，一个比最低点稍差的参考点并不总是合适的，特别是在多目标优化和/或小群体规模的情况下。此外，HV指标偏向膝关节区域，偏向凸区域多于凹区域。证明，一组达到最大HV值的解的分布很大程度上取决于帕累托前缘的斜率。例如，HV可能倾向于高度非线性帕累托前缘上非常不均匀的解集。这已经得到证明。 hyperarea ratiohyperarea 定义为 $PF_{known}$ 值所包含的空间，例如，在二维目标优化中，就是原点和函数值所覆盖的矩形面积： H = \{ \bigcup_i a_i | v_i \in PF_{known} \}其中，$v_i$ 是 $PF_{known}$ 中的非支配解向量，$a_i$ 是由 $v_i$ 分量和原点确定的超面积。 以下图为例： 被(0,0) 与 (4,4) 所围成的矩形的面积是 16。被 (0,0) 与 (3,6) 所围成的为 (3 x (6-4)) = 6个，依次…结果： $P_{true}$ ‘s H = 16 + 6 + 4 + 3 = 29. $PF_{true}$ ‘s H = 20 + 6 + 7.5 = 33.5. 同时，也注意到：如果 $PF_{true}$ 是 non-convex ，这种测量方法会有错误。它们还隐式地假设MOP的目标空间原点坐标为(0..，0)，但情况并非总是如此。$PF_{known}$ 中的向量可以转换为以零为中心的原点，但是由于MOPs之间每个目标的范围可能完全不同，因此最佳 $H$ 值可能相差很大。也定义了 $hyperarea \ ratio$ 定义如下： HR = \frac{H_1}{H_2}$H_1$ 为 $PF_{known}$ 的超面积，$H_2$ 为 $PF_{true}$ 的超面积。在极小化问题里：ratio 值为 1，当 $PF_{known} = PF_{true}$ ；如果大于 1，即为 $PF_{known}$ 的超面积大于 $PF_{true}$ ，上例中，$HR = \frac{33.5}{29} = 1.155$ 。 Hyperarea Difference (HD)使 $A,B \subseteq X $ 是两个decision vectors，那么，函数 $D$ 定义如下： D(A,B):= \xi (A+B) - \xi (B)所给的是被 $A$ weakly dominate 但是不被 B weakly dominate 的空间的大小(objective space)。 如上右图，A 为 前沿1，B 为前沿2。一方面，$\alpha $ 是被前沿1但不被前沿2所占的大小。另一方面，$\beta$ 是被前沿2但是不被前沿1所占的大小，黑色的区域是被两个都占的大小，因此，$D(A,B) = \alpha$ ，$D(B,A) = \beta$ 。因为： \alpha + \beta + \gamma = l(A+B)\\ \alpha + \beta = l(A)\\ \alpha + \gamma = l(B)\\在这个例子中，$D(B,A) &gt; D(A,B)$ 意味着与C度量相比，这两个方面的差异体现。另外，它给出了集合是否完全支配另一个集合的信息，例如 $D(A,B) = 0$，$D(B,A) &gt;0$ 意味着 $A$ 支配 $B$。 理想下，D 测量常用于被 $V$ 归一化的 $l$ 指标，对于应该最大化问题： V = \prod_{i = 1}^k (f_i^{max} - f_i^{min})$f_i^{max},f_i^{min}$ 是 目标 $f_i$ 的最大值，最小值。可是，也有其他的情况，$V = l(X_p)$ ，表现得也不错。结果，四个值被考虑，当考虑两个解集时，$A,B \in X_f$: $l(A) / V$ ，它给出了目标空间中被 $A$ 弱支配的区域的相对大小。 $l(B) / V$ ，它给出了目标空间中被 $B$ 弱支配的区域的相对大小。 $D(A,B) / V$ ，她给了被 $A$ 弱支配但不被 $B$ 弱支配的区域的相对大小。 $D(B,A) / V$ ，她给了被 $B$ 弱支配但不被 $A$ 弱支配的区域的相对大小。 由于 $D$ 度量是在 $l$ 度量的基础上定义的，因此不需要额外的实现工作。 Volume measure粗略的说$ \mathcal{V}(A,B) $ 是包含严格由 $A$ 的元素支配但不受 $B$ 的元素支配的两条边的最小超立方体的体积的分数(并且在[0,1]区间内)。如下图所示，两个连续的前沿 $A$ 和 $B$ 在目标空间的不同区域存在不同程度的差异，并且相互支配。(目标函数最小化) $\mathcal{V}(A,B)$ 定义如下，对任何D维的向量 $Y$ ，$H_Y$ 为包括 $Y$ 的最小的轴平行超立方体。 H_Y=\{ z \in R^D:a_i \leq z_i \leq b_i \ for \ some \ a,b \in Y \ i=1,...,D \}现在用映射到单位超立方体上的规格化缩放和平移来表示$h_Y(y):H_Y \rightarrow [0,1]^D$。此转换用于消除目标伸缩的影响。相当于 $k = h_Y(y)$ 把原来的点 $y$ 通过缩放与平移到单位超立方体中的点 $k$ 。 D_Y(A)=\{ z \in [1,0]^D:z \prec h_Y(a) \ for \ some \ a \in A \}上式为超立方体中被归一化控制的点的集合，那么$\mathcal{V}(A,B)$定义如下： \mathcal{V}(A,B)=\lambda(D_{A \cup B}(A) \backslash D_{A \cup B}(B) )其中，$\lambda(A)$ 是 $A$ 的勒贝格测量。个人认为： ​ 绿色的部分为$\mathcal{V}(B,A)$ 。 ​ 红色的部分为$\mathcal{V}(A,B)$ 。 尽管这个描述相当繁琐，但是 $\mathcal{V}(A,B)$ 和 $\mathcal{V}(B,A)$ 很容易通过对 $H_{A \cup B}$ 的蒙特卡罗抽样来计算，并计算A或b占绝对优势的样本的比例。本研究选取5万个样本进行蒙特卡罗估计。体积测量 $\mathcal{V}$ 的好处是,它将奖励设置更大的区段当这些区段是前面的比较,而不是当他们在后面,不受点分布方面,而且它也给信息多远一组(平均)面前的另一个地方。 不幸的是，这个测度 $\mathcal{V}$ ，像原来的度规$\mathcal{C}$ 一样，具有这样的性质，如果$\mathcal{W}$ 是一个非支配集，并且 $A \subseteq W$，$B \subseteq W$ ，$\mathcal{V}(A,B)$ 与 $\mathcal{V}(B,A)$ 两者都是积极的。 Integrated Preference Functional (IPF)作为一组在运筹学上已建立完善的QIs，IPF测量由集合中的每个非支配解和给定的效用函数在相应的最优权值上确定的polytopes的体积。它可以被理解为表示解决方案集为DM[16]所携带的预期实用程序。IPF指标的计算分为两个步骤:1)找出每个非优解的最优权重区间;2)对这些最优权重区间上的效用函数进行积分。 形式上，$A \subset \mathcal{R}^m$ 是非支配解集，其中 m 是目标函数个数。考虑一个参数化的效用函数族 $u (a,w)$，其中给定的权重 $w$ 产生一个要优化的值函数，其中 $a \subset A$ 和 $w \in W \subset \mathcal{R}^m$ ,对于给定的 $w$，让 $𝑢^*(A,𝑤)$ 为在A中最好的效用函数值的解决方案。给定权重密度函数$h:W \rightarrow \mathcal{R}^+$ ，表示未知权重w的概率分布，并且$\int_{w \in W}h(w)dw=1$，那么集合A的IPF值为： IPF(A)=\int_{w\in W} h(w)u^*(A,w)dw效用函数可以表示作为目标的凸组合(即加权线性和函数)或加权Tchebycheff函数。前者只考虑受支持的解决方案，而后者则涵盖所有非支配的解决方案。IPF indicator可以在/不需要 DM 的输入的情况下使用。当DM的偏好可以按照某个部分权重空间来表达时，IPF衡量的是该集合在部分帕累托前沿所代表的偏好的好坏。当没有可用的偏好信息时，可以假设所有的权重都是相等的(即$h(W)=1,\forall w \in W$)，IPF 衡量的是这组数据如何很好地代表整体帕累托。较低的IPF值为佳。然而，使用IPF指标的一个限制是，随着目标数量的增加，其计算复杂度呈指数级增长，因为它需要在(连续的)权重空间上进行积分。 以下为论文原文： 对于多目标优化问题，经常使用值(utility)函数法将各种目标函数组合成输入的一个标量函数。这个组合目标可以表示为参数化函数族 $g(x;α)$，一个给定的值参数向量 $\alpha$ 在它的领域 A 中代表一个特定的标量目标，并且目标是最小化。在二元目标的情况下0和1之间的 $\alpha$ 是一个标量,凸组合的情况下的目标。 对于给定的集合 $X$ (多目标函数的非支配解)，对于任意给定 $\alpha$ ，通过 $g(x;\alpha)$ 至少存在一个最优解。对于给定的 $g$ ，定义一个函数 $x_g:A \rightarrow X$，它把参数值( $\alpha$ )映射到 $X$ 中的相应解上。这个函数 $x_g(\alpha)$ 很清晰的把 $A$ 分成了几个区域，反函数 $x_g^{-1}(x)$ ，随着 $x \in X$，定义参数空间 A 在解上的分区，其中 $x_g$ 是常量： A = \bigcup_{x \in X}x^{-1}_g(x) = \bigcup_{x \in X} A_x对于，$x_1 \neq x_2$ ，其中，$A_{x_1} \ and \ A_{x_2}$ ，在二元目标例子中，至多有一个值相同。通常情况下，$A_{x_1} \bigcap A_{x_2}$ 是 对于 $x_1 \ and \ x_2$ 最优解时，两个区域间的边界。在所有实际情况下，这将是一组测度零，不会影响指规数的计算。给定: $h:A \rightarrow R_+$ ,$\int_{\alpha \in A} h(\alpha) d \alpha = 1$, IPF(X) = \int h(\alpha) g(x_g(\alpha);\alpha)d\alpha将解集映射到实数的积分偏好函数。因为 $x_g$ 是(piecewise)分开的常量，上式的积分可以分解成与 $x \in X$ 相对应的 $x_g^{-1}(x)$ 的不同区域。 IPF(X) = \int_{\alpha \in A} h(\alpha) g(x_g(\alpha);\alpha)d\alpha = \sum_{x \in X}\left[ \int_{\alpha \in x^{-1}_g(x)} h(\alpha) g(x;\alpha)d\alpha \right]因此,给定一个 $\alpha$ 的值产生一个特定的目标函数，因为至少有一个最优解在集 $x_g(\alpha)$ 。密度函数 $h(\alpha)$ 分配不同的值给权重向量 $\alpha$ 值,然后 $IPF$ 提供了一个通用的“最优”的解决方案,在已选择权重密度函数。 最后形式的方程表明,我们只需要能够评估积分 $h(\alpha)$ ,因此目标函数的形式是无关紧要的。此外，这里提出的 $IPF$ 测度没有考虑到决策者的任何个人偏好结构，因此可以认为是一般性的。当然，所有这些的主要困难是计算出 $x_g$ 为分段常数的 $A$ 的适当区域，以及计算式(2)中的积分。这些困难取决于函数g、函数h的类型和考虑的目标的数量。 $h(\alpha)$ 如下图： 在实际问题中应用IPF测量，需要对每个目标进行适当的标度。当所考虑的目标是不可比较的(例如，延迟作业的数量和总完成时间)，则无法解释混合的目标值。此外，当每个目标值的范围之间的差异非常大，以至于一个目标值可以被另一个目标值抵消时，将多个目标混合到一个合理的标量值需要适当的缩放。Schenkerman(1990)提出在缩放目标值时，合适的最小值和最大值分别是最大化问题中近似集的非支配最小值和理想点。他还坚持认为，其他的最低要求可能会阻止决策者做出自己喜欢的决定。De et al.(1992)在比较最小化问题的近似集与面积和长度度量时采用了相同的比例方法。正如Gershon(1984)所指出的，规模可以衡量目标的重要性，这影响所考虑的权重。 另外下面是在另一个论文里对 $IPF$ 的介绍，觉得更通俗，就摘过来了。 IPF(Tchebycheff)如Carlyle et al.(2003)所述，决策者的价值函数可以表示为目标的凸组合这一假设意味着只有支持的点才有助于IPF度量。一般来说，当决策者的隐式值函数是非线性的时，这是一个严重的限制，因此，在非支配解集中某些不受支持的点实际上可能是更可取的。在极端情况下，可能会出现这样的问题:受支持的有效解决方案非常少，而绝大多数有效解决方案可能不受支持。在评估非支配点集时，考虑不支持点的影响的一个好方法是使用加权的Tchebycheff函数来表示决策者的价值函数。Tchebycheff函数对应于权重规度$L_p​$ ,在下式中，当 $p = \infty​$ : minimize_{i \in I} \left( \sum_{j \in J} \alpha^p_j (z_j^i - z_j ^{**})^p \right)^{1/p}其中，$I=\{ 1,2,…,n \}$ 是解集的索引，$J=\{ 1,2,…,m \}$ 是目标的索引，$\alpha_j$ 是第 j 个目标的权重，并且 $\alpha_j \geq 0$，同时，$\sum_{j \in J} \alpha_j = 1$ ，$z^i_j$ 是第 i 个解的第 j 个目标函数值，$z^{**}_j$ 是第 j 个目标的理想值。注意到这一点可以通过最小化第 j 个目标本身来找到。当 $p = 1$ 时，测度对应于目标函数的凸组合当 $z^{**}_j = 0$ 时。当 $p = \infty$ 时，权重测度 $L_{\infty}$ 测度为： minimize_{i \in I} [ \max_{j \in J}\{ \alpha_j(z^i_j - z^{**}_j) \} ]加权Tchebycheff函数可用于生成所有非支配点，支持点和不支持点，因此在多目标优化问题中得到了广泛的应用。当使用加权Tchebycheff函数时，集合中的每个非支配点都有一个最优权区间。本文给出加权契比雪夫函数的IPF测度的计算方法。如果我们假设所有的权值都是等可能发生的($h(\alpha ) = 1 ,for \ all \ \alpha \in A​$)。当考虑二元函数时： minimize_{i \in I} [ \max \{ \alpha z^i_1, (1-\alpha) z^i_2 \} ]其中，$0 \leq \alpha \leq 1$ ， 并且考虑两个问题，1). 找出每个非支配点的最优权区间。2).在分解后的最优权值区间上对标量值函数积分。 以下的 Step.1~Srep.3可解释第一个问题，Step.4解释第二个问题。 Step.1：按照解的第一维度升序排好解如下： z_1^1 < z_1 ^2...>z_2^n Step.2：获得 break-even 权重，$\alpha ^i_b$ 对于每一个点 $i \in I$ ，其中，关系如下： \alpha_b^iz_1^i = (1-\alpha_b^i)z_2^i下图a，有在目标空间中的一个点，虚线表示一个break-even点 $\alpha ^i_b$ 。满足上式，并从图b，看出所有的点 $z^i,i \in I$ 满足如下： \alpha^b_i = \frac{z^i_2}{z^1_1 + z_2^i}使用break-even权重，$\max\{ \alpha z^i_1, (1-\alpha) z^i_2 \}$ 对每一个非支配点都可以如下这样： \max\{ \alpha z^i_1, (1-\alpha) z^i_2 \} = \begin{cases} \alpha z_1^i &\alpha \geq \alpha_b^i\\ (1-\alpha_b^i)z_2^i &\alpha \leq \alpha_b^i \end{cases} 定理一：对于所有的不等式，均满足：$\alpha_b^1 &gt; \alpha_b^2&gt;…&gt;\alpha_b^n$ ，具体推导见原论文。 Step.3：对于每一个非支配解，获得一个上界，$\alpha_U^i$，获得一个下界，$\alpha_L^i$ 。 \alpha_U^1 = 1\\ \alpha_L^1 = \alpha_U^2 = \frac{z_2^1}{z_1^2 + z_2^1}\\ .\\ .\\ \alpha_L^i = \alpha_U^{i+1} = \frac{z_2^i}{z_1^{i+1} + z_2^i}\\ .\\ .\\ \alpha_L^n = 0 Step.4：在步骤3分解的最优权重区间上对加权Tchebycheff函数进行积分，对在 Z 中每一个点有如下： IPF(Z) = \int^1_0 h(\alpha) \min_{i \in I}[\max\{ \alpha z^i_1,(1-\alpha)z_2^i \}]d \alpha\\ =\sum_{i \in I} \left( \int_{\alpha^i_L}^{\alpha^i_b} h(\alpha)(1-\alpha)z_2^i d \alpha\\+ \int_{\alpha^i_b}^{\alpha^i_U} h(\alpha)\alpha z_1^i d \alpha \right) 下面为一个实例： R Family与IPF指标类似，R族[73]也将DM s偏好纳入评价。然而，与IPF不同的是，R质量指标中的集成是基于效用函数(而不是权重空间)的。给定两个解集A和B，一个效用函数空间R和一个效用密度函数(U)，可以定义为 R(A,B,U) = \int_{u \in U} h(u)x(A,B,u)du根据结果函数 $x(A,B,u)$，R家族有三个指标。R1考虑DM优先选择其中一个的概率，R2考虑效用函数的期望值(就像IPF指标)，R3引入了基于R2的比值。其中R2使用频率最高，可以表示为 R2(A,B,U) = \int_{u\in U}h(u)u^*(A)du - \int_{u \in U}h(u)u^*(B)du其中，$u^*(A)$ 表示A在此特定效用函数上所获得的最佳值。可以看出，两个集合的R2值可以单独计算。与IPF指标一样，当偏好信息不可用时，$h(u)$可以均匀分布在 $u$ 上。然而，计算中通常使用离散有限集 $U$，这与IPF中考虑的连续集$W$相反。这可以使R2的计算变得友好。特别地，如果效用函数 $u$ 的集合可以用权值 $W$ 的集合和这些权值上的参数化效用函数来表示，那么R2可以进一步计算为 R2(A) = \frac{1}{|W|}\sum_{w \in W} u^*(A,w)与IPF一样，$u(A,w)$ 的物化过程中也存在多项选择，如加权线性和函数和加权Tchebycheff函数，但后者在实践中应用较为广泛。 当h(w)设为1时，将式(13){R2(A)}与式(10)(IPF(A))进行比较，R2和IPF指标表现得非常相似。IPF指标考虑的是一个连续的权值空间，它需要相对于目标维数呈指数增长的计算时间，而R2指标考虑的是一组离散的权值，它的计算速度很快，但其精度自然低于IPF。]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>AllaspectsQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Spread and Uniformity]]></title>
    <url>%2F2019%2F02%2F06%2Fspreadanduniformity%2F</url>
    <content type="text"><![CDATA[过年期间，学习就更加懈怠了。。 前言spread和uniformity的品质方面是密切相关的，需要将它们放在一起考虑，以反映解决方案集的diversity。这激发了QIs去覆盖spread和uniformity的品质。这类QIs大多数可以分为两类，distance-based indicators and region division-based indicators，尽管也有其他的选择，如基于集群的指标和基于容量的指标。如图： Distance-based QIs(59~62)该类中的QIs(表中项目59-62)通常考虑解与其邻域之间的距离，然后将这些距离相加，从而估计整个集合的覆盖范围。沿着这个思路的是 $\Delta$ ,然后是sparsity index，extended spread，and $\Delta_{Line}$ 。然而，这样的评估只能在双目标问题中工作，如非支配解连续位于两个目标上。这些QIs的另一个问题是，它们需要帕累托前沿(例如边界)的信息作为参考，这在实践中往往是未知的 $\Delta$此论文来自于伟大的$NSGA-II$ ！ \Delta = \frac{d_f + d_l + \sum_{i=1}^{N-1}|d_i-\bar{d}|}{d_f + d_l + (N-1)\bar{d}}具体参数可看下图，一目了然。 值得注意的是，这并不是可能的解决方案的最坏情况。我们可以假设$d_i$有很大的方差。在这种情况下，度量可能大于1。因此，上述度量的最大值可以大于1。然而，一个好的分布会使所有的 $d_i$ 距离都等于 $\bar{d}$ 并且会$d_f=d_l=0$(在非支配集中存在极端解)。因此，对于最广泛且均匀分布的非支配解集，其分子为零，使得度规 $\Delta$ 取的值为零。对于任何其他分布，度规的值都大于零。对于两个具有相同$d_f$和$d_l$值的分布，度规$\Delta$取的值较高，而极值解的分布较差。请注意，上述多样性度量可用于任何非支配解集，包括非帕累托最优集的解集。利用三角化技术或Voronoi图方法[1]进行计算$d_i%=$，可以将上述过程推广到估计高维解的spread。 Extended spreadS 是一个解集，$S^*$ 是已知的Pareto-optimal solutions。 $\Delta(S,Q)$ : 原始度规计算两个连续解之间的距离，这只适用于2个目标问题。我们通过计算一个点到它最近的邻居的距离来进行扩展: \Delta(S,S^*) = \frac{\sum_{i=1}^m d(e_i,S) + \sum_{X \in S^*} |d(X,S)-\bar{d}|}{\sum_{i = 1} ^m d(e_i,S) + |S^*| \bar{d}}其中，$\{ e_1,…,e_m \}$ 是 $ S^*$ 中的 m 个极值解，并且： d(X,S) = \min_{Y \in S,Y \neq X}||F(X)-F(Y)||^2\\ \bar{d} = \frac{1}{|S^*|} \sum_{X \in S^*} d(X,S).如果解决的方案有很好的distribute 并且包括这些极值点，那么，$\Delta(S,S^*) = 0$。 $\Delta_{Line}$(不是很懂)The $\Delta_{Line}$ measures the diversity and spread of approximate solutions without the need for the $PF_{true}$ . Let $\beta$ be the mid-points of equally divided intervals in the range of [0, 1] $\left( [0,\frac{1}{N}],[\frac{1}{N},\frac{2}{N}],…,[\frac{N-1}{N},1] \right)$ where $N$ is the number of solutions in approximate the PF, then the objective line distribution ( $\Delta^i_{Line}$ ) is defined as:但是并不是很懂下标的规定。 \Delta^i_{Line}(S,\beta) = \frac{\sum_{j=1}^{|\beta|}\min_{s \in S}|\beta_j - F_i(s)| }{|\beta|}其中，$F_i(s)$ 是第 $i$ 个目标的归一化近似解，第i个目标线分布的零值表示近似PF沿第i个目标的均匀分布。$S$ 是一个粗略前沿PF。 整体线分布测度定义为: \Delta_{Line}(S,\beta) = \frac{\sum_{j=1}^{M} \Delta^i_{Line}(S,\beta)}{M}其中，$M$ 是目标函数的个数。 Region Division-based QIs (63~74)此分类的基本思想把一个特定的空间分割成许多细胞(重叠)，然后计算细胞的数量有解决方案集。这是基于一组更多元化的解决方案通常占据更多的细胞。考虑到细胞的不同形状，大多数用于扩散和均匀性的QIs都属于这一类。 它们中的一些考虑以解为中心的niche-like细胞，比如，Chi-square-like deviation, U-measure and sparsity。 有些则考虑gird-like的单元格将空间划分为多个超盒，比如，cover rate, number ofdistinct choices , diversity metric , entropy and diversity comparison indicator. 其余的考虑fan-shaped(扇形)细胞，它们用一组均匀分布的光线来分割空间(权重向量)，比如，Sigma diversity metric, M-DI and DIR. 除此之外，考虑最小能量点(s-energy)划分空间[74]也是一种潜在的方法，因为它们可以很好地表示各种形状的空间。 Chi-square-like deviation把search space (也就是自变量的空间)，分成几等分，每一个小区域叫做subregion。 v = \sqrt{\sum_{i=1}^{q+1} \left( \frac{n_i-\bar{n_i}}{\sigma_i } \right) } $q$ 是所期望得到最优解的个数。第$(q + 1)$个子区域是受支配的区域 $n_i$ 是第 $i $ 个非支配子区域实际的个数 $\bar{n}_i$ 是第 $i$ 个非支配子区域的期望个数 $\sigma^2_i$ 是第 $i$ 个非支配子区域的个数的方差 通过概率论，他可以由下面估计： \sigma_i^2=\bar{n}_i(1-\frac{\bar{n}_i}{P})$P$ 是种群尺寸 因为并不希望任何的子代落到非支配区域，因此 $\bar{n}_{q+1} = 0$。并且有，$\sigma^2_{q+1} = \sum_{i = 1}^q \sigma^2_i$ 。如果点的分布是理想的话，那么$v=0$。因此，具有良好分布能力的算法具有低偏差度量的特点。 Sparsity index好像没有公式，只有一段话。。。 大体意思就是：找到一个超平面，把每一个解映射过去，并且每一个解占有一定的大小(size=d 的 hyper-box) 越重合就越不稀疏，再把体积求和，当然体积越大越好。主要在于d的取值，不易太大，不易太小。 U-Measure(太多 再说)Cover rateCover rate is the index(指标) for the diversity of the Pareto optimum individuals. The cover rate is derived in the following steps. At first, one of the object functions is focused. Secondly, the distance between the individuals that have the maximum and the minimum values is divided into the certain number of the division. Thirdly, the division area that have the Pareto optimum individuals is counted. Fourthly, the counted number is divided by the number of division. When every divided area has at least one Pareto optimum individual, this number becomes 1. When there are no area that has the Pareto optimum individuals, this number becomes 0. Fifthly, these steps are treated for every objective function. Finally, the cover rate is determined to average the number of each objective function. When the cover rate is close to 1, it means that the Pareto optimum individuals are not concentrated on one point but they spreads. In Figure 4, the concept of the cover rate are shown, when there are two objective functions. 翻译一下就是：如上图，为一个目标函数(个人觉得如果对于一个目标，理应是在一维坐标上)，找到最大值最小值，分成确定的几份，在每个小间隔中，如果有点就为1，否则为0。依次遍历每一个函数，最后求平均值。 Number of Distinct Choices ($NDC_\mu$)从设计者的角度来看，所观察到的帕累托解集中包含的点越多，可供选择的设计选项就越多。然而，如果观测到的帕累托解在目标空间中过于接近，那么对于设计者来说，观测到的帕累托解之间的变化可能无法区分。换句话说，观察到的帕累托解的数量越多，并不一定意味着设计选择的数量越多。简而言之，对于一个观察到的帕累托解集$p=(p_1,…,p_{\bar{np}})$ ，只有那些彼此之间有足够差异的解决方案才应被视为有用的设计选项。 设数量$\mu , \ (0 &lt; \mu &lt;1)$为设计人员指定的数值，可将m维目标空间划分为$1/\mu^m$的小网格。为了简单起见，将$1/\mu$作为整数。每个网格都是指一个正方形(m维中的超立方体)，即无差异区域$T_{\mu(q)}$，其中区域内任意两个解点$p_i$和$p_j$都被认为是相似的，或者设计人员对这些解不感兴趣。下图给出了二维目标空间中的量$\mu$ 和 $T_{\mu(q)}$。 $T_{\mu(q)}(q,P)$ 表示是否有任何点$p_k \in P$属于区域$T_{\mu}(q)$。当至少有一个解点$p_k$落在无差异区域$T_{\mu}(q)$中时，$T_{\mu(q)}(q,P)$等于单元(或1)。$T_{\mu(q)}(q,P)$等于0(或0)只要$T_{\mu}(q)$区域没有解。一般来说，$T_{\mu(q)}(q,P)$可以表述为: T_{\mu(q)}(q,P) = \begin{cases} 1 & \exists p_k \in P \ p_k \in T_\mu(q)\\ 0 & \forall p_k \in P \ p_k \notin T_\mu(q) \end{cases}质量度量$NDC_{\mu}(q)$，即预先指定的m值的不同选择的数量，可以定义为: NDC_{\mu}(P)=\sum_{l_m=0}^{v-1}...\sum_{l_2=0}^{v-1}\sum_{l_1=0}^{v-1}NT_\mu(q,P)where $q = (q_1,q_2,…,q_m)$ with $q_i=\frac{l_i}{v} $ 其中，$v=1/\mu​$ ，点 $q​$ 位于目标空间m-网格线的任意交点上，坐标为$(q_1,q_2,…,q_m)​$。如本节开头所示，，如果想让$NDC_{\mu}(P)​$值较高的观察到的Pareto解集，对于预先指定的 $\mu​$ 就要有相对于较低的值(网格越密，被删去的点就越少)。 $NDC_{\mu}$ 和 cover rate 是有区别的，前者是把目标空间看作整体，并分成了很多个hyper-box；后者是分析每一个维度，最后加权平均一下。 Diversity metric(DM)规定： $P^{(t)}$ 为每一代的种群。$\mathcal{F}^{(t)}$ 是 $P^{(t)}$ 的非支配解。目标(参考点集) $P^*$ 从 $P^{(t)}$ 中确定 $\mathcal{F}^{(t)}$ , 使 $\mathcal{F}^{(t)}$ 非支配于$P^*$ 对于网格的每一个索引 $(i,j,…)$ ，并计算下面两个： H(i,j,...)=\begin{cases} 1,& if \ the \ grid \ has \ a \ representative \ point \ in \ P^* \\ 0,& otherwise \end{cases} and h(i,j,...)=\begin{cases}1,& H(i,j,...)=1 \ and \ if \ the \ grid \ has \ a \ representative \ point \ in \ F^{(t)} \\0,& otherwise\end{cases} 给 $m(h(i,j,…))$ 赋值根据该索引本身与它邻居的$h()$。同样的，用 $H()$ ，来计算 $m(H(i,j,…))$ 计算多样性衡量标准 by averaging the individual $m()$ values for $h()$ with respect to that for $H()$: D(P^{(t)}) = \frac{ \sum_{i,j,...\\H(i,j,..) \ne 0} m(h(i,j,...)) }{\sum_{i,j,...\\H(i,j,..) \ne 0} m(H(i,j,...))}在这个简单的例子中，网格的值函数 $m()$ 可以通过使用它的 $h()$ 和相邻的两个 $h()$ 维度来计算。对于一组连续的三个二进制 $h()$ 值，总共有8种可能。任何值函数的赋值方法如下: 111 是最好的分布，000 是最坏的分布。 010 或 101表示具有良好扩展的周期模式，其值可能大于 110 或 011 。例如，上述估值将使网格覆盖率为50%的近似集具有更大的分布(如1010101010)，优于具有相同覆盖但分布较小的另一集(如1111100000)。 110 或 011 的值可能超过 001 或 100，因为有更多的网格覆盖。 h(…j-1…) h(…j…) h(…j+1…) m(h(…j…)) 0 0 0 0.00 0 0 1 0.50 1 0 0 0.50 0 1 1 0.67 1 1 0 0.67 0 1 0 0.75 1 0 1 0.75 1 1 1 1.00 对于 $H()$ 使用相同的值。在目前的研究中，通过计算上述度量维度来处理两个或多个维度的超平面，而通过考虑一组移动的超盒来非常谨慎设计地上述值函数的一个高维版本。对一个包含9个盒子的二维集合的考虑如下: 作为上述计算过程的说明，下图显示了一个两目标最小化问题的一组目标点(标记为填充圆$P^*$)和一组总体点(标记为阴影和打开的方框 $P^{(t)}$ )。用阴影框标记的点是相对于目标点的非支配点( $\mathcal{F}^{(t)}$ )，用于多样性计算(这是步骤1)。这里以f2 =0平面为参考平面，将 $f_1$ 值的完整范围划分为G=10个网格。下一步，计算每个网格的 $h()$ 和 $H()$ 值。对于边界网格(极端网格和网格$(…,j,…)$ 与 $H(…,j - 1 ….)= 0$ 。 在边界处的网格，假设一个假想的相邻网格的h()或h()值为1，例如上图的虚线格子。也注意到，有的格子里有不止一个点在其中，但也就算一个。移动的三个格子，确定中间位置的数值。为避免边界效应(使用虚网格的效应)，我们将上述度量归一化如下: \bar{D}(P^{(t)}) = \frac{ \sum_{i,j,...\\H(i,j,..) \ne 0} m(h(i,j,...)) -\sum_{i,j,...\\H(i,j,..) \ne 0} m(0)}{\sum_{i,j,...\\H(i,j,..) \ne 0} m(H(i,j,...))-\sum_{i,j,...\\H(i,j,..) \ne 0} m(0)}0 为值为零的数组。仔细想想就会发现，计算上述$\bar{D}(P^{(t)})$项和边界网格调整时$H(i,j…) \ne 0$ 允许使用一种通用方法来处理具有断开的pareto最优前端的问题。该度量不包括不存在参考解的网格的值函数。 如果不知道帕累托最优前沿(特别是对实际问题)，则可以用以下方法确定目标集。 首先，MOEA运行T代，并存储按代计算的总体($P^{(T)}， t = 0,1…T)$)。 然后,将每个种群的非支配成员 $\mathcal{F}^{(t)}$ 组合在一起，则target set 就是这些的总和。 P^*=Non-dominated(\cup^T_{t=0}\mathcal{F}^{(t)}) Diversity comparison indicator (DCI)很巧这里介绍了DM的缺点： a reference set，它要求是均匀分布在PF，这是必需的，以便准确反映分布的optimal front。也是要求，解决方案参考集近似近似的解决方案的数量以保证理想的分布近似可以达到最佳的DM值(一个)。这些要求在多目标优化问题中通常是不可用的。 DM需要访问网格中的每个hyperbox来估计分布，这对数据结构和计算成本都带来了很大的挑战。对于m个目标的优化问题，需要考虑 $r^{m-1}$ 超盒，其中r为每个维度的划分数。 在超盒的分布估计中，DM需要通过一个值函数给每个相邻的超盒分配一个合适的值，以区分其邻域内解的不同分布。由于超盒的邻居数量随着目标数量的增加呈指数增长(m维超盒最多有($3^m$-1)个邻居)，当涉及大量目标时，很难定义准确反映不同分布的值函数。 由于网格中解的邻域的指定，DM可能无法给出具有大量目标的近似的精确分集结果。DM中解的邻域的设置是基于解的网格坐标的曼哈顿距离，而不是它们的欧氏距离。它可能会误导性地消除相邻解，但将更远的解视为相邻解。 网格的位置和大小在该指标中具有重要意义。设置网格区域不应涉及整个目标空间，而应针对给定问题的帕累托前沿不远的区域，因为不同近似有意义的多样性比较的前提是它们已经接近最优前沿[50]。假设较高和较低的网格边界为：$LB=(lb_1,lb_2,…,lb_m)$ and $UB=(ub_1,ub_2,…ub_m)$ ,m 是目标函数的个数，如果一个解向量$(q_1,q_2,…,q_m)$ 在 $LB$ 与 $UB$ 之外，(也就是说 $k \in \{ 1,2,…,m \}:q_k ub_k$ ) 那么此解向量在indicator calculation 中会被忽略掉。 在将所提出的DCI应用于不同问题时，网格边界可以由用户定义的“满足区域”来确定，也可以由问题的理想点和最低点来设置。“满足区域”是用户的一种估计，即在该区域中所获得的解被认为满足收敛性的质量要求。当用户没有明确规定他/她“满意的地区,”网格边界可以通过给定问题的理想点和最低点(下图所示),理想点和最低点是两个重要的概念在多目标优化中,当PF是未知时他们可以通过一些有效的方法估计。在这里，将一个问题的理想点和最低点所构成的区域的轻微松弛看作网格环境： ub_k = np_k + \frac{np_k - ip_k}{2 \times div}\\ lb_k = ip_k其中，$ip_k$ 是第 $k$ 个目标的理想点，$np_k$ 是第 $k$ 个目标的最低点，$div$ 是一个常数(一个维度中目标空间的划分数，例如下图为5) 根据网格的边界和划分的数量，第 $k$ 个目标中的超盒大小 $d_k$ 可以形成如下图所示： d_k = \frac{ub_k-lb_k}{div}在这种情况下，通过下边界和超盒尺寸可以确定解在帕累托前近似中的网格位置如下(向下取整): G_k(q)=\lfloor (F_k(q)-lb_k)/d_k \rfloor其中$G_k (q)$ 表示第 $k$ 个目标中解 $q$ 的网格坐标。$F_k(q)$ 是 $q$ 在第 $k$ 个目标中的真实值。上图中，A，B，C，D 的坐标一次为 (0,4)，(0,3)，(2,2)，(4,0)。以下在引入关于距离的两个概念 $h_1,h_2$ 是网格中的两个超立盒子，那么两个网格的距离为： GD(h_1,h_2)=\sqrt{\sum_{k=1}^m (h_1^k - h_2^k)^2}$h_1^k，h_2^k$ 是 $h_1,h_2$ 的在第 $k$ 个目标函数的坐标。$m$ 是目标函数总数。例如 B 与 C 距离为 $\sqrt{(0-2)^2 + (3-2)^2} = \sqrt{5}$ . P 是也该粗略解集， h是一个超方体盒，从 P 到 h的最短格距离为： D(P,h) = \min_{p \in P} \{ GD(h,G(p)) \}例如，上图中 在粗略解集A，B，C，D中距坐标为(1,3)的超方体盒子的距离为 $GD(h^{(1,3)},G(B)) = 1$ 。显然，在网格环境中解分布均匀且分布广泛的近似，其到所有超盒的平均距离值较低。 不同的帕累托前近似解可能位于不同的超盒中。在这里，我们只考虑在混合逼近集中非支配解所在的超盒，因为受支配解的多样性对用户来说可能毫无意义。对于一个近似解，如果它的解覆盖或接近所有被考虑的超盒，那么与其他近似相比，它将获得一个相对较好的多样性;另一方面，如果它的解决方案远离大多数这些超盒，则会获得相对较差的多样性。算法1给出了计算待比较近似的DCI值的主要步骤。 贡献度(算法1的第6行)反映了近似对超盒的贡献，并由它们之间的距离决定。对于近似，如果所考虑的超盒中至少存在一个解，则可获得该近似对超盒的最大贡献程度。如果从近似值到超框的距离大于指定的阈值即，则贡献度为0。具体地说，近似P对超盒h的贡献程度定义为： CD(P,h) = \begin{cases} 1 - D(P,h)^2 / (m + 1) &d(P,h) < \sqrt{m+1}\\ 0 &d(P,h) \geq \sqrt{m+1} \end{cases}值得指出的是，我们将网格距离的阈值设置为$\sqrt{m+1}$，是为了保证相邻的两个个体始终能够交互(即，它们所在的超盒总是相邻的)。直观地说，如果两个超盒的个体可以任意接近，那么它们应该被视为邻居(在个体之间没有另一个超盒)。显然，满足上述条件的最远的两个超盒是网格距离为$\sqrt{m}$的对角超盒。由于超盒之间的网格距离始终为离散值$\sqrt{0},\sqrt{1},…,\sqrt{m},\sqrt{m+1}…$，将阈值设置为$\sqrt{m+1}$，只是使这些超盒对角相邻，在计算贡献度时可以相互作用。 图3为不同目标数下贡献度函数曲线。注意，贡献度取一个离散值，因为$D(P,h) \in \{\sqrt{0},\sqrt{1},…,\sqrt{m},\sqrt{m+1}…\}$。从图中可以看出，一些观察结果如下: 贡献度取0到1之间的值。在一定范围内，从近似到超盒在一定范围内，它单调地减小。 hyperbox的邻域半径随着目标数量的增加而增加。这表明，当目标数量增加时，可以考虑更大范围的个体进行交互。 当距离变量D(P, h)相等时，贡献度随着目标个数的增加而增加。这种增加似乎是合理的，因为随着网格中超盒总数的增长，超盒之间的相对距离变得更小。 总体而言，贡献度函数不仅考虑了近似到超盒的距离信息，还考虑了目标个数不同的网格环境的性质，对目标个数的变化具有良好的适应性。实际上，任何形式的函数都可以通过记住上述性质来赋值为贡献度函数。为了简单起见，这里使用二次函数。 根据贡献度函数，近似的DCI值是[0,1]区间内的一个数值。需要重申的是，DCI只是评估不同的帕累托前近似的相对分布质量，而不是为单个近似提供分布的绝对度量。最佳价值(即由近似得到的DCI = 1)不能反映其在整个帕累托前缘的均匀分布。相反，它表明该近似比其他近似有一个完美的优势。 上图展示了DCI的计算过程，有三个二元目标问题的pareto approximation $P_1,P_2,P_3$ ，并放在了网格环境中，有11个超方盒(灰色)被决定，其中解A，B并没有考虑其中，因为他们在混合解中被支配了。然后，对于每个超盒，根据前面提到的公式式计算三种近似的贡献度。比如，当考虑$h^{0,7}$ 时，$P_2$ 的贡献值为1，因为它在超方盒中。对于$P_1$ 来说：$1-1^2/3=2/3$ 作为$D(P_1,h^{0,7}) = 2/3$ ，对于$P_3$ 为0，因为$\sqrt{10} &gt; \sqrt{3}$ 。最后，根据算法1(第10行)求出各近似对这些超盒的平均贡献度，分别为：0.848，0.606，0.515。 M-DIM-DI 是在 DCI 的基础上修改的。 在DCI方法中，将各种算法的NDFs(non-dominated fronts)集合在一起，识别出一组帕累托最优解。使用由网格划分参数定义的网格，将每个算法的贡献与组合的帕累托最优解进行比较。 假设有两组帕累托前近似 $P_1$ 和 $P_2$ ，如下图所示。在这种情况下，$P_2$ 是组合的Pareto front，它的DCI度量是最高的(值1)，但是我们可以看到，这个值并没有反映出在front的极限之间的目标空间中解的均匀分布。在没有关于POF(Pareto Optimal Front)的任何资料的情况下，如果假定有一个连续的front，$RTF$ 很可能提供尽可能最好的解决办法。 在M-DI中，多样化是相对于RPF(Reference Pareto Front：单位截距在超平面上均匀分布的一组点)计算的。Nadir and Ideal point 计算方式与 DCI 相同。在RPF上的reference的数量 $W$ ，与在优化进程期间的人口尺寸 $N$ 有关 。For example, a population of 90 used for a 3-objective optimization problem would mean use of 91 reference points on the RPF. 其中 $CD(P,h)$ 仍不变，$h$ 是被RPF所占据的hyper-boxes。而不是在DCI 中的把所有解集合在一起，取出非支配解所占的hyper-boxes，因此： M-DI = \frac{\sum_{i=1}^{|h|}CD(P,h_i)}{|h|}EntropyIdeal/good points： 将理想点定义为目标空间中的一个点，该点的分量分别由目标函数的约束最小化得到： Minimize \ f_i(x) \quad s.t.:x \in DNadir/bad points： 在此论文中是，在本文中，我们任意地高估了目标的范围，以至于没有遇到违反估计上限的设计点。 Influence Function： 在决策空间中，第 i 个解的影响函数$\Omega_i:F^m \rightarrow R$ ，$\Omega_i$ 是随第 i 个解而下降的函数，种类很多，本轮中选择 Gaussian influence function。 \Omega(r) = \frac{1}{ \sigma \sqrt{2 \pi} } e^{-r^2/2\sigma^2}Density Function： 将可行目标空间各点的密度函数定义为各解点影响函数的集合，设共有 N 个解点，可行目标空间 $F_m$ 中任意点 $y$ 处的密度函数可得： D(y) = \sum_{i=1}^{N} \Omega_i(r_{i \rightarrow y})$r_{i \rightarrow y}$ 是一个标量，它展示了 $y$ 与 第 $i$ 个解点的欧式距离。$\Omega_i( . ) $ 是 第 $i$ 个点的影响函数，下图展示了在一维里一些点的影响距离。 Entropy： Claude Shannon 引入信息论熵来测量随机过程的信息含量，从而建立了信息论领域。从那时起，熵的许多不同的应用在不同的领域有他们自己的解释和定义。假设一个随机过程有n个可能的结果第i个结果的概率是。这个过程的概率分布可以表示为： P = [p_1,...,p_i,...,p_n];\sum_{i=i}^{n}p_i=1; p_i \geq0这个概率向量有一个相关的Shannon s熵，H的形式这个概率向量有一个相关的Shannon s熵，H的形式： H(P) = - \sum_{i=1}^n p_i \ ln(p_i)其中， 当 $p_i = 0$ 时，$p_i \ ln(p_i)=0$。最大值为 $H_{max}=ln(n)$ 当所有的值都相同的，最小值为0，当一个为1，其他的所有均为0。事实上，香农熵衡量的是 $P$ 的平整度，即。，如果向量中各分量的值近似相等，则熵值很大，但如果各分量的值相差很大~概率分布不均匀!，对应的熵值较低。 如下图所示 网格的尺寸是 $a_1 \times a_2$ 在feasible domain。对每一个密度函数，$D_{ij} = D(y_{ij})$，$a_1$ 和 $a_2$ 的数量确定为每个单元格的大小小于或等于无差异区域(无差异区域定义为任意两个解点被认为是相同的单元格大小，或决策者对这些解不感兴趣)。$a_1$ 和 $a_2$ 的数量可以根据设计者的经验或对类似问题的认识主观确定；或者客观地基于可用的计算能力和期望的精度。假设非常小的网格大小有助于提高准确性，但它也增加了计算熵的计算负担，这反过来可能使质量评估过程非常缓慢，甚至在计算上不可行。显然，适当的网格大小取决于问题，并且在不同的情况下有所不同。因为这些项的和。在香农熵的定义中，熵是1，我们定义一个归一化密度，$\rho_{ij}$ ，为: \rho_{ij} = \frac{D_{ij}}{\sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2} D_{k_1k_2}}实际上，上述归一化密度的定义对于空解集的定义并不好，这就是为什么在密度函数的定义中假设解集是非空的原因。我们将给空解集的熵赋值为0来表示最坏的情况，即0的多样性。现在我们有: \sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2} \rho_{k_1k_2} = 1\\ \rho_{k_1k_2} \geq 0,\forall k_1,k_2这样一个分布的熵可以定义为: H = -\sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2} \rho_{k_1k_2}ln(\rho_{k_1k_2})并且，对于m维目标空间，将目标空间中的可行域划分为a13a23。3am细胞，熵定义为: H = -\sum_{k_1=1}^{a_1}\sum_{k_2=1}^{a_2}...\sum_{k_2=1}^{a_m} \rho_{k_1k_2...k_m}ln(\rho_{k_1k_2...k_m})熵值越大的解集在目标空间的可行域内分布越均匀，覆盖范围越广。 Sigma diversity metric提出了计算目标空间中解的位置的Sigma多样性度量。 如下图： 上左图，对于每一个线，都有$f_2 = af_1$，因此，$\sigma $ 便为： \sigma = \frac{ f_1^2 - f_2^2 }{f_1^2 + f_2^2}所有的在此线上的点都满足 $f_2 = af_1$ ，也就都有相同的 $\sigma = \frac{1-a^2}{1 + a ^2}$ 在一般情况下，$\sigma$ 被定义为 $ C_m^2$ 个元素的向量，其中m是目标空间的维数。在这种情况下，$\sigma$的每个元素都是上式中两个坐标的组合。例如f1、f2、f3三个坐标，定义如下:(如上又图) \sigma =\begin{pmatrix} f_1^2 - f_2^2 \\ f_2^2 - f_3^2 \\ f_3^2 - f_1^2 \end{pmatrix} /(f_1^2 + f_2^2 +f_3^2)例，$\sigma = (0 \ 0.5 \ -0.5)$ 时，$f_1 = 1,f_2 = 1,f_3 = 0$ 带入上面即可。 这意味着目标空间中的每个点都可以用向量来描述。直线上的所有点都有相同的向量在非常接近的直线上的解都有相似的向量。这是用来构造多样性度量的思想。 在计算近似集的多样性之前，必须先计算一组reference lines(参考线)。参考线数必须等于近似解的个数。必须强调的是，每个目标的参考线必须计算一次，并且可以存储在一个表中。那么Sigma多样性度量可以计算如下: 计算参考线 计算每条参考线的向量(参考向量 reference sigma vector)。 在每个参考向量旁边保留一个初始值为零的二进制标记。每个引用的旗帜 $\sigma$ 向量为 1,当至少有一个解决方案有一个向量 $\sigma$ 等于它或在一个距离(欧式距离)低于d。d的值取决于测试函数,但是它应该减少当有大量的参考线时。 计数器C对标记为1的引用行进行计数，多样性度量D变为： D = \frac{C}{number \ of \ reference \ lines}Sigma多样性度量表示的是在目标空间中非支配解的分布百分比 在极值解位于坐标轴上的情况下，我们可以得到D的一个高值，即,100%。对于离散解集或不连通解集，D的值永远达不到最大值。 由上式中的D可知，如果D的值很高，就意味着解的分布很好。但当D很小时，它意味着解是： 集中在空间的一部分 分布在前沿的小群体中 的确，这两种解决方案之间存在差异。下图显示了具有相同D值的两组解之间的差异。这两组解有不同的扩展，Sigma多样性度量无法区分它们。 红线是红点所占的reference lines；黑线是红黑点所占的reference lines。可以看到都是6条！ $\mathcal{M} ^* _2$Analogously, we define one metric $\mathcal{M}_2^*$ and on the objective space. Let $Y’,\bar{Y} \subseteq Y$ be the sets of objective vectors that correspond $X’$ to $\bar{X}$ and , respectively, and $\sigma ^* &gt; 0$ and $|| \cdot ||^*$ as before: 公式： \mathcal{M}^*_2({Y'}):=\frac{1}{|Y'-1|}\sum _{p'\in Y'} \{q' \in Y'; ||p'- q'||^* > \sigma ^*\}]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>diversity</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Cardinality]]></title>
    <url>%2F2019%2F01%2F28%2Fcardinality%2F</url>
    <content type="text"><![CDATA[这部分要么太难，要么太简单。。。。怀疑自己智商 介绍基数的QIs可以归结为一个简单的概念——计算非支配解决方案的数量。基于基数的QIs所具有的一个可取(必要)的属性是，在考虑的集合中添加一个不同的非支配解决方案应该能够改进(而不是降低)评估结果。这(弱)单调的概念是一致的。下面列出了10个基数QIs(项目45-54)： 根据Pareto最优解的参与程度，基于基数的QIs可以分为两类(项目45-48和项目49-54)。 一种是直接考虑集合中的非支配解，例如 the indicators cardinality 𝐷 number of unique nondominated solutions overall nondominated vector generation (ONVG) ratio of nondominated individuals 另一类是比较解集中的非支配解(nondominated solutions)和问题的帕累托最优解(Pareto optimal solutions)，该类中的QIs通常返回属于帕累托最优集的非支配解与最优集大小的比值，(例如：C1，ONVG ratio）,或者或者与解集本身的比值(例如，C2，error ratio) 除此之外，还有其他一些指标，它们只是简单地计算属于最优集的解决方案的数量。 虽然这里描述的是对最终MOEA性能的度量，但是其中许多度量也可以用于跟踪世代总体的性能。然后，除了一个总体性能度量之外，它还指示执行期间的性能(例如，到MOEA optimu的收敛速度)。虽然使用了两个目标的例子，但是这些指标可以扩展到具有任意数量的目标维度的PF。 由于解决方案集的基数通常几乎没有与帕累托前沿的代表性相关的信息，因此通常认为它不如其他三个质量方面重要。然而，如果优化器能够找到问题的很大一部分帕累托最优解决方案，那么评估基数质量可能会变得更加合理。这对于一些组合多目标优化问题尤其适用，其中帕累托最优解的总数很小。在这类问题中，计算得到的帕累托最优解的个数是反映解集质量的可靠指标。事实上，这种评价在一些组合问题的早期研究中经常使用。 Error ratio此indicator与onvg与onvg ratio 出自同一本书，并且有如下定于，为了方便引用原文： 简而言之：$P$ (也就是 pareto optimal set 包含于solution set) 是针对自变量来说的 也就是decision space。 $PF$ (也就是 Pareto Front ) 是针对目标函数值来说的，也就是objective space An MOEA reports a finite mumber of vectors in $PF_{known}$ which are or are not members of $PF_{true}$. If they are not members of $PF_{true}$ the MOEA has erred or perhaps not converged. This metric is mathematically represented by: E=\frac{\sum_{i=1}^{n}}{n}e_iwhere n is the number pf vectors in $PF_{known}$ and e_i = \begin{cases} 0 & if \ vector \ i,i=(1,...,n) \in PF_{true},\\ 1 & otherwise \end{cases}$E=0$，表明$PF_{known}$都在 $PF_{true}$。 $E=1$，表明$PF_{known}$中一个都不在 $PF_{true}$。 上图的值：$E=\frac{2}{3}$ 。 我们还注意到一个类似的度量，它度量由另一个 $P_{true}$ 支配的$P_{known}$ 中的解决方案的百分比。 然而，由于ER只在帕累托最优解中起作用，它可能会带来一些反直觉的情况。例如，在一个集合中添加更多的非支配解决方案可能会导致更差的分数。因此，考虑比较集本身中的非支配解可能是更好的选择，而且也不需要帕累托前沿。 Overall Nondominated Vector Generation and Ratio(ONVG)测试的MOEAs 每一代把 $P_{current}$ 添加到 $P_{known}$ 中，可能导致不同的数量的$P_{known}$ 。这个测量度计算的是在MOEA期间所找到的所有非支配解的数量，定义如下： ONVG = |PF_{known}|Schott 使用这个测量标准(尽管是在帕累托最优集上定义的，例如$|P_{known}|$ ) 。基因型或表现型地的定义这个测量标准可能是偏好问题，但是我们再一次注意到多个解可以映射到相同的向量上，或者换句话说，$|P_{known}| \geq |PF_{known}|$ (多对一)。尽管计算非支配解的数量可以让我们了解MOEA在生成所需解方面的有效性，但它并没有反映出$|P_{known}|$中的向量与$|PF_{known}|$ 之间的“距离”有多“远”。此外，太少的向量和$|PF_{known}|$的代表性可能很差;太多的向量可能会压倒DM。 ONVG Ratio很难确定$|ONVG|$的最佳值是多少。$PF_{known}$ 的基数可能在不同的计算分辨率下发生变化，也可能在拖把之间存在差异(可能是根本的)。报告$PF_{known}$的基数与离散$P_{true}$的比值可以让我们对找到的非支配向量的数量与要找到的存在向量的数量有一定的感觉。然后将这个度量定义为: ONVGR = \frac{|PF_{known}|}{|PF_{true}|}上图中，可知 $ONVG=3$，$ONVGR=0.75$。 C1如果已知由所有有效解组成的参考集R，那么看起来最自然的质量度量就是找到的参考点的比例。度量可以用以下方式定义： C1_R(A)=\frac{|A \cap R| }{ |R| }C2如果参考集不包含所有的非支配点，那么A中的被R所非支配的点集也许是属于非支配解集上的，这种情况，使用下列方法也许更可行： C2_R(A)=\frac{ | \{ u \in A | \ \nexists r \in R , r \succ u \}| }{|A|}然而，这些主要措施也有一些明显的缺点。他们对近似值的改进无动于衷。例如，考虑图11中所示的两个近似参考集，这两个近似是针对一个双目标背包问题得到的。显然，近似1比近似2好得多。近似1中的所有点都非常接近参考集，它们覆盖了参考集的大部分区域。然而，这两种近似的测度值都是相同的C1和C2。 图12中的示例说明了主要度量的另一个缺点。这两个近似由5个不占主导地位的点组成，所以它们的基数测度是相等的。然而，构成逼近3的所有点在目标空间中都是非常接近的，即它们代表了非支配前沿的同一区域。另一方面，近似值4的点分散在整个参考集合中。它们携带着丰富得多的信息，例如关于可能的目标范围的信息。这个例子表明，对于基本测度，无论它们的接近程度和关于非支配集形状的信息如何，逼近中的每个点都具有相同的权重。 Ratio of non-dominated individuals (RNI)这个绩效指标被定义为非主导个体的比率(RNI)对于给定的总体X， RNI(X)=\frac{nondom\_indiv}{P}nondom_indiv是种群X中非支配的个体数量，P是种群X的大小。因此，RNI = 1的值表示种群中所有的个体都是不受支配的，RNI = 0表示种群中没有一个个体是非支配的。由于通常需要大于零的总体大小，所以在$0 \leq RNI \leq 1$的范围内总有至少一个非支配个体。]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>CardinalityQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Uniformity]]></title>
    <url>%2F2019%2F01%2F26%2Funiformity%2F</url>
    <content type="text"><![CDATA[又看了好几天的剧。。。罪恶感啊 介绍均匀性的质量指标衡量集的解分布的均匀程度。由于解集的质量可以看作是其表示PF的能力，所以一个均匀分布的解集比一个非均匀分布的解集提供更好的帕累托前沿表示，可以认为它具有更好的质量。理想的均匀性QI应该排在由完全相等的解组成的集合的最高位置，对这个集合的一点干扰会导致更糟糕的评估结果。具体的indicators如下: 通常可以通过测量解之间距离的变化来评价解集的均匀性。这个类中的许多QIs都是按照这些思路设计的，例如，$spacing(SP)$[39],$deviation \ measure \Delta$[35] , $ uniformity \ distribution$[43], $\ minimal \ spacing$[38], $ spacing \ measure$[37] 和$uniformity$[41]。 其他则考虑解之间的最大最小距离[40 36 44]，和构建集群[34]或最小生成树[42]。 Minimal spacing前言先介绍spacing： S=\sqrt{\frac{1}{|Q|} \sum_{i=1}^{|Q|}(d_i-d)^2 } $d_i=min_{k\in Q\ and\ k \ne i}\sum_{m=1}^M|f_m^i-f_m^k|$ 。离$Q_i$最近的点的距离，距离公式每一维度(目标函数)的差值的平均值。 $f_m^i​$：在最后的非支配解$Q​$中第$i​$个解的第$m​$个目标函数值。 $d$：所有$d_i$的平均值。 $S$越接近0越说明解集是更加的均匀分布的帕累托最优前沿。 但此算法在上图所示中便展现出缺点： 可以直观的看出fig(b)的一致性比fig(a)要好，但通过公式却体现出相反的结论。 原因：离a最近的是b，离b最近的a，离c最近的是d，离d最近的是c。那么S值一定比fig(b)的低。而忽略了fig(a)中b与c之间很大的距离。 正文此算法更像是一个流程，总结下来就是把每个解看成一个点，每一个点只访问一次，求把所有点连起来的距离总和的最小值。 把所有点设为unmarked状态，随机找一个解，作为seed，此点变为marked。 在所有的unmarked点中，找到离刚刚设为marked/seed点最近的点。此点设为marked， 依次循环(2)，直至所有点均是marked，并记录路径的距离和。 把每个点都作为seed，取路径和最小。最后再处以$|Q|-1$ 其中，由于每个目标函数的性质可能不同，他们的取值范围也就可能不同，距离公式归一化修改为： d_i=\frac{1}{|F^{max}_m-F^{min}_m|}min_{k\in Q\ and\ k \ne i}\sum_{m=1}^M|f_m^i-f_m^k|$F^{max}_m$，$F^{min}_m$第m个目标的最大值和最小值。 如此算法,易得fig(b)的值会比fig(a)更小，更有效！ Spacing(SP)SP 测量一组解集之间解的距离变化。特别的，$A = \{a_1,a_2,…,a_N \}$, SP(A)=\sqrt{\frac{1}{N-1} \sum^N_{i=1} (\bar{d} - d_1(a_i,A/a_i))^2 }其中： $\bar{d}$ 是所有 $d_1(a_1,A/a_1)$ $d_1(a_2,A/a_2)$ $d_1(a_2,A/a_2)$,…, $d_1(a_N,A/a_N)$ 的平均值，$d_1(a_i,A/a_i)$ 是 $a_i$ 对 $A/a_i$ 的一范数(Manhattan distance)， d_1(a_i,A/a_i)=\min_{a \in A/a_i} \sum_{j=1}^m|a_{ij}-a_j|$m$ 是目标函数的个数，$a_{ij}$ 是第 $a_i$ 的解的第 $j$ 个目标的值。SP被最小化;数值越低，均匀性越好。SP值为0表示解集的所有成员在曼哈顿距离的基础上间距相等。请注意，SP仅测量解决方案的“邻域”分布。即使与MS一起工作，SP也不能涵盖集合的多样性质量，尽管这两个指标在文献中经常一起使用来达到这一目的。以下图为例，图2(b)和(c)中的解集均采用SP和MS满分;然而，它们分别位于帕累托前沿的边界和极端点。 Spacing metric假设有两个目标函数， spacing = \left[ \frac{1}{N-1}\sum_{i=1}^{N-1}(1 - \frac{d_i}{\bar{d}}) \right]为了计算$d_i$，我们考虑第一个目标，将$PF$中的所有点按升序排序。接下来，为了计算$d_i$，我们使用下面的公式: d_i = \sqrt{(f_1(\vec{x_i} )-f_1(\vec{x_i+1}) )^2+((f_2(\vec{x_j} )-f_2(\vec{x_j+1}) )^2}$\bar{d}$ 便为 $d_i$ 的和的平均值。 Deviation measure $\Delta$由于优化解的多样性是多目标优化中的一个重要问题，我们设计了一种基于最终总体中最优非支配前沿解之间连续距离的度量方法。将得到的第一组非优解与均匀分布进行比较，计算偏差如下:(这个有特殊的背景才可适用) \Delta = \sum_{i=1}^{|\mathcal{F}_1|}\frac{|d_i-\bar{d}|}{|\mathcal{F}_1|}$\mathcal{F} = \{ \mathcal{F}_1,\mathcal{F}_2,… \}$ 是所有的非支配前沿。 为了确保这种计算考虑到解在真实前沿的整个区域的扩散，我们将边界解包含在非主导锋$\mathcal{F}_1$中。对于离散的帕累托最优前沿，我们为每个离散区域计算上述度量的加权平均值。在上式中，$d_i$是目标函数空间中最终总体的第一非支配前沿上两个连续解之间的欧式距离。参数$\bar{d}$是这些距离的平均值。 Cluster ($CL_\mu$)需要先介绍 Number of Distinct Choices ($NDC_\mu$). $NDC_\mu$从设计者的角度来看，所观察到的帕累托解集中包含的点越多，可供选择的设计选项就越多。然而，如果观测到的帕累托解在目标空间中过于接近，那么对于设计者来说，观测到的帕累托解之间的变化可能无法区分。换句话说，观察到的帕累托解的数量越多，并不一定意味着设计选择的数量越多。简而言之，对于一个观察到的帕累托解集$p=(p_1,…,p_{\bar{np}})$ ，只有那些彼此之间有足够差异的解决方案才应被视为有用的设计选项。 设数量$\mu , \ (0 &lt; \mu &lt;1)$为设计人员指定的数值，可将m维目标空间划分为$1/\mu^m$的小网格。为了简单起见，将$1/\mu$作为整数。每个网格都是指一个正方形(m维中的超立方体)，即无差异区域$T_{\mu(q)}$，其中区域内任意两个解点$p_i$和$p_j$都被认为是相似的，或者设计人员对这些解不感兴趣。下图给出了二维目标空间中的量$\mu$ 和 $T_{\mu(q)}$。 $T_{\mu(q)}(q,P)$ 表示是否有任何点$p_k \in P$属于区域$T_{\mu}(q)$。当至少有一个解点$p_k$落在无差异区域$T_{\mu}(q)$中时，$T_{\mu(q)}(q,P)$等于单元(或1)。$T_{\mu(q)}(q,P)$等于0(或0)只要$T_{\mu}(q)$区域没有解。一般来说，$T_{\mu(q)}(q,P)$可以表述为: T_{\mu(q)}(q,P) = \begin{cases} 1 & \exists p_k \in P \ p_k \in T_\mu(q)\\ 0 & \forall p_k \in P \ p_k \notin T_\mu(q) \end{cases}质量度量$NDC_{\mu}(q)$，即预先指定的m值的不同选择的数量，可以定义为: NDC_{\mu}(P)=\sum_{l_m=0}^{v-1}...\sum_{l_2=0}^{v-1}\sum_{l_1=0}^{v-1}NT_\mu(q,P)where $q = (q_1,q_2,…,q_m)$ with $q_i=\frac{l_i}{v} $ 其中，$v=1/\mu$ ，点 $q$ 位于目标空间m-网格线的任意交点上，坐标为$(q_1,q_2,…,q_m)$。如本节开头所示，，如果想让$NDC_{\mu}(P)$值较高的观察到的Pareto解集，对于预先指定的 $\mu$ 就要有相对于较低的值(网格越密，被删去的点就越少)。 正文上一节的质量度量，即 $NDC_{\mu}(P)$。然而，仅使用这个质量度量，无法正确解释集群现象。例如，假设有一个预先指定的m值，观察到的Pareto解集$P_1$提供了10个不同的解，有$NDC_{\mu}= 10$。现在假设，这里有另一组解$P_2$ 它提供了100个解，$NDC_{\mu}=10$ 。可以看出，设计人员并不希望看到解决方案集P2，因为该集中的许多解决方案可能是集群的。因此，引入了质量度量集群$CL_\mu(P)$: CL_\mu(p)=\frac{N(P)}{NDC_\mu(P)}其中$N(P)$为观察到的帕累托解的个数。在理想情况下，得到的每一个帕累托解都是distinct的，那么数量$CL_\mu(p)$的值等于1。在所有其他情况下，$CL_\mu(p)$都大于1。此外，集群数量$CL_\mu(p)$的值越高，解决方案集的集群化程度就越高，因此解决方案集的受欢迎程度就越低。 Hole relative size当我们看到这组度量标准，特别是间距度量(spacing metric)标准时，我们意识到它们有时在显示沿帕累托边界(帕累托边界上的一个洞)的点分布的不连续时是不准确的。因此，为了克服这个缺点，我们设计了一种新的度量，称为孔相对大小(hr)。 HRS度量允许计算沿帕累托边界分布的点的最大孔的大小。然后用孔的大小除以点与点之间的平均间距进行归一化。如下所示： HRS = \frac{\max_i d_i}{\bar{d}}$d_i$ 两个相邻解的距离。 $\bar{d}$ 点间的平均距离。 这个度量比间距度量提供的信息更多，但是，在尝试规避间距度量中的一个缺点时，我们在HRS度量中引入了另一个缺点:它不能在不连续的帕累托边界上工作。事实上，不连续的帕累托边界有天然的漏洞。因此，对于帕累托边界上的固定数量的解，HRS度规总是以高概率测量相同的值。因此，我们建议在不连续测试问题中不要使用这个度量。 Uniformity assessment(没看懂，心力憔悴)]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>UniformityQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QIs for Spread]]></title>
    <url>%2F2019%2F01%2F23%2Fspread%2F</url>
    <content type="text"><![CDATA[玩了好几天，看了好多剧，所以这几天的进度稍微有点慢，另外，《一起同过窗》真香！ 延展特性涉及解集覆盖的区域。一个具有良好分布的解集应该包含来自PF每个部分的解集，而不遗漏任何区域。然而，大多数扩展的QIs只度量解决方案集的范围(extent)。下表为总结： 这些QIs通常考虑the range formed by the extreme solutions of the set(由集合的极值解构成的范围)，例如maximum spread，和它的变体[25] [27] [28] [29] [30] [33]，或者 考虑the range enclosed by the boundary solutions of the set(集合的边界解所围成的范围)，实例如下图： 只考虑这些解的QIs可能会忽略PF的内部区域。 幸运的是，确实存在一些为解决方案集的整个覆盖范围设计的QIs。 例如，[23]测量解集的支持点的面积和长度；[24]计算解集在PF的最大不相似度；[31]将每个解与解集的其余解的不同之处相加。 Maximum Spread (MS)$MS(or \ \mathcal{M}_3^*)$ 被广泛的使用于延展性indicator。它通过考虑每个目标的最大范围来度量解决方案集的范围，公式为： MS(A) = \sqrt{\sum^m_{j=1} \max_{a,a' \in A} (a_j-a_j')^2 }m 是目标函数的个数。MS应求极大值。值越高，说明的可延展性越好，在二元目标情形下，非支配解集的MS值为其两个极值解的欧氏距离。 但是，如前所述，MS只考虑集合的极值解，不能反映扩散的特性。此外，由于它不涉及集合的收敛性，远离PF的解通常对MS值有很大贡献。这很容易引起误导性的评价。例如，一个解集集中于PF的一小部分，但有一个离PF很远的离群值，那么它的MS值就很好。为了解决这一问题，引入帕累托前缘的范围作为评价的参考，例如[27] [28]。 Extension规定： \min_{x \in X} F(x) = (f_1(x),f_2(x),...,f_l(x))\\ U_i = \max_{x \in Pareto(U)}f_i(x)\\ L_i = \min_{x \in Pareto(U)}f_i(x)\\ i=1,...,l令：$P_r = \{ F_1^1,…,F_l^1 \}$，其中，$F_i^1=(L_1,…,L_{i-1},U_i,L_{i+1},…,L_l),[i=1,…,l]$ 规定： d_r^p=\min\{ d(p_r,p)|p \in P \},p_r \in P_r因此，得表达式： EX=\sqrt{\sum(d_r^p)^2 }/l易知，$d_r^p$ 越小，说明有更好的延展性。 如果是三维图的话，$P_r$ 分别如下： Modified MS(勿看，瞎记的)只有知道正常和期望的条件，才能定义和避免异常和不期望的条件，例如解在目标空间的次优区域的分散，或者收敛到感兴趣区域之外的次优解。换句话说，为了克服收敛性和多样性的矛盾要求，需要一个应用相关的尺度来定义低、理想和高多样性的近似概念，这在高维问题中尤为明显。在所提议的机制的上下文中，决策人员DM1(通常，最好是领域专家)只需要对所需折衷表面的定义极值提出近似估计。这些极值将作为包含理想的PF的超立方体的顶点。 I_s = D / \left[ \sum_{m=1}^M \left( \max_{z_* \in Z_*}\{z_{*_m}\} - \min_{z_* \in Z_*} \{z_{*_m} \} \right)^2 \right]^{1/2}$z_t \in Z_t$ 可以表示PF的目标集。$I_s$ 能取任何正的实数值。理想情况下，要找到一个接近统一($I_S = 1$)的指标值(理想的多样性)。小于1 ($I_S &lt; 1$)的指示值表示与期望的解决方案的扩展相比，操作的解决方案之间的多样性较低。另一方面，指示符值大于1($I_S &gt; 1$)突出了目标空间中解的过度分散(高多样性)。这种超空间的过度分散很可能导致解与PF的发散，并通过引入循环行为，迫使MOEA反复探索空间中以前访问过的区域，从而阻碍了优化过程。 第二个多样性管理机制是DM2，它预测NSGA-II中使用的多项式突变算子可能会使潜在的解点广泛分散。DM2试图通过引入自适应突变算子，以一种可控的方式控制这种离散。这个新的变异算子试图定义组决策变量的变异范围在每一代的基础上的多样性程度的局部non-dominated集解决方案,为每个单独的决策变量中设置,在当地的多样性以NSGA-II年代拥挤的措施。 计算第i代近似集的扩展指标。 if $I_s&lt;1$ 在变异选择和生存选择过程中激活多样性促进机制。 Else If $I_s \geq 1$ 在变异选择和生存选择过程中，失活多样性促进机制。 Coverage error $\epsilon$$\epsilon$ 的概念： 解释一下就是：有两个集合$D,Z$，$D$ 是 $Z$ 的一部分，如果想要用 $D$ 代表 $Z$，那么就要用符号 $d_{\epsilon}$ 表示。并规定，遍历 $Z$ 中的每一个点，画一个圆，半径是 $\epsilon$ ，都要有 $D$ 中的解存在，并且找最小的 $\epsilon$。 $\delta$ 的概念： 翻译一下：这个是单对 $D$ 集合来说的，$D$ 中两两点的最小距离。 例子如下：实心 + 空心 = Z；实心 = D 因此 $\epsilon$ 要尽可能的小，$ \delta$ 尽可能的大。 \epsilon = \max_{z \in Z} \min_{x \in D} d(z,x)For a fixed element $z$ of $Z$, how well it is covered is determined by the closest point to $z$ in the representation $D$. How well the entire set $Z$ is covered depends on how well an arbitrary element of $Z$ is covered, and thus the coverage error \epsilon$ is equal to the maximum of coverage error quantities for individual points in Z. Similarly, the uniformity level $\delta$ is determined by the quantity. \delta=\min_{x,y \in D,x \ne y}d(x,y)the fact that $D$ is of finite cardinality, computing the uniformity level $\delta$ is simple as long as the metric $d$ is computable. PD PD(X) = \max_{s_i \in X}(PD(X-s_i)+d(s_i,X-s_i))where d(s,X)=\min_{s_i \in X}(dissimilarity(s,s_i))$d(s_i,X-s_i)$ 是从一个物种 $s_i$ 到另一个种群 $X$ 的相异度。 下图提供了一个方式展示了PD是如何计算的，在左图,解$s_i$和其他方案 $X−si$ 视为两个社区，他们的多样性之和是 $X−si$ (black dots)的和 与 $si$ 到 $X−si$ 的相异值的和组成： 每个解与整个总体的不同之处是可以计算的，每个解都与其最近的未复制邻居相关联。然后，这些差异的和导致了整个种群的多样性，可以看作是X的结构，上右图所示，(其中较暗的线比较亮的线连接得早)。具体算法如下： 其中： $d$ 是n*n的矩阵，例如(i,j)就是 第i个解与第j个解的p范数距离($L_p-norm$)，因此是对称矩阵。 $min(d,[],2)$ 出自于matlab语法，对每一行取最小值，因此输出是一列。 另外，这位老师居然还是我们学校的老师，在电院，好奇翻了一下个人主页，居然有代码！我会附录在本博客最后，其中中文为我注释。 不同的相异评价在计算PD占很重要的作用。通常采用两个解之间的距离作为它们的相异之处。但是请注意，欧几里得距离不太适合在高维空间中测量邻域。由于MaOPs的解分布在高维目标空间中，基于$L_2$范数的欧氏距离不适用于PD中的不相似度计算。 从下图中我们可以清楚地看到，p越小，各维$L_p$对0越敏感。相反，基于$L_p-norm-based$的距离测度不适用于测量p&gt;1的高维数据的差异性。因此，为了测量MaOPs的多样性，需要将p设置为p &lt; 1。已有研究表明，只要p&lt; 1，该测度的有效性对p不敏感。因此，p在PD中不是一个参数，本文将p设为0.1。 指示器使用单个标量值来描述m维分布。因此，无论哪个指标，都会丢失一些信息。因此，尽管不同的指标可能捕获不同的信息，但希望捕获一些关键信息。当得到PF f1 +f2 +f3 = 1的三个极值点时，在这三个极值点的集合中加入不同的解，多样性度量的值是不同的。下图为在三个极值点集合中加入PF的另一个解时PD、MS、NDC (b =4)、熵(b =4)的变化值，其中颜色表示矩阵的大小(颜色较深的点值小于颜色较浅的点值)。如果根据这些指标选择一个解决方案以增加多样性，下图中较亮的部分优先于较暗的部分。一旦得到极值点。MS值达到最大值。因此，没有任何解决方案能够改进MS。虽然中间部分是由NDC和熵推动的，但解在网格内是无法区分的。对于PD，中间部分提升，值不断变化。从图4可以看出，PD通常可以促进不同的解决方案。 Overall Pareto Spread当设计的目标函数都被考虑时，总体的PF延展性度量量化了所观测的目标在目标空间中的延展能力。这个度量被定义为两个超矩形的体积比，其中一个是 $HR_{gb}$ ，它对于每一个所设计的目标的好点与坏点。类似地， $HR_{ex}$ 定义了所观察到的Pareto解集的极值点。整个PF的延展性变为$HR_{gb}$与 $HR_{ex}$ 之比： OS(P)=\frac{HR_{ex}(P)}{HR_{gb}}$P$ 是所观测的Pareto解，$m$ 为目标函数个数，其中： OS(P)= \frac{\prod_{i=1}^{m}|\max_{k=1}^{\bar{np}} (p_k)_i -\min_{k=1}^{\bar{np}}(p_k)_i |}{\prod_{i=1}^{m}|(p_b)_i-(p_g)_i|}\\ =\prod_{i=1}^{m}| \max_{k=1}^{\bar{np}}[\bar{f_i}(x_k)] -\min_{k=1}^{\bar{np}}[\bar{f_i}(x_k)] | 例如，在图4所示的两个目标空间中，PF-spread的计算公式为: OC(P) = \frac{h_1h_2}{H_1H_2}其中： $P_1,P_2$ 是两个Pareto solution sets。if $OS(P_1)&gt;OS(P_2)$, then the solution set P1 is preferred to P2 . h_1=|\bar{f_1}_{max}-\bar{f_1}_{min}|\\ h_2=|\bar{f_2}_{max}-\bar{f_2}_{min}|\\ H_1=|(p_g)_1-(p_b)_1|\\ H_2=|(p_g)_2-(p_b)_2|PD’s code123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990% Authors: Handing Wang, Yaochu Jin, Xin Yao% University of Surrey, UK, and University of Birmingham, UK% EMAIL: wanghanding.patch@gmail.com, yaochu.jin@surrey.ac.uk, X.Yao@cs.bham.ac.uk% WEBSITE: http://www.surrey.ac.uk/cs/people/handing_wang/% DATE: March 2016% ------------------------------------------------------------------------% This code is part of the program that produces the results in the following paper:% Handing Wang, Yaochu Jin, Xin Yao, Diversity Assessment in Many-Objective Optimization, Cybernetics, IEEE Transactions on, Accepted, 10.1109/TCYB.2016.2550502.% You are free to use it for non-commercial purposes. However, we do not offer any forms of guanrantee or warranty associated with the code. We would appreciate your acknowledgement.% ------------------------------------------------------------------------function [ pd ] = PD( X )% Usage: [ pd ] = PD( X )%% Input:% X -Objective values of the population n*m (n solutions with m objectives)%% Output: % pd -PD value of population X%p=2;%lp norm setting0.1C=zeros(size(X,1),size(X,1));%connection arrayD=zeros(size(X,1),size(X,1));%dissimilarity array%Calculate the dissimilarity between each two solutionsfor i=1:size(X,1)-1 for j=i+1:size(X,1) d=sum(abs(X(j,:)-X(i,:)).^p,2).^(1/p); D(i,j)=d; D(j,i)=d; endendDMAX=max(max(D))+1;D(logical(eye(size(D))))=DMAX;n=size(X,1);pd=0;for k=1:n-1 %Find the nearest neighbor to each solution according to D in each row. [d,J]=min(D,[],2); %Find solution i with the maximal di to its neighbor j [dmx,i]=max(d); while liantong(C,i,J(i))==1 %i and j are connected by previous assessed solutions if D(J(i),i)~=-1 D(J(i),i)=DMAX; %Mark the connected subgraph end if D(i,J(i))~=-1 D(i,J(i))=DMAX; end [d,J]=min(D,[],2); %Find solution i with the maximal di to its neighbor j [dmx,i]=max(d); end C(J(i),i)=1; C(i,J(i))=1; pd=pd+dmx; if D(J(i),i)~=-1 D(J(i),i)=DMAX;%Mark the used dissimilarity di. end D(i,:)=-1;%Mark the chosen solution iendendfunction [w]=liantong(C,I,J)% Usage: [w]=liantong(C,I,J)%% Input:% C -Connection array% I -index I% J -index J%% Output: % w -1 if solutions I and J are connected, 0 if solutions I and J are not connected.%V=I;Child=find(C(V,:)==1);if isempty(find(Child==J))==0 % 直接连接 w=1; returnelse C(V,:)=0; % 删掉点I C(:,V)=0; for i=1:size(Child,2) % 遍历连接点I的其他点 w=liantong(C,Child(i),J); % 进行递归 if w==1 return end endendw=0;end]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>SpreadQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convergence--Distance-based QIs]]></title>
    <url>%2F2019%2F01%2F21%2Fdistancebased%2F</url>
    <content type="text"><![CDATA[可谓呕心沥血，翻译的累死我了，这篇是关于收敛性的indicators的《Distance-based QIs》。 分类可以进一步分为两类： 测量所考虑的解集到从帕累托前缘导出的一个或多个特定点的距离：the ideal point, knee point(s) , the Zeleny point and the seven particular points。 the ideal point是由帕累托前缘各目标的最优值所构造的点。 knee point(s)是帕累托前缘上的点，它具有从相邻点计算出的最大反射角。 the Zeleny point是通过分别最小化每个目标得到的点。 the seven particular points是由二元多目标问题的帕累托前缘的理想点和极值点导出的7个特殊点 测量到一个很好地表示帕累托前沿的reference set的距离。在这一组中，最常用的指标是GD。GD首先计算解集中每个解到参考集中最近点的欧氏距离，然后求所有这些距离的二次平均值。这一组中的其他QIs可以看作是GD的变体，例如taking the arithmetic mean of the distances，the powermean，considering the Tchebycheff distance，introducing the dominance relation between solutions and points in the reference set。 measure the distance of the considered solution set to one or several particular points derived from the PFTchebycheff distance to the knee point d(z,z^*,\lambda)=\max_{1 \leq j \leq k}\{ \lambda_j |z_j^* - z_j| \}其中： k：目标函数的数量。 $\lambda_i=\frac{1}{R_i}$，$R_i$是第i个目标函数的范围(range)。 Defined this way, the knee of the Pareto front is the point in the feasible objectivespace, $ \Lambda$, which corresponds to $ \min_{z∈\Lambda} d(z, z^∗, \lambda)$. 如果一个偏好关系的$PF_{approx}$集合比另一个关系的$PF_{approx}$集合包含更多的膝节点周围的解，则该偏好关系优于其他关系。 Seven point average distance该问题用于二元目标优化。 如果有效集的先验知识是可用的，MCGA完全解析E的能力可以很精确的理解为它与个体的标准和其他(已决定的)点有多接近。由于E对于任何一个测试问题都是未知的，因此为每个问题生成7点法，以衡量算法的有效性。个体准则最优约束了两个标准问题的有效集，但也可通过单独优化每一个准则而不考虑另一个准则来求出个体准则最优。有了这两点，七点法再$J_1-J_2$被定义如下原点[0,1]，最大点(在E范围内)[0,$J_2^{worst}$]和[$J_1^{worst}$,0]，和在原点与最大值之间的每个轴上的两个点。 原文： With the resulting two points at hand, the seven comparison points are denned on $J_1-J_2$as the origin [0,0], the maximum (within the range of E) of each criterion [0, $J_2^{worst}$] and [$J_1^{worst}$, 0], and two points on each axis between the origin and the maximum value. 全距离测量是通过距离处以7，该距离是从七个点中每一个点到离此点最近的MCGA种群的点的距离和。因此，每次创建距离度量时，使用总体中的7个成员。他的优点是比较不同人群在某一特定问题上的相对优势的准确方法。对于给定的问题，距离度量值最小的总体将是最接近E的总体。 这七个点具体是什么我实在没有翻译出来，在查找文献时《Evolutionary Algorithms for Solving Multi-Objective Problems 》作者：Carlos Coello Coello， David A. Van Veldhuizen， Gary B. Lamont时，有如下叙述： measure the distance to a reference setGenerational Distance (GD)GD首先计算解集中每个解到参考集中最近点的欧氏距离，然后取所有这些距离的二次平均。 公式： GD(A)=\frac{1}{N} ( \sum_{i=1}^{N} (d_2(a_i,PF)^2)^{1/2}a solution set $A=\{ a_1,a_2…,a_N\}$ $d_2(a_i,PF)$是$a_i​$到PF的2范式距离(欧几里距离) 在实际应用中使用了一个很好地表示PF的参考集R。 d_2(a_i,PF)=\min_{r \in R}d_2(a_i,r)$d_2(a_i,r)$是$a_i$与$r$的欧几里距离。如果前端的几何性质是已知的，GD不一定需要一个表示PF的引用集。 GD的值理应是要极小的。如果值为0表明该集合位于Pareto front /reference set中。作为为后代间的代际评估而设计时，GD通常用于度量solution set 向PF的演化过程。然而，由于GD考虑的是二次平均值(quadratic mean)，因此它对异常值非常敏感，无论其他解的表现如何，它都会返回一个异常值得分很低的解集。当$ N \rightarrow \infty, \ GD \rightarrow 0 $，尽管这个集合远离PF。因此，只有当考虑的集合具有相同/或非常相似的大小时，GD才可靠地可用。幸运的是，公式中的算术平均数代替二次平均数，这个问题可以解决。事实上,在一些最近的研究，GD指标的一般形式的指数“p”和“1 /p”而不是“2”和“1/2”。设置p = 1现在已经被普遍接受，并与它的反转版本IGD一起使用(度量从帕累托前的点到所考虑集合中最近解的距离的算术平均值)。 来自“Measuring the Averaged Hausdorff Distance to the Pareto Front of a Multi-Objective Optimization Problem”的下文： 虽然在许多研究中使用了GD，但并不是EMO社区的所有研究人员都接受GD。我们推测一个可能的原因(可能是主要的原因)是它的归一化策略，如下面的例子所示:假设我们有一个(任意的)点$a \in Q$，在不丧失通用性的情况下，让图像F(a)到PF的距离为1。现在将 archive $A_n$定义为由a的n个副本给出的multisets，即$A= {a,…,a}$。“平均”距离的F(A)向PF，有: GD(F(A_n),F(P_Q))=\frac{||(1,...,1)^T||_p}{n}=\frac{\sqrt[p]{n}}{n}我们可以看到，随着n的增加，近似质量就会变得越来越“好”，尽管估计值并没有怎么变，archives $A_n$甚至收敛到“完美”估计： \lim_{x \to \infty}{GD(F(A_n),F(P_Q))=0}由上述的结果可以推广:例如，我们可以考虑a的小扰动，而不是multisets。或者，如果$F(A)​$是有界的，不管$A_n​$的a是否被支配，也不管$F(a)​$离PF有多远，甚至满足$|A_n|=n​$的任意archive序列任何$A_n​$都能被选择。因此，在EMO上下文中，从这个角度来看，用进一步的、甚至占主导地位的解决方案“填充”归档文件是有好处的，因为通常较大的集合会产生更好的GD值。在社区中，它的建立是为了固定种群大小，以便对不同的算法进行比较(例如，N = 100)。然而，这给基于不受先验定义值限制的存档的MOEAs带来了麻烦。因此，“完美的”归档器(关于GD)可以接受所有(或至少是尽可能多的)候选解决方案。这当然不是我们想要的效果。 为解决以上问题，便提出了$GD_p$： $GD_p$ GD_p(X,Y)=\left(\frac{1}{N} \sum_{i=1}^{N}dist(x_i,Y)^p\right)^{1/p}$dist(x_i,Y)=\inf_{v \in Y}||x_i,v||$，$\inf$ 为下界(最小值)。 公式上的区别：把$\frac{1}{N}$在$()^{1/p}$从放括号外变为括号里。 我们把这个新指标命名为$GD_p$(索引p)只区分经典版本，这是需要在这项工作中进一步比较。“新”指标不具有上述讨论的不需要的特征，因此在比较具有不同大小的集合时似乎更为公平。特别是，大型候选集不再必须是“好”的。例如上例中$GD(F(A_n),F(P_Q))=1$ 对于所有的$n \in \mathbb{N}$ 。 \min_{x \in Q}{F(x)}\\ F(x) = (f_1(x),...,f_k(x)),the \ vector \ of \ the \ objective \ functions命题1：令$k=2​$(二元目标优化问题)，$F(P_Q)​$是连接的，有$a,b\in Q​$，有： a \prec b \ \Rightarrow \ dist(F(a),F(P_Q)) 0$dist(F(b),F(P_Q))$是固定值 r($r \ne 0$)，以$F(b)$为圆心，r为半径画一个圆，交点便是$p_b$(有点圆与$P_Q$相切的感觉)。分情况讨论： 当 $a \in P_Q$ 时，那么$dist(F(a),F(P_Q))=0$ ，以此得结论结果。 当 $a \notin P_Q $ 时 当 $ p_b \prec a$ 时 因为$a \prec b$ dist(F(a),F(P_Q)) \leq||F(a)-F(p_b)|| < ||F(b)-F(p_b)||=dist(F(b),F(P_Q)) 当 $p_b \nprec a$ 时，也就是 $p_b$ 和 $a$ 互相非支配，那么应该存在$i,j \in \{1,2\}, i \ne j$ f_i(p_b) < f_i(a) \ \ and \ \ f_j(p_b) > f_j(a） ​ 因为$a \notin P_Q$，那么也会存在$p_a \in P_Q$ 令 $p_a \prec a$(满足上面两个都是小于号) ，因为$F(P_Q)$ 是 ​ 连贯的( index from &gt; to &lt; 一定有一个=)，这存在一条$F(p_a)$到$F(p_b)$ 的路径， ​ 那么一定存在 $\bar{p} \in P_Q \ let: \ f_j(\bar{p})=f_j(a)$ ，又因为 $\bar{p}$ 和 $p_b$ 互相不支配(同在$P_Q$)， ​ 那么有： dist(F(a),F(P_Q)) \ \leq \ ||F(a)-F(\bar{p})|| \ = |f_i(a)-f_i(\bar{p})| \ < \ |f_i(b)-f_i(p_b) | \\ \ \leq \ ||F(b)-F(p_b)|| \ = \ dist(F(b),F(P_Q))证明完毕。其中要解释一下，为何： |f_i(a)-f_i(\bar{p})| \ < \ |f_i(b)-f_i(p_b) | $f_i(b) &gt; f_i(a)$ ，这是因为 $ a \prec b$ $f_i(p_b) &lt; f_i(\bar{p})$，这个比较麻烦QWQ $\bar{p} \ and \ a $ = $\begin{cases} f_j(\bar{p})=f_j(a) &amp; (1 \\ f_i(\bar{p}) &lt; f_i(a) &amp; (2 \end{cases}$ $ p_b \ and \ a $= $\begin{cases} f_j(p_b) &gt; f_j(a) &amp;(3 \\ f_i(p_b ) &lt; f_i(a) &amp;(4 \end{cases}$ ​ $ (1,(2 \Rightarrow f_j(p_b) &gt; f_j(\bar{p}) $ ，又因为 $\bar{p}$ 和 $p_b$ 互相不支配，那么$for \ i \ must \ be:f_i(p_b) &lt; f_i(\bar{p})$ 一个有趣的问题当然是如果拖把涉及两个以上的目标会发生什么。但是，我们不得不把这个问题留到以后调查。 当帕累托前缘断开时，上述结果不成立。然而，如果一个元素足够接近帕累托集合，这种“单调行为”仍然成立。下面的例子和命题分别给出了反例和证明。 例如：$F(P_Q)=\{(10,0)^T,(0,1)^T \}$ , $F(a)=(11,3)^T,F(b)=(5,2)^T \ so \ a \prec b, but$ $dist(F(b),F(P_Q)) = \sqrt{1^2 + 3^2}=\sqrt{10} &lt; \sqrt{29}=\sqrt{5^2 + 2^2} = dist(F(a),F(P_Q))$ 命题2： 翻译一下就是：对于一个$k$个目标的问题，任何一个维度$i$，存在$y(a,i)$的目标值向量属于$F(P_Q)$，并且满足$y(a,i)$在除了第$i$维度上的值与 $F(a)$ 相同,（第$i$维任意）。【其中与命题1的差别是，在1中$k = 2$，但在此命题中，并没有这个限制】 证明：推到与前一个类似，只是推广到高纬度上了$k&gt;2$。 因为$P_Q$是紧凑的，所以一定存在 $p_b\in P_Q$，满足： dist(F(b),F(P_Q)) =||F(b)-F(p_b)|| 当 $ p_b \prec a$ 时 因为$a \prec b$ dist(F(a),F(P_Q)) \leq||F(a)-F(p_b)|| < ||F(b)-F(p_b)|| 当 $ p_b \nprec a$ 时，存在$i \in \{1,…k\}$，满足$f_j(p_b) &gt; y(a,i)_i$ (翻译一下：一个解y(ami)，它的第i维满足$f_j(p_b)$ 与，其他维度的数值与$a$相同)并且： dist(F(a),F(P_Q)) \ \leq \ ||F(a)-y(a,i)|| \ = f_i(a)-y(a,i)_i \ < \ f_i(b)-f_i(p_b) \\ \ \leq \ ||F(b)-F(p_b)|| \ = \ dist(F(b),F(P_Q)) 这个结果的关键是投影$y(a, i)$的存在性，$F(a)$足够接近帕累托前沿，在这种情况下不需要$F(P_Q)$的连通性。如下图： 总结，假使PF是连贯的(至少对于k = 2)，主导解(dominating solutions) $a$ 产生更好的 $dist$ 值比其被支配解(dominated points)$b$。此外，这个依然保留的话，要么当F (a)是“足够远”帕累托前面(在这种情况下，声明：$dist(F(b),F(P_Q))=||F(b)-F(p_b)|| &gt; 0$，则必须 $p_b$ 支配 $a$ )，要么就足够接近(命题2)。 从GDp的角度来看，这些结果可以解释为:如果新的归档结果来自于前一个归档，用一个支配解替代了一个被支配解，那么$GD_p$值就会下降。对于$A1 = \{b, x_2，…， x_n\}$， $A2 = \{a, x_2，…， x_n\}$，其中$a$和$b$为上式，则为: GD_p(F(A_2),F(P_Q)) < GD_p(F(A_1),F(P_Q))然而，下面的结果更为普遍，则需要进一步的假设： 命题3： $A,B \subset \mathbb{R}^n \ be \ finite \ sets \ such \ that​$ $ \forall a \in A \ \exists b \in B:F(b) \leq_p F(a) $ $ \forall b \in A \ \exists a \in B:F(b) \leq_p F(a) $ $ \exists b \in B \backslash A ,\ \exists a \in A\backslash B:b \prec a$ $ \forall a \in A \ \forall b \in B:if \ a \prec b \Rightarrow dist(F(a),F(P_Q))&lt;dist(F(b),F(P_Q)) $ 那么： GD_p(F(B),F(P_Q))]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>ConvergenceQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Convergence--Dominance-based QIs]]></title>
    <url>%2F2019%2F01%2F14%2Fdominancebased%2F</url>
    <content type="text"><![CDATA[此篇介绍的是QIs for Convergence的第一部分《Dominance-based QIs》。看了一部分方法的论文，剩下一部分实在看不下去了，想继续看看别的，有时间有精神回来补一下~ QIs用于收敛性收敛性作为解集质量的一个重要方面，在集的评价中受到了广泛的关注。文献中存在两类收敛QIs。一是考虑解或集之间的帕累托支配关系(表2项目1-9);另一种方法是考虑解集到帕累托前的距离，或者从帕累托前导出的一个/多个点(项目10 -22)。 Dominance-based QIsA type of frequently-used dominance-based QIs is to consider the dominance relation between solutions of two sets , such as the C indicator , $\widetilde{C}$ indicator, $\sigma- \tau- and \ \kappa-$ metrics , and contribution indicator.Other QIs concerning solutions’ dominance include wave metric, purity, Pareto dominance indicator, and dominance-based quality. The wave metric crunches the number of the nondominated fronts in a solution set. The purity indicator counts nondominated solutions of the considered set over the combined collection of all the candidate sets. The Pareto dominance indicator measures the ratio of the combined set’s nondominated solutions that are contributed by a particular set. The dominance-based quality considers the dominance relation between a solution and its neighbours in the set. $C-metric$定义为： C(A,B)=\frac{|\{b\in B:\exists a \in A,a \preceq b \}|}{|B|}$C​$一方面可以计算出$B​$中的解被$A​$中解所支配的比例部分，另一方面也可以计算出$A​$相对于$B​$的性能。 当$C(A,B)=1​$时，意味着$B​$中的所有解都被$A​$中的所$\preceq​$。 当$C(A,B)=0​$时，意味着$B​$中的所有解都无法被$A​$中的$\preceq​$。 注意：$C(A,B) \ne 1-C(B,A)$ $ C(A,A) \ne 0$ 如果$W​$是一个非支配解集，$A,B​$满足$A \subseteq W​$，$B \subseteq W​$，但$C(A,B)​$可为[0,1]中的任意一个值。 $\widetilde{C}-metric$多目标优化环境下的性能度量是评价优化器定量性能的数学工具，它通过单独考虑优化器或与其他优化器进行比较来评价的。这种方法可以与优化器在线评估和性能改进的优化器结合使用，也可以离线应用于两个或两个以上优化器的最终结果，以比较它们的性能、产生的结果的质量和/或要求的计算努力。 性能指标可以大致分为两类： 基本度量:一个标准度量满足一定要求的解决方案的数量或比例，比如度量关系。 序数或几何度量:这些方法不度量数量，而是通过考虑几何位置来度量。 \widetilde{C}(A,B)=\frac{|\{b\in B:\exists a \in A,a \prec b \}|}{|B|}$ C(A,A) = 0$ ，因为$A$是非支配解集。 对于$\widetilde{C}(A,B)$、$C(A,B)$，值越高说明B中的解受A所$\preceq$的比例越多。 如果$W$是一个非支配解集，$A,B$满足$A \subseteq W$，$B \subseteq W$，但$C(A,B)=0$。 $\widetilde{C}(A,B)$与$C(A,B)$都没有考虑到前沿的延展性(extent)与一致性(uniformity)。 上图可以看出(minimise)：A的一致性(uniformity)更好，而B集中聚到了一个区域。 但有：$ C(A,B) = C(B,A) =\widetilde{C}(A,B) =\widetilde{C}(B,A) =\frac{4}{12}​$，即使A的元素在B的大部分区段上占主导地位。 上图，尽管B有很好的延展性(extent)， 但是：$ C(A,B)=\widetilde{C}(A,B) =\frac{2}{12}$ ， $C(B,A)=\widetilde{C}(B,A) =\frac{0}{12}$ ，从$C$、$\widetilde{C}$中的值看出$A$优于$B$。 Contribution indicatorThe contribution of algorithm $PO_2$ relatively to $PO_2$ is roughly the ratio of non dominated solutions produced by $PO_2$. 规定： $C = PO_1 \cap PO_2$ 集合$W_1$为$PO_1$中支配$PO_2$的解集，集合$W_2$为$PO_2$中支配$PO_1$的解集。 集合$L_1$为$PO_1$中被$PO_2$支配的解集，集合$L_2$为$PO_2$中被$PO_1$支配的解集， 集合$N_1$为$PO_1$中不可与$PO_2$构成不可比较的解集，即$PO_1 \backslash (C \cup W_1 \cup L_1) $ 集合$N_2​$为$PO_2​$中不可与$PO_1​$构成不可比较的解集，即$PO_2 \backslash (C \cup W_2 \cup L_2) ​$ 表达式为： CONT(PO_1 / PO_2) = \frac{\frac{|C|}{2}+|W_1|+|N_1|}{|C|+|W_1|+|N_1|+|W_2|+|N_2|}可知： ​ 如果$PO_1$与$PO_2$是相同的解集，那么$CONT(PO_1 / PO_2)=CONT(PO_2 / PO_1)=1/2$ ​ 如果$PO_2$中的所有解都被$PO_1$所支配，那么，$CONT(PO_2 / PO_1)=0$。 我的理解： CONT(PO_1 / PO_2) = \frac{\frac{|C|}{2}+|W_1|+|N_1|}{|C|+|W_1|+|N_1|+|W_2|+|N_2|}\\ =\frac{\frac{|C|}{2}+\frac{|W_1|+|W_1|}{2}+\frac{|N_1|+|N_1|}{2}}{|C|+|W_1|+|W_2|+|N_1|+|N_2|}\\ =\frac{1}{2}\frac{|C|+|W_1|+|W_1|+|N_1|+|N_1|}{|C|+|W_1|+|W_2|+|N_1|+|N_2|}也就是说：对于$CONT(PO_1 / PO_2)$，如果$|W_1|+|N_1| &gt; |W_2|+|N_2|$，则大于0.5。 也就是说：$PO_1$中支配$PO_2$的解和不能与$PO_2$比较的解越多，$CONT(PO_1 / PO_2)$越大。 $\sigma-\ \ \tau- \ \ \kappa- \ metric$前言对于一个评价指标，无论是类别如何，想要使他可用，都要满足以下五个特征： Monotonicity/compatibility(单调性/兼容性)：对于两个PFs的支配关系，度量标准应该满足单调性/兼容性，例如，设度量标准为$\xi$，如果A支配B，A就应该比B好或至少不能差于B。因此 $A \succeq B \Rightarrow \xi (A) \geq \xi(B)$或严格单调$A \succ B \Rightarrow \xi (A) &gt; \xi(B)$ 。 Transitivity(传递性)：在所比较的所有PFs的完全顺序中，一个度量应该是可传递的。如果A优于B，B优于C，那么通过$\xi()$也应得出，A优于C。直接比较度量通常会在被比较的不同PFs之间产生不可传递关系。传递性通常只在引用度量和独立度量中得到保证。这是因为这两种方法都为每个PF分配一个数字，并且实数之间的比较是可传递的。 Scaling/meaningfulness(缩放性/有意义性)：目标函数通常需要进行缩放，例如进行单调变换以映射给定范围内的目标值，例如在[0,1]中。在这种情况下，一个度量应该是缩放不变的或有意义的，即，该度量不应受任何缩放的影响。尺度不变度量通常只利用解之间的优势关系，而不是它们的绝对客观值。 Computational effort(计算工作量)：此属性用于计算给定pf的度量值所需的计算资源。为了比较不同度量的性能，通常只考虑运行时复杂性作为所需的计算工作。 Additional information(附加信息)：许多指标依赖于不同类型的附加问题信息。一些假设问题的POF是已知的，而另一些则依赖于一些用户定义的依赖于问题的引用目标向量或引用PFs。因此，希望一个度量具有尽可能少的参数。 $\sigma-metric$规定：a dominates b is $a \succ b$ 原文： Sigma-metric($\sigma $-metric): The performance value, $\sigma_{ij} $, assigned to the j-th PF of the i-th optimizeris the number of solutions of the r-th optimizer which are strictly dominated by at least one solution of that PF of the i-th optimizer,where $i,r \in {1,2}$ and $i \ne r$. 公式： \sigma_{ij}=\sum_{s=1}^{F_r}\sum_{t=1}^{L_{rs}}\max_{k\in \{1,...L_{ij} \}}I(p_{ijk}\succ \succ p_{rst})具体规定如下： optimizer\ i_{th}=\begin{cases} PF_1 & |PF_1|=L_{i1} \\ PF_2 & |PF_2|=L_{i3} \\ ...\\ PF_j & |PF_j|=L_{ij}\\ ...\\ PF_{F_i} & |PF_{F_i}|=L_{i{F_i}} \\ \end{cases}\\ optimizer\ r_{th}=\begin{cases} PF_1 & |PF_1|=L_{r1} \\ PF_2 & |PF_2|=L_{r3} \\ ...\\ PF_j & |PF_j|=L_{rj}\\ ...\\ PF_{F_r} & |PF_{F_{r}}|=L_{rF_r} \end{cases}有两个优化器 $i$ (optimizer)，每个优化器都$F_i$个$PFs$，对于第$i$个优化器，第$j$个$PF$，它有$L_{ij}$个解(solutions)。而$p_{ijk}$则为第$i$个优化器，第$j$个$PF$的第$k$个解。 $I(\bullet)$如果内部true则返回1，否则返回0。 \max_{k\in \{1,...L_{ij} \}}I(p_{ijk}\succ \succ p_{rst})翻译为：对于指定的解 $p_{rst}$ 如果在第$i$个优化器，第$j$个$PF$中有$\succ \succ p_{rst} $关系的解，就为1，都没有则为0。 整体来看：对于第$r$个优化器的所有解中，被第$i$个优化器的第$j$个$PF$的所有$L_{ij}$个解所支配的个数。 因此，最大值为$optimizer\ r_{th}$的所有解的个数。 ps.原论文写的是$F_rL_{rs}$,但是我不赞同…..我认为是$\sum_{s=1}^{F_r}{L_{rs}}$，当$L_{r1}=L_{r2}=…=L_{F_r}$时与原论文一致。 $\tau-metric$原文： Tau-metric ($\tau -metric$): The performance value, $\tau_{ij}$, assigned to the j-th PF of the i-th optimizer is the number of solutions of the r-th optimizer which are weakly dominated by at least one solution of that PF of the i-th optimizer,where $i,r \in \{1,2\}$ and $i \ne r$. Further, $\tau_{ij} $may also be rewarded if the j-th PF of the i-th optimizer weakly outperforms a PF of the r-th optimizer. Since the metricis based on the concept of weak dominance,it may be done just as an attempt to take into account the compatibility of the metric with the ‘‘weak outperformance relation’’ given indefinition (8). However, it would be a new dimension of research in order to generalize the outperformance relations in terms of multiple(more than two) PFs.​ 公式： \tau_{ij}=\sum_{s=1}^{F_r}\{ [\sum_{t=1}^{L_{rs}}\max_{k\in \{1,...L_{ij} \}}I(p_{ijk}\succeq p_{rst})] + I(A_{ij} \ \vartheta_w \ A_{rs} ) \}规定： $\vartheta_w$ (weakly outperform): $A \ \vartheta_w \ B$ means $ A \succeq B $ and $\exists c \in A \ but \ c \notin B $。 ​ A不会比B差，并且A有B不存在的解。 在遍历$r_{th}\ optimizer$的$PF_s$时，如果与第$i$个优化器，第$j$个$PF$ 满足 ： $A_{ij} \ \vartheta_w \ A_{rs} $，再加1。 因此，相对于$\sigma-metric$最大值再加上$F_r$即$F_r(L_{rs}+1)$。 $\kappa-metric$原文： Kappa-metric ($ \kappa-metric$): The performance value, $\kappa_{ij}$ , assigned to the j-th PF of the i-th optimizer is the number of solutions of the r-th optimizer which cannot weakly dominate a given solution of that PF of the i-th optimizer,where $i,r \in \{1,2\}$; and $i \ne r$. For the same reason as in the case of the $\tau-metric$, $k_{ij}$ may also be rewarded if the j-th PF of the i-th optimizer weakly outperforms a PF of the r-th optimizer. 公式： \kappa_{ij}=\sum_{s=1}^{F_r}\{ \sum_{l=1}^{L_{ij}} \sum_{t=1}^{L_{rs}} I(p_{rst}\nsucceq p_{ijl}) + I(A_{ij} \ \vartheta_w \ A_{rs} ) \}遍历$r_{th}\ optimizer$的所有解，对于每一个解$p_{rst}$，如果$p_{rst} \nsucceq p_{ijl} (l \in [1,…,L_{ij}])$，则加1。 如果与第$i$个优化器，第$j$个$PF$ 满足 ： $A_{ij} \ \vartheta_w \ A_{rs} $，再加1。 因此，最大值为 $F_r(L_{ij}L_{rs}+1)$。 至此三种indicator已介绍完毕。 再分析当初说的五个特点，探究是否满足： Monotonicity/compatibility(单调性/兼容性)：对于两个PFs的支配关系，度量标准应该满足单调性/兼容性。如果A支配B，通过度量标准得出的结果，A就应该比B好或至少不能差于B，这个概念可应用与两个PF之间，但并不能应用于M-ary度量标准，M-ary它是和很多个PFs进行比较的而不是仅仅和另一个PF比较。如果$A$与$\{ B_1,B_2,…B_m\}$进行比较，这是不可能的说A的分数和$B_i’s$的总分数有什么样的关系，尤其在$A$支配一些$B_i’s$ 或/和 $A$被一些$B_i’s$支配 或/和 $A$和一些/全部$B_i’s$交叉。在一些特殊的情况，比如当$A$支配所有的$B_i’s$时，$A$相对于与其他的所有$B_i’s$比较时，一定比任何$B_i$分数高。另一方面，当仅仅比较两个PFs时来作为简化的例子，M-ary度量标准遵守单调/兼容性，只要一个PF支配另一个PF而不是部分PF。 Transitivity(传递性)：就像刚刚谈及Monotonicity时解释的一样，当前的概念并不适用于M-ary度量指标。在对某些PFs进行成对比较简化的情况下，在提出的基于基数的M-ary度量中，并不能保证传递性。例如$\sigma(A,B) &gt; \sigma(B,A) \ and \ \sigma(B,C) &gt; \sigma(C,B)$并不能得出$\sigma(A,C) &gt; \sigma(C,A)$。正如Knowlesand Corne所观察到的，直接的比较指标往往会在被比较的不同PFs之间产生这种不可传递关系。这种情况在Noilublao and Bureerat被称为“剪刀-纸-石头”的情况。 Scaling/meaningfulness(缩放性/有意义性)：所提出的度量标准是基于解决方案之间不同形式的优势关系设计的。由于两个解之间的优势关系是基于它们在目标空间中的相对位置，所以这些关系不会因为它们的双射值的缩放而改变(例如在给定范围内的单调变换)。因此，所提出的度量是缩放不变的。 Computational effort(计算工作量)：因为一个优化器的PF与其它优化器相比,提出的每一个最糟糕的复杂性度量是$O(dFL^2)$,d是目标的数量,F是PFs的数量与一个给定的PF相比,和L的最大尺寸是比较PFs。 Additional information(附加信息)：除了比较优化器的PFs之外，所提出的度量中不需要其他信息。 实例讨论这些测试首先在一组基准实例上进行，这些基准实例包含不同共拓扑的PFs，并且知道PFs之间的确切关系。最后。这些指标应用于另一组实例，并与三个已知指标的结果进行比较。在这个集合中，每个优化器都涉及从多次运行中获得的多个PF，并且不知道PFs之间的确切关系。 izarraga等人提出了8个测试用例来评估指标的性能。测试用例是这样构造的:考虑的PFs之间的确切关系是已知的。每个测试用例包含五个PFs (A, B, C, D和E),除了第六测试此用例只包含两个PFs (A和B)。三维版本中也是如此创建的模式和关系,每个测试用例的PFs是类似的。 假设每一个优化器只有一个PF，并且也已知与其他优化器的PFs的关系如何。 a：此测试样例是关于PFs收敛性分析，$AO_cB; BO_cC;CO_cD;DO_cE​$，除此之外，所有的PFs都有相同数量的解集，多样性，延展性。 b：此测试样例是关于收敛性与多样性分析，$AO_cB,C; B,CO_cD,E$。$B$与$C$，$D$与$E$之间没有任何关系。所有的PFs有相同数量的解集，相同的多样性，但不同的延展性。 c：此测试样例中，所有的PFs有相同数量的解集，相同的收敛性，但是每一个PF都有一个洞，每个洞的大小不一。 d：此测试样例仅关于多样性。所有PFs有相同的收敛性和延展性但多样性不同。A是一致性分布，剩余的PFs都添加了一致性噪音(uniform noise)，但并没有影响其收敛性与延展性。 e：此测试样例用来独立评估收敛性和多样性的用例。A有三个均匀分布的解。B是通过给A添加一个新的非支配解来构造的，C是通过给B添加一个新的非支配解来构造的，以此类推，从而得到$EO_wDO_wCO_wBO_wA$。PFs也是这样构造的，E相对于D有一个更好的多样性，D相对于C有一个更好的多样性，依此类推。 f：此测试样例用于检测是否受到PF的凸性影响，所考虑的PFs具有相同的收敛性、多样性、扩散性和solu离子个数，但它们具有不同的凸性。 g：此测试样例是检查一个度量是否受到PF位置的影响，所有设计的PFs都具有相同的收敛性、多样性、扩张性和解的个数，但是它们都位于POF的不同位置。 h：最后一个测试样例被设计来研究具有多个解决方案的度量的行为。所考虑的五种PFs具有相同的收敛性、相同的扩散性和均匀的多样性，但解的个数不同。 最终实验结果如下： 请注意，测试用例5、7和8的PFs(图2(e)、g)和(h)由于以下原因不能正确区分。在测试用例5中，通过向A添加一个新的非支配解来构造B，通过向B添加一个新的非支配解来构造C，以此类推。因此，A完全被B重叠，B完全被C重叠，以此类推。因此使PFs不可区分。同样的，由于测试用例8的PFs具有相同的收敛性、相同的发散性和一致的多样性，所以它们之间是重叠的。另一方面，虽然测试用例7的每个PF在一个唯一的位置上都有一个曲线的模式，但是由于PF中有大量的高密度的解，所以PFs的指示符号并不能被清晰地识别出来。 论文中也介绍了一个optimizer with multiple PFs的情况。 但是实在不想翻译了。。。。。累死人 Purity原文： 其中的rank one可难倒我了，以为要看前文才能理解，结果看完了还是不懂，直到我查阅材料时发现以下这段话： an iterative ranking procedure: First all non-dominated individuals are assigned rank one and temporarily removed from the population. Then, the next nondominated individuals are assigned rank two and so forth. Finally, the rank of an individual determines its fitness value. 我才恍然大悟，原文里说的是$r_i$be the number of rank one solutions obtained from each MOO strategy.注意是solutions而不是nondominated solutions ，所以就会分等级制度，rank one、rank two。。。具体分法那段话就是步骤。 规定： 有N个MOO策略，$\{R^1_1,…R_1^N\} \ N &gt; 2$ ，下标是rank，上标是第几个策略。 $r_i$ 是第i个策略 $R_i$的等级1(rank one)的个数。 $R^* = \bigcup^N_{i=1}\{R^i_1\}$ 是所有集合的等级1集合的并集。 $r_i^*=|\{\gamma| \gamma \in R_1^i and \ \gamma \in R_1^*\}|$ 是在$R^i_1$与$R^*$的交集。 表达式为： P_i = \frac{r_i^*}{r_i}, i = 1,2,...,N该值是在[0,1]区间，并且越接近1越有良好的性能，纯度越高。 现在想想纯度的的命名还是很形象的。 Wave metric这个度量标准允许我们从这个解集中提取的以帕累托边界的个数来计算解集的深度。我们应该说“pareto layers”，但是下面的算法解释了为什么“pareto frontier”更适合这种情况。Wave metric只能应用于有限尺寸的解集。要计算wave，通常是这样进行的： set i=1 compute the Pareto frontier using solutions set points. Remove Pareto frontier points from the solutions set if the result set is empty, Wave = i else i = i + 1 and go to step 2 一个好的方法必须产生低wave的结果集。当wave = 1时，解集中的所有解都属于帕累托边界。在下图中，我们可以看到波度规在一个简单集合上的结果。对于这个集合，wave = 4： 实际上，我们可以从这个集合中提取4个帕累托边界。 wave metric有两个严重的缺点:它不能在两个解集之间进行区分，而且不可能在两个不同的解集上比较波度量计算的相同结果。 例如,如果我们计算解决方案上图的wave metric，,我们有wave = 4因为我们可以从这个集合中提取四个帕累托前沿。如果我们再加10分的Pareto frontier和计算wave metric,度量的值还是一样的。所以这个度规不能在这两个集合之间进行区分。第二，如果我们没有任何关于解集的先验信息，我们就不能说4是好是坏。 Dominance-based quality（有时间再说）Dominance ranking(太长了再说)Pareto dominance indicator(有时间再说)总结然而，所有dominance-based QIs都有一些弱点。它们提供的信息很少，不知道一组在多大程度上优于另一组。更重要的是，如果集的所有解互不支配，它们可能会使解集变得不可比较，这在多目标优化中经常发生。此外，值得注意的是，一些dominance-based QIs可能部分表示着解集的基数(cardinality)，因为一组尺寸大一点的解可能会导致更多的非支配解。]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicator</tag>
        <tag>ConvergenceQI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[毕设]]></title>
    <url>%2F2019%2F01%2F12%2Findicator%2F</url>
    <content type="text"><![CDATA[放假回家了也要准备准备我的毕业设计，题目是《基于自适应的indicators的多目标优化算法》，如题古老的多目标优化的题目，首先当然是要先了解了解indicators，老师就把他最近写好的关于indicators的综述发给了我，真可谓综述啊！足足100个indicators，路漫漫。。。。。 概念介绍以下是解与解、集合与集合之间的关系： 把解与解的总结到表格里： 一般来说，解集的质量可以解释为它如何很好地表示帕累托前沿，可以分为四个方面:收敛性(convergence)、扩散性(spread)、一致性(uniformity)和基数性(cardinality)。 解集的收敛性(convergence)是指解集与帕累托前缘的距离。解集的扩展考虑集覆盖的区域。它涉及到集合的外部和内部部分。这不同于只考虑集合的边界的质量的广泛性(extensity)。注意，在存在问题帕累托前沿的情况下，解集的扩展也称为集的覆盖(coverage)。集的均匀性(uniformity)是指解分布在集中的均匀程度;解决方案之间的等距间距是所希望的。传播和均匀性是密切相关的,他们共同被称为一组的多样性(diversity)。解集的基数(cardinality)是指解决方案集的数量。总的来说,我们的期望足够的解决方案明确地描述集,但不是太多,可能会损害DM与选择。然而，如果使用相同数量的计算资源生成两个集，则认为具有更多解决方案的集是首选的。 比较解决方案集的质量的一种直接方法是将这些集可视化，并直观地判断一个集相对于另一个集的优越性。这种目视比较是最常用的方法之一，非常适用于双目标拖把或三目标拖把。当目标个数大于3时，解决方案集的直接观察不可用时(散点图),人们可能会求助于从数据分析领域的工具。然而，这些可视化方法可能无法清晰地反映解决方案集质量的所有方面;例如，常用的平行坐标只能部分反映收敛性、扩散性和均匀性。此外，可视化比较不能量化解决方案集之间的差异，因此不能用于指导最优化。 质量指标(Quality indicators, QIs)通过将解决方案集映射为实数来克服可视化比较的问题，从而提供解决方案集之间的数量差异。QIs能够提供解决方案集质量的精确表述，例如，在这些表述中，一个集的质量优于另一个集，以及一个集在某些方面比另一个集好多少。原则上，将一组向量映射成标量值的任何函数都可以看作是一个潜在的质量指示器，但通常它可能需要反映集合质量的一个或多个方面:收敛性、扩展性、一致性和基数性。注意，当比较由精确方法生成的解集时，由于生成的解集是问题的帕累托前沿的子集，所以不考虑解集的收敛性评价。 本节根据Qls主要捕获的质量方面来审查Qls。一般来说，QIs可分为六类:1)QIs用于收敛，2)QIs用于扩展，3)QIs用于均匀性，1)QIs用于基数性，5)QIs用于扩展和均匀性，6)Qls用于四个质量方面的组合质量。在每个类别中，我们还详细介绍了一个或几个示例指示器。这些QIs通常在文献中使用，并且/或在它们的类别中具有代表性。表2总结了所有100篇文献。请注意，它不包括由多个QIs组合而成的度量。 当当当！！！这是论文中总结的100个indicators！！WTF！！！老师让了解了解的时候我是崩溃的。我慢慢来… 加黑加粗的是我已经整理好的~ 安排： QIs for Convergence Dominance-based QIs Dominance-based QIs QIs for Spread]]></content>
      <categories>
        <category>indicators</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>indicators</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab]]></title>
    <url>%2F2019%2F01%2F02%2Fmatlab%2F</url>
    <content type="text"><![CDATA[此文会持续更新，记录一些在matlab中的一些常用函数。 repmat123456&gt;&gt; a = [1 2 3];&gt;&gt; repmat(a,2,3) %把矩阵整体堆叠成新矩阵ans = 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 1 2 3 sort12345678910&gt;&gt; a = [6 3 2 1 4 5];&gt;&gt; [~,ans] = sort(a) % 默认从小到大的索引值ans = 4 3 2 5 6 1&gt;&gt; a(ans)ans = 1 2 3 4 5 6 尺寸扩展12345678&gt;&gt; a = ones(3);&gt;&gt; a(1,(4:5)) = 10a = 1 1 1 10 10 1 1 1 0 0 1 1 1 0 0]]></content>
      <categories>
        <category>matlab</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>matlab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(三)]]></title>
    <url>%2F2019%2F01%2F02%2Fmoead3%2F</url>
    <content type="text"><![CDATA[“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”第三部分，论文中一些具体的细节。 测试函数以下为具体函数，和所给定的前端解 ZDT1 f_1(1)=x_1 \\f_2=g(x)[1-\sqrt{\frac{f_1(x)}{g(x)}}] \\where \quad g(x)=1 + \frac{9(\sum_{i=2}^{n}{x_i})}{n-1} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=30 ZDT2 f_1(x) = x_1 \\f_2=g(x)[1-(\frac{f_1(x)}{g(x)})^2] \\where \quad g(x)=1 + \frac{9(\sum_{i=2}^{n}{x_i)}}{n-1} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=30 ZDT3 f_1(x) = x_1 \\f_2=g(x)[1-\sqrt{\frac{f_1(x)}{g(x)}}-\frac{f_1(x)}{g(x)}sin(10\pi x_1)] \\where \quad g(x)=1 + \frac{9(\sum_{i=2}^{n}{x_i)}}{n-1} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=30 ZDT4 f_1(x) = x_1 \\f_2=g(x)[1-\sqrt{\frac{f_1(x)}{g(x)}}] \\where\quad g(x)=1 + 10(n-1)+\sum_{i=2}^{n}[x_i^2-10cos(4\pi x_i)] \\x=(x_1,...x_n) ，x_1\in [0,1] \times [-5,5]^{n-1},n=10 ZDT6 f_1(x)=1-exp(-4x_1)sin^6(6\pi x_1) \\f_2=g(x)[1-(\frac{f_1(x)}{g(x)})^2] \\g(x)=1 + 9[\frac{\sum_{i=2}^{n}{x_i}}{n-1}]^{0.25} \\x=(x_1,...x_n) ，x_1\in [0,1]^n,n=10 DTLZ1 f_1(x)=(1+g(x))x_1x_2 \\f_2(x)=(1+g(x))x_1(1-x_2) \\f_3(x)=(1+g(x))(1-x_1) \\where\quad g(x)=100(n-2)+100\sum_{i=3}^{n}{\{(x_i-0.5)^2-cos[20\pi (x_i-0.5)]\}} \\x=(x_1,...,x_n)^T \in [0,1]^n,n=10The function value of a Pareto optimal solution satisfies$\sum_{i=1}^{3}{f_i}=1,f_i \geq0$ DTLZ2 f_1(x)=(1+g(x))cos(\frac{x_1\pi}{2})cos(\frac{x_2\pi}{2}) \\f_2(x)=(1+g(x))cos(\frac{x_1\pi}{2})sin(\frac{x_2\pi}{2}) \\f_3(x)=(1+g(x))sin(\frac{x_1\pi}{2}) \\where\quad g(x)=\sum_{i=3}^{n}{x_i^2}, \\x=(x_1,...x_n)^T\in [0,1]^2\times [-1,1]^{n-2},n=10The function value of a Pareto optimal solution satisfies$\sum_{i=1}^{3}{f_i}^2=1,f_i \geq0$ 基本参数设置12345678910N=300;%种群大小T=20;%邻居规模大小max_gen=250;%进化代数pc=1;%交叉概率pm=1/x_num;%变异概率fun='DTLZ2';%有 ZDT1 ZDT2 ZDT3 ZDT4 ZDT6 DTLZ1 DTLZ2yita1=2;%模拟二进制交叉参数2yita2=5;%多项式变异参数5x_num = ;%根据以上每一个函数的定义f_num = ;%根据以上每一个函数的定义 权值向量初始化1234567891011121314151617181920212223242526272829303132function lamda = genrate_lamda( N,f_num )%产生初始化向量lamdalamda2=zeros(N+1,f_num);%初始化if f_num==2 array=(0:N)/N;%均匀分布的值 for i=1:N+1 lamda2(i,1)=array(i); lamda2(i,2)=1-array(i); end len = size(lamda2,1); index = randperm(len); index = sort(index(1:N)); lamda = lamda2(index,:);elseif f_num==3 k = 1; array = (0:25)/25;%产生均匀分布的值 for i=1:26 for j = 1:26 if i+j&lt;28 lamda3(k,1) = array(i); lamda3(k,2) = array(j); lamda3(k,3) = array(28-i-j); k=k+1; end end end len = size(lamda3,1); index = randperm(len); index = sort(index(1:N)); lamda = lamda3(index,:);endend 建立权值向量的邻域1B=look_neighbor(lamda,T); 其中look_neighbor.m为： 12345678910111213141516function B = look_neighbor( lamda,T )%计算任意两个权重向量间的欧式距离N =size(lamda,1);B=zeros(N,T);distance=zeros(N,N);for i=1:N for j=1:N l=lamda(i,:)-lamda(j,:); distance(i,j)=sqrt(l*l'); endend%查找每个权向量最近的T个权重向量的索引for i=1:N [~,index]=sort(distance(i,:)); B(i,:)=index(1:T);end 种群初始化1234567function X = initialize( N,f_num,x_num,x_min,x_max,fun )% 种群初始化X = repmat(x_min,N,1)+rand(N,x_num).*repmat(x_max-x_min,N,1); for i=1:N X(i,(x_num+1:(x_num+f_num))) = object_fun(X(i,:),f_num,x_num,fun); X(i,(x_num+f_num+1)) = 0;end 其中object_fun.m: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485function f = object_fun( x,f_num,x_num,fun )% 测试函数的设置%--------------------ZDT1--------------------if strcmp(fun,'ZDT1') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*(sum/(x_num-1)); f(2)=g*(1-(f(1)/g)^0.5);end%--------------------ZDT2--------------------if strcmp(fun,'ZDT2') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*(sum/(x_num-1)); f(2)=g*(1-(f(1)/g)^2);end%--------------------ZDT3--------------------if strcmp(fun,'ZDT3') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*(sum/(x_num-1)); f(2)=g*(1-(f(1)/g)^0.5-(f(1)/g)*sin(10*pi*f(1)));end%--------------------ZDT4--------------------if strcmp(fun,'ZDT4') f=[]; f(1)=x(1); sum=0; for i=2:x_num sum = sum+(x(i)^2-10*cos(4*pi*x(i))); end g=1+9*10+sum; f(2)=g*(1-(f(1)/g)^0.5);end%--------------------ZDT6--------------------if strcmp(fun,'ZDT6') f=[]; f(1)=1-(exp(-4*x(1)))*((sin(6*pi*x(1)))^6); sum=0; for i=2:x_num sum = sum+x(i); end g=1+9*((sum/(x_num-1))^0.25); f(2)=g*(1-(f(1)/g)^2);end%--------------------------------------------%--------------------DTLZ1-------------------if strcmp(fun,'DTLZ1') f=[]; sum=0; for i=3:x_num sum = sum+((x(i)-0.5)^2-cos(20*pi*(x(i)-0.5))); end g=100*(x_num-2)+100*sum; f(1)=(1+g)*x(1)*x(2); f(2)=(1+g)*x(1)*(1-x(2)); f(3)=(1+g)*(1-x(1));end%--------------------------------------------%--------------------DTLZ2-------------------if strcmp(fun,'DTLZ2') f=[]; sum=0; for i=3:x_num sum = sum+(x(i))^2; end g=sum; f(1)=(1+g)*cos(x(1)*pi*0.5)*cos(x(2)*pi*0.5); f(2)=(1+g)*cos(x(1)*pi*0.5)*sin(x(2)*pi*0.5); f(3)=(1+g)*sin(x(1)*pi*0.5);end%--------------------------------------------end 交叉变异操作模拟二进制交叉(SBX)for j = 1.....x_num x'_{1j}(t)=0.5\times[(1+\lambda_j)x_{1j}+(1-\lambda_j)x_{2j}(t)] \\x'_{2j}(t)=0.5\times[(1-\lambda_j)x_{1j}+(1+\lambda_j)x_{2j}(t)]其中： \lambda_j=\begin{cases} (2u_i)^{\frac{1}{\eta+1}}, & u_j < 0.5\\ \frac{1}{2(1-u_i)}^{\frac{1}{\eta+1}}, & other \end{cases}随机$u_j$，使$0 \leq u_j \leq 1$. endfor 多项式变异for j = 1.....x_num x_{1j}(t)=x_{1j}(t) + \Delta_j其中： \Delta_j=\begin{cases} (2u_i)^{\frac{1}{\eta+1}}-1, & u_j < 0.5\\ 1-(2(1-u_i))^{\frac{1}{\eta+1}}, & other \end{cases}随机$u_j$，使$0 \leq u_j \leq 1$. endfor 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273function chromo_offspring = cross_mutation( chromo_parent_1,chromo_parent_2,f_num,x_num,x_min,x_max,pc,pm,yita1,yita2,fun )%模拟二进制交叉与多项式变异%%%模拟二进制交叉if(rand(1)&lt;pc) %初始化子代种群 off_1=zeros(1,x_num+f_num); %进行模拟二进制交叉 u1=zeros(1,x_num); gama=zeros(1,x_num); for ind=1:x_num u1(ind)=rand(1); if u1(ind)&lt;=0.5 gama(ind)=(2*u1(ind))^(1/(yita1+1)); else gama(ind)=(1/(2*(1-u1(ind))))^(1/(yita1+1)); end off_1(ind)=0.5*((1-gama(ind))*chromo_parent_1(ind)+(1+gama(ind))*chromo_parent_2(ind)); %使子代在定义域内 if(off_1(ind)&gt;x_max(ind)) off_1(ind)=x_max(ind); elseif(off_1(ind)&lt;x_min(ind)) off_1(ind)=x_min(ind); end end %计算子代个体的目标函数值 off_1(1,(x_num+1):(x_num+f_num))=object_fun(off_1,f_num,x_num,fun);end% %%%多项式变异 注释这种方法为上方公式代码，但在ZDT4，DTLZ1中效果不好，% if(rand(1)&lt;pm) 因此换成下方代码，效果甚好！% u2=zeros(1,x_num);% delta=zeros(1,x_num);% for j=1:x_num% u2(j)=rand(1);% if(u2(j)&lt;0.5)% delta(j)=(2*u2(j))^(1/(yita2+1))-1;% else% delta(j)=1-(2*(1-u2(j)))^(1/(yita2+1));% end% off_1(j)=off_1(j)+delta(j);% %使子代在定义域内% if(off_1(j)&gt;x_max(j))% off_1(j)=x_max(j);% elseif(off_1(j)&lt;x_min(j))% off_1(j)=x_min(j);% end% end% %计算子代个体的目标函数值% off_1(1,(x_num+1):(x_num+f_num))=object_fun(off_1,f_num,x_num,fun);% end% chromo_offspring=off_1;% end%%%多项式变异 具体改变：一次变异只改变一个位置，并不是像之前那样都要变异if(rand &lt; pm) r=randperm(x_num); ind=r(1); u2=rand; if(u2 &lt; 0.5) delta=(2*u2)^(1/(yita2+1))-1; else delta=1-(2*(1-u2))^(1/(yita2+1)); end off_1(ind)=off_1(ind)+delta*(x_max(ind)-x_min(ind)); %使子代在定义域内 if(off_1(ind)&gt;x_max(ind)) off_1(ind)=x_max(ind); elseif(off_1(ind)&lt;x_min(ind)) off_1(ind)=x_min(ind); end %计算子代个体的目标函数值 off_1(1,(x_num+1):(x_num+f_num))=object_fun(off_1,f_num,x_num,fun);endchromo_offspring=off_1;end 更新领域解1X=updateNeighbor(lamda,z,X,B(i,:),off,x_num,f_num); 其中updateNeighbor.m： 1234567891011function X = updateNeighbor( lamda,z,X,Bi,off,x_num,f_num )%更新领域解for i=1:length(Bi) gte_xi=tchebycheff_approach(lamda,z,X(Bi(i),(x_num+1):(x_num+f_num)),Bi(i)); gte_off=tchebycheff_approach(lamda,z,off(:,(x_num+1):(x_num+f_num)),Bi(i));% gte_xi=ws_approach(lamda,X(Bi(i),(x_num+1):(x_num+f_num)),Bi(i));% gte_off=ws_approach(lamda,off(:,(x_num+1):(x_num+f_num)),Bi(i)); if gte_off &lt;= gte_xi X(Bi(i),:)=off; endend 其中tchebycheff_approach.m： 123456789function fs = tchebycheff_approach( lamda,z,f,i)%tchebycheff_approachfor j=1:length(lamda(i,:)) if(lamda(i,j)==0) lamda(i,j)=0.00001; endendfs=max(lamda(i,:).*abs(f-z));end 评价指标C-metric令 A和 B是一个 MOP中两个接近PF的集合，定义 C(A,B)如： C(A,B)=\frac{\{u\in B|\exists v\in A:v\quad dominates\quad u\}}{|B|}C(A,B)不等于 1-C(B,A)。C(A,B)=1意味着 B中所有的解都被 A中的某些解支配了， C(A,B)=0意味着 B中没有解被 A中的解支配。 1234567891011121314151617181920212223242526function C_AB = cal_c(A,B,f_num)[temp_A,~]=size(A);[temp_B,~]=size(B);number=0;for i=1:temp_B nn=0; for j=1:temp_A less=0;%当前个体的目标函数值小于多少个体的数目 equal=0;%当前个体的目标函数值等于多少个体的数目 for k=1:f_num if(B(i,k)&lt;A(j,k)) less=less+1; elseif(B(i,k)==A(j,k)) equal=equal+1; end end if(less==0 &amp;&amp; equal~=f_num) nn=nn+1;%被支配个体数目n+1 end end if(nn~=0) number=number+1; endendC_AB=number/temp_B;end D-metric令 $P^*$为一组均匀分布在 PF上的点集合。 A是一个接近 PF的集合。 的集合。 $P^*$到 A的平均距离定义为： D(A,P)=\frac{\sum_{v\in P^*}d(v,A)}{|P^*|}这里 $𝑑(𝑣,𝐴)$是v和A中的点最小欧式距离。如果 $P^*$足够大,说明其可以很好的代表PF。$D(A,P^*)$可以从某种意义上评估A的收敛性和多样。为了让$D(A,P^*)$的值很低，必须设置 A非常接近PF，并且不能缺失整个PF的任何部分。 12345678910function D_AP = cal_d(A,P)[temp_A,~]=size(A);[temp_P,~]=size(P);min_d=0;for v=1:temp_P d_va=(A-repmat(P(v,:),temp_A,1)).^2; min_d=min_d+min(sqrt(sum(d_va,2)));endD_AP=(min_d/temp_P);end ‘]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(二)]]></title>
    <url>%2F2019%2F01%2F01%2Fmoead2%2F</url>
    <content type="text"><![CDATA[“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”第二部分，算法的流程框架。 规定本文提出的基于分解的多目标进化算法(MOEA/D)需要对MOPs进行分解。任何分解方法都可以达到这个目的。在下面的描述中，我们假设使用了Tchebycheff方法。在使用其他分解方法时，修改下面的MOEA/D也非常简单。 $\lambda^1​$,…$\lambda^N​$ 是均匀分布的权值向量 $z^*$ 是reference point 选用Tchebycheff Approach把多目标问题拆成N个标量优化子问题，表达式如下: g^{te}(x|\lambda^j,z^*)=\max\limits_{1\leq i \leq m}\{\lambda_i^j|f_i(x)-z_i^*|\} 其中 $\lambda ^j=(\lambda_1^j,…\lambda_m^j)^T$. $\lambda=(\lambda^1,…,\lambda^N)$ 可知$g^{te}$是关于$\lambda$连续的，当$\lambda^i$与$\lambda^j$彼此接近，那么接近$\lambda ^i$向量的$g^{te}$权向量的信息也对最优解$g^{te}(x|\lambda^j,z^*)$有一定的作用。这也是MOEA/D的理论基础。 在MOEA/D中，权向量的邻域被定义为它的几个最近的权向量的集合。第$i$个子问题的邻域由所有的子问题组成，这些子问题的权向量来自于第$i$个子问题的邻域。在MOEA/D中，只有相邻子问题的当前解被用来优化子问题。 切比雪夫法的MOEA/D算法中，有以下规定： $x^1,…x^N \in \Omega$ $x^i$是当前的第i个子问题 $FV^1,…,FV^N$ ，其中 $FV^i = F(x^i)$ $ x \in [1,N]$ $z=(z_1,…z_m)^T $ ，$z_i$ 是目前对目标$f_i$所找到的最好的点。 Input MOP(1) 一个终止准则 N：子问题的个数 N 个均匀分布的权值向量$\lambda_1,…\lambda_N$ T 每一个权值向量的邻居的数量Output: EP STEP 1) Initialization:Step 1.1) 使EP为空集 Step 1.2) 计算任意两个权值向量间的欧式距离，并找到离每个权值距离最近的T个点 ​ $B(i)=\{i_i,…i_T\}$ ，其中，$\lambda^{i_1},…\lambda^{i_T}$就是T个最近的权值向量 Step 1.3) 随机产生初始化种群 $x^1,…,x^N$ ，规定$FV^i=F(x^i).$ Step 1.4) 初始化 $z=(z_1,…z_m)^T $ STEP 2) Update:for i=1,…N Step 2.1) 复制 ：从$B(i)$随机产生两个索引$k,l$ ，然后通过遗传算子从$x_k,x_l$ 中产生新的子代$y$ Step 2.2) 提升 ：通过提升或者修理来启发式的由$y$产生$y’$ Step 2.3) 更新参考点$z$：if $z_j &lt; f_j(y’)$ then $z_j = f_j(y’)$ $j \in 1,…m$ Step 2.4) 更新相邻解：对于每一个$j \in B(i)$,if $g^{te}(y’|\lambda^j,z)\leq g^{te}(x^j|\lambda^j,z)$ then $x^j=y’, FV^j=F(y’)$ Step 2.5) 更新EP：​ — 从 EP中移除被 $F(y’)$支配的所有向量​ — 如果 EP中没有向量支配 $F(y’)​$，就将 F(y’)加入到EP中 STEP 3) Stopping Criteria 如果停止准则满足，并输出EP。否则，转向 STEP 2)。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(一)]]></title>
    <url>%2F2018%2F12%2F31%2Fmoead1%2F</url>
    <content type="text"><![CDATA[最近在复现“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”这篇论文，但多目标优化门都没入，所以作为复现的第一篇MOEA算法，我要趁此好好肢解这篇论文，尽量理解。 在Weighted Sum Approach表达式为： min\quad g^{ws}(x|\lambda)=\sum_{i=0}^{m}\lambda_if_i(x)m：m个优化目标， $\sum_{i=1}^{m}\lambda_i = 1$ $\lambda​$ 被称为权重向量。 通过公式，把算法求出的一个目标点和原点相连构造成一个向量与对应权重向量点乘，由向量点乘的几何意义可知，所得的数为该向量在权重向量方向上的投影长度，因为权重向量不变，最大/小化该长度值其实就是在优化该向量。可知若要增大该向量在权重向量上投影的长度，一方面可以增大/减小与权重向量的夹角，另一方面可以增大/减小该向量的长度。样例图如下： 红色权重向量，因为是最小化问题，所以减小长度，增大夹角都是可行的方案，绿色为等高线，垂直于权重向量。阴影部分为所有解，因此，在每一个绿色的等高线上找角度最大的即为边界。 Tchebycheff Approach表达式为： minimize\quad g^{te}(x|\lambda,z^*)=\max \limits_{1\leq i \leq m}\{\lambda_i|f_i(x)-z^*|\}注意该方法中不再含有$\sum$符号，故不能再从向量点乘的角度理解。该方法大致思想是减少最大差距从而将个体逼近PF。 首先解释等高线为什么是这样的。单看$f_1$函数，即只考虑纵坐标，若两点等值，必然是$\lambda_i|f_i(x)-z^*|$式中$f_1$的函数值相等（因为另外两个量是不变的），即纵坐标相等，所以$f_1$函数的等高线是一组平行于横轴的直线。$f_2$类似，为一组平行于纵轴的直线。第一次相比较的是m个维度中最大的$max ( \lambda _1(y-z_1),\lambda _2(x-z_2))$，所以等高线便是一个点之内各个维度的比较。那么，图中的等高线是横竖相交且刚好交在权重向量的方向上的，证明：可知，对于任何一个可行的解，我们从$f_1$的角度上可以得到一个$f_1$的值y，从$f_2$的角度上可以得到一个$f_2$ 的值x，他们的切比雪夫值是相等的，自然想到：点(x,y)（图中紫色点）为该切比雪夫值得横纵两条等值线的交点，那么有：$\lambda _1(y-z_1)=\lambda_2(x-z_2)$，化简的$(y-z_1)/(x-z_2)=\lambda_2/\lambda_1$，可知该交点位于权重向量的方向上。需要注意一点，这里的权重向量起点是$z^*$，不再是原点。此时可知，若某个个体位于其($\lambda -z^*$)向量方向的上部，则max得到的一定是其$f_1$部分，故优化也需要减小其$f_1$的值，即个体向下移动，相反，若在($\lambda -z^*$)向量方向的下部，则应像左移动。以此来保证个体目标值落在黄点附近。 一种可能的个体运动路线如下图，橘色—&gt;黄色所示： Boundary IntersectionApproach表达式为： minimize\quad g^{bi}(x|\lambda,z^*)=d \\subject\quad to\quad z^*-F(x)=d\lambda \\x \in \Omega参数含义如下如所示： 式子中等式约束其目的是为了保证F(x)位于权重向量λ的方向上，通过减小d来使算法求出的解逼近PF。但该条件不太容易实现，故将其改进为下边这种方法。 Penalty-based Boundary Intersection Approach minimize\quad g^{bip}(x|\lambda,z^*)=d_1 + \theta d_2 \\subject \quad to \quad x \in \Omega \\where \quad \quad d_1 =\frac{||(z_*-F(x))^T\lambda||}{||\lambda||} \\and \quad d_2 = ||F(x)-(z^*-d_1\lambda)||参数含义如下如所示： 可知算法放宽了对算法求出的解得要求，但加入了一个惩罚措施：你可以不把解生成在权重向量的方向上，但如果不在权重向量方向上，你就必须要接收惩罚，你距离权重向量越远，受的惩罚越厉害，以此来约束算法向权重向量的方向生成解。 接下来是关于$d_1$和$d_2$两个参数的计算表达式的含义说明，我依然是从几何角度理解的。 $d_1$——观察$d_1$的计算表达式，$Z^*-F(x)$可以看做原点到$Z^*$点的向量减去原点到$F(x)$的向量，得到的是从$F(x)$出发指向$Z^*$的一个向量，暂且命名为$\mu$，之后$\mu$与$\lambda$相乘得到$\mu$在方向上的投影，这$\lambda$个长度值与λ的长度值之比为$d_1$。$d_2$——其表达式的含义其实也无非就是利用向量运算构造出$d_2$所表示的向量，取模即可得到$d_2$.构造过程如下： $Z^*$表红色向量，$d_1\lambda$表蓝色向量（因为减法，所以方向取反），红色减蓝色得紫色向量，$F(x)$表绿色向量，绿色减紫色得黄色向量，即$d_2$表黄色向量的长度 引自]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MOEA/D算法(0)]]></title>
    <url>%2F2018%2F12%2F30%2Fmoead0%2F</url>
    <content type="text"><![CDATA[最近在复现“MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition”这篇论文这是一个后补上的文章，几乎是翻译的原论文呢】，因为课程设计凑字数，也为了省事，就干脆发在我的小博客上了。 多目标优化问题可以表示如下： maximize \quad F(x)=(f_1(x),...,f_m(x))^T \\subject \ to \ x \in \Omega其中，$\Omega$是决策空间，$F$：$\Omega \rightarrow R^m$是m个实数目标函数，$R^m$叫做目标空间，可实现的目标定义如下： \Omega=\{x \in R^n|h_j(x)\leq 0,j=1,...,m\}$h_j$是连续的函数，因此，我们也称$F(x)$是连续的MOP问题。 在现实生活中，大多数的目标函数却是相互矛盾的，并不存在$\Omega$可以同时放大所有的目标值。因此需要找相应的方法去平衡这些目标。目标之间的最佳权衡可以用帕累托(Pareto)最优性来定义。 定义$u,v\in R^m$,如果对于$\forall i \in \{1,…,m\}$，使得$u_i\geq v_i$，并且$\exists j \in \{1,…,m\} $，使得$u_i &gt; v_i$，则称$u$支配$v$。如果存在这种点$x^\in \Omega$，不存在点$x$，使$F(x)$支配$F(x^)$，那么称$F(x^*）$为帕累托最优目标向量。换言之，一个目标中帕累托最优点的任何改进都必须导致至少另一个目标的恶化。所有帕累托最优点的集合称为帕累托集合(PS)，所有帕累托最优目标向量的集合称为帕累托阵(PF)。 在多目标优化的许多实际应用中，决策者需要近似于PF来选择最终的首选解决方案。大多数MOPs可能有许多甚至无限帕累托最优向量。获取完整的PF是非常耗时的。另一方面，由于信息的溢出，决策者可能对拥有过多的帕累托最优向量不感兴趣。因此，许多多目标优化算法都是为了找到一个可管理的帕累托最优向量。一些研究者也尝试用数学模型来近似PF。 目前没有涉及到分解的大部分多目标进化算法，将MOP视为一个整体。它们不会将每个单独的解决方案与任何特定的标量优化问题关联起来。在标量目标优化问题，所有的解决方案都可以在它们目标函数值的基础上进行比较，标量目标的任务进化算法(EA)往往是寻找一个单一的最优的解决方案。然而，在MOPs中，支配并非定义目标函数中解的完整顺序空间，MOEAs旨在产生一些帕累托最优尽可能多样化的解决方案来代表整体PF。 因此，最初设计用于标量优化的传统选择算子不能直接用于非分解MOEAs。那么可以说，如果有一种适合度分配方案，用于为单个解决方案分配一个相对适合度值，以反映其选择的实用价值，那么标量优化EAs可以很容易地扩展到处理MOPs。因此，适应度分配一直是当前的一个主要问题MOEA研究。目前流行的适应度分配策略包括基于交互目标的适应度分配，如向量评价遗传算法(VEGA);基于优势的适应度分配，如帕累托存档进化策略（PAES）。 分解的思想在一些针对MOPs的元启发式中得到了一定程度的应用。例如，两阶段局部搜索(TPLS)考虑了一组标量优化问题，其中目标是所考虑的MOP中的目标的集合，基于集合系数的序列将标量优化算法应用于这些标量优化问题中，将前一个问题得到的解作为下一个问题求解的起点，因为它的集合目标与前一个问题的集合目标略有不同。多目标遗传局部搜索(MOGLS)旨在同时优化加权和方法或Tchebycheff方法构建的所有聚合。在每次迭代中，它优化随机生成的聚合目标。]]></content>
      <categories>
        <category>MOEA</category>
      </categories>
      <tags>
        <tag>MOEA</tag>
        <tag>MOEA\D</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vscode配置c环境]]></title>
    <url>%2F2018%2F12%2F22%2Fvscode%E9%85%8D%E7%BD%AEc%2F</url>
    <content type="text"><![CDATA[在sublime和vscode的权衡下，选择了vscode，毕竟之前一直用的是sublime，想换一换了。于是就遇到一个老问题，配环境！ 此内容几乎完全来自于某乎 安装 vscode LLVM 选Pre-Built Binaries中的Clang for Windows (64-bit)，不需要下.sig文件 添加环境变量：Add LLVM to the system PATH for all users 安装路径推荐：C:\LLVM 工具链：MinGW 其他默认 MinGW-w64 - for 32 and 64 bit Windows 链接，提取码：dclo 下好后，把x86_64-7.2.0-posix-seh-rt_v5-rev0.7z\mingw64 中所有的文件都复制到 C:\LLVM中 检验：打开cmd 输入gcc，如果为no input files而不是其他，即为成功。 ​ 输入clang，如果为no input files而不是其他，即为成功。 ​ 插件一定要下： C/C++ C/C++ Clang Command Adapter Code Runner 自由推荐： Bracket Pair Colorizer：彩虹花括号 Include Autocomplete：提供头文件名字的补全 One Dark Pro：VS Code安装量最高的主题 环境配置打开vscode，一定要选 open folder 选择刚才那个文件夹，点VS Code上的新建文件夹，名称为.vscode（这样做的原因是Windows的Explorer不允许创建的文件夹第一个字符是点），然后创建 launch.json，tasks.json，settings.json，c_cpp_properties.json放到.vscode文件夹下 launch.json:12345678910111213141516171819202122232425262728// https://github.com/Microsoft/vscode-cpptools/blob/master/launch.md&#123; &quot;version&quot;: &quot;0.2.0&quot;, &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;(gdb) Launch&quot;, // 配置名称，将会在启动配置的下拉菜单中显示 &quot;type&quot;: &quot;cppdbg&quot;, // 配置类型，这里只能为cppdbg &quot;request&quot;: &quot;launch&quot;, // 请求配置类型，可以为launch（启动）或attach（附加） &quot;program&quot;: &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot;, // 将要进行调试的程序的路径 &quot;args&quot;: [], // 程序调试时传递给程序的命令行参数，一般设为空即可 &quot;stopAtEntry&quot;: false, // 设为true时程序将暂停在程序入口处，我一般设置为true &quot;cwd&quot;: &quot;$&#123;workspaceFolder&#125;&quot;, // 调试程序时的工作目录 &quot;environment&quot;: [], // （环境变量？） &quot;externalConsole&quot;: true, // 调试时是否显示控制台窗口，一般设置为true显示控制台 &quot;internalConsoleOptions&quot;: &quot;neverOpen&quot;, // 如果不设为neverOpen，调试时会跳到“调试控制台”选项卡，你应该不需要对gdb手动输命令吧？ &quot;MIMode&quot;: &quot;gdb&quot;, // 指定连接的调试器，可以为gdb或lldb。但目前lldb在windows下没有预编译好的版本。 &quot;miDebuggerPath&quot;: &quot;gdb.exe&quot;, // 调试器路径，Windows下后缀不能省略，Linux下则去掉 &quot;setupCommands&quot;: [ // 用处未知，模板如此 &#123; &quot;description&quot;: &quot;Enable pretty-printing for gdb&quot;, &quot;text&quot;: &quot;-enable-pretty-printing&quot;, &quot;ignoreFailures&quot;: false &#125; ], &quot;preLaunchTask&quot;: &quot;Compile&quot; // 调试会话开始前执行的任务，一般为编译程序。与tasks.json的label相对应 &#125; ]&#125; tasks.json:命令行参数方面，-std根据自己的需要修改。如果使用Clang编写C语言，把command的值改成clang。如果使用MinGW，编译C用gcc，编译c++用g++，并把-target和-fcolor那两条删去。 123456789101112131415161718192021222324252627282930313233// https://code.visualstudio.com/docs/editor/tasks&#123; &quot;version&quot;: &quot;2.0.0&quot;, &quot;tasks&quot;: [ &#123; &quot;label&quot;: &quot;Compile&quot;, // 任务名称，与launch.json的preLaunchTask相对应 &quot;command&quot;: &quot;clang++&quot;, // 要使用的编译器 &quot;args&quot;: [ &quot;$&#123;file&#125;&quot;, &quot;-o&quot;, // 指定输出文件名，不加该参数则默认输出a.exe，Linux下默认a.out &quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;.exe&quot;, &quot;-g&quot;, // 生成和调试有关的信息 &quot;-Wall&quot;, // 开启额外警告 &quot;-static-libgcc&quot;, // 静态链接 &quot;-fcolor-diagnostics&quot;, // 彩色的错误信息？但貌似clang默认开启而gcc不接受此参数 &quot;--target=x86_64-w64-mingw&quot;, // clang的默认target为msvc，不加这一条就会找不到头文件；Linux下去掉这一条 &quot;-std=c++17&quot; // C语言最新标准为c11，或根据自己的需要进行修改 ], // 编译命令参数 &quot;type&quot;: &quot;shell&quot;, // 可以为shell或process，前者相当于先打开shell再输入命令，后者是直接运行命令 &quot;group&quot;: &#123; &quot;kind&quot;: &quot;build&quot;, &quot;isDefault&quot;: true // 设为false可做到一个tasks.json配置多个编译指令，需要自己修改本文件，我这里不多提 &#125;, &quot;presentation&quot;: &#123; &quot;echo&quot;: true, &quot;reveal&quot;: &quot;always&quot;, // 在“终端”中显示编译信息的策略，可以为always，silent，never。具体参见VSC的文档 &quot;focus&quot;: false, // 设为true后可以使执行task时焦点聚集在终端，但对编译c和c++来说，设为true没有意义 &quot;panel&quot;: &quot;shared&quot; // 不同的文件的编译信息共享一个终端面板 &#125; // &quot;problemMatcher&quot;:&quot;$gcc&quot; // 如果你不使用clang，去掉前面的注释符，并在上一条之后加个逗号。照着我的教程做的不需要改（也可以把这行删去) &#125; ]&#125; settings.json: Code Runner的命令行和某些选项可以根据自己的需要在此处修改，用法还是参见此扩展的文档和百度gcc使用教程。如果你要使用其他地方的头文件和库文件，可能要往clang.cflags和clang.cxxflags里加-I和-L，用法百度gcc使用教程。12345678910111213141516171819202122232425262728293031&#123; &quot;files.defaultLanguage&quot;: &quot;cpp&quot;, // ctrl+N新建文件后默认的语言 &quot;editor.formatOnType&quot;: true, // 输入时就进行格式化，默认触发字符较少，分号可以触发 &quot;editor.snippetSuggestions&quot;: &quot;top&quot;, // snippets代码优先显示补全 &quot;code-runner.runInTerminal&quot;: true, // 设置成false会在“输出”中输出，无法输入 &quot;code-runner.executorMap&quot;: &#123; &quot;c&quot;: &quot;cd $dir &amp;&amp; clang $fileName -o $fileNameWithoutExt.exe -Wall -g -Og -static-libgcc -fcolor-diagnostics --target=x86_64-w64-mingw -std=c11 &amp;&amp; $dir$fileNameWithoutExt&quot;, &quot;cpp&quot;: &quot;cd $dir &amp;&amp; clang++ $fileName -o $fileNameWithoutExt.exe -Wall -g -Og -static-libgcc -fcolor-diagnostics --target=x86_64-w64-mingw -std=c++17 &amp;&amp; $dir$fileNameWithoutExt&quot; &#125;, // 设置code runner的命令行 &quot;code-runner.saveFileBeforeRun&quot;: true, // run code前保存 &quot;code-runner.preserveFocus&quot;: true, // 若为false，run code后光标会聚焦到终端上。如果需要频繁输入数据可设为false &quot;code-runner.clearPreviousOutput&quot;: false, // 每次run code前清空属于code runner的终端消息 &quot;C_Cpp.clang_format_sortIncludes&quot;: true, // 格式化时调整include的顺序（按字母排序） &quot;C_Cpp.intelliSenseEngine&quot;: &quot;Default&quot;, // 可以为Default或Tag Parser，后者较老，功能较简单。具体差别参考cpptools扩展文档 &quot;C_Cpp.errorSquiggles&quot;: &quot;Disabled&quot;, // 因为有clang的lint，所以关掉 &quot;C_Cpp.autocomplete&quot;: &quot;Disabled&quot;, // 因为有clang的补全，所以关掉 &quot;clang.cflags&quot;: [ // 控制c语言静态检测的参数 &quot;--target=x86_64-w64-mingw&quot;, &quot;-std=c11&quot;, &quot;-Wall&quot; ], &quot;clang.cxxflags&quot;: [ // 控制c++静态检测时的参数 &quot;--target=x86_64-w64-mingw&quot;, &quot;-std=c++17&quot;, &quot;-Wall&quot; ], &quot;clang.completion.enable&quot;:true // 效果效果比cpptools要好&#125; c_cpp_properties.json: 1234567891011121314151617181920212223&#123; &quot;configurations&quot;: [ &#123; &quot;name&quot;: &quot;MinGW&quot;, &quot;intelliSenseMode&quot;: &quot;clang-x64&quot;, &quot;compilerPath&quot;: &quot;C:/LLVM/bin/gcc.exe&quot;, &quot;includePath&quot;: [ &quot;$&#123;workspaceFolder&#125;&quot; ], &quot;defines&quot;: [], &quot;browse&quot;: &#123; &quot;path&quot;: [ &quot;$&#123;workspaceFolder&#125;&quot; ], &quot;limitSymbolsToIncludedHeaders&quot;: true, &quot;databaseFilename&quot;: &quot;&quot; &#125;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++17&quot; &#125; ], &quot;version&quot;: 4&#125; 编译技巧 ctrl+shift+B单纯编译 按F5为运行并调试（运行前会自动编译） 加断点在列号前面点一下就行，如果想从一开始就停下来，可以加在main函数那里，或者launch.json中设置&quot;stopAtEntry&quot;: true。 按f11可以一步一步进行，箭头所指的那行代码就是下一步要运行的代码。 左边有个调试栏，可以看到变量的值,自动栏没有的可以手动添加表达式 把鼠标放到变量上可以看到变量的值，但是只能识别简单的表达式 栈帧对于递归很有用；在某些时候还可以抓取“异常”。 如果你不需要调试，可以直接右键选run code。 输出端可以输入，在settings.json中添加&quot;code-runner.runInTerminal&quot;: true]]></content>
      <categories>
        <category>vscode</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>vscode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo恢复]]></title>
    <url>%2F2018%2F12%2F22%2Fhexo%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[想重新开始写博客，第一件事当然是恢复博客的正常使用啦！搜了小半天终于找到了符合我条件的教程。 背景：起初已配置好，但之后从未使用，期间重新做了一次系统。待我有时间再查询一下如何备份至云端。(已完成) 恢复安装git、node.js在原来储存博客的文件夹中(blog)`右键`-&gt;`选择`-&gt;`Git Bash Here` 再输入：1npm install hexo -g 因为重装系统有可能删除了配置文件包括环境变量里面的，没有配置 name 和 email 的话，git 是无法正常工作的。所以首先得重新配置name跟email在git bash里面输入下面两行 12git config --global user.name &quot;你的名字&quot;git config --global user.email &quot;你的邮箱&quot; 如果上面两条命令fail了的话，记得先用命令git init再输入上面两条命令 创建SSH输入 ssh-keygen -t rsa -C &quot;myemail@example.com&quot; 再按两次回车输入 cd ~/.ssh 再输入 cat id_rsa.pub会输出 12ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxmyemail@example.com 登陆我的Github在settings中找到ssh and GPG keys点击new ssh key，title随意 把ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx输入到key位置在git bash输入ssh -T git@github.com 可验证时候正确 修改blog目录下_config.yml如果执行hexo deploy提示 123Logon failed, use ctrl+c to cancel basic credential prompt.bash: /dev/tty: No such device or addressINFO Catch you later 则需要把下方的 1234deploy: type: git repo: https://github.com/mygithubName/mygithubName.github.io.git branch: master 修改成： 1234deploy: type: git repo: ssh://git@github.com/mygithubName/mygithubName.github.io.git branch: master 执行 hexo g -d 大功告成 常规操作： 1234hexo cleanhexo generatehexo server(本地测试用)hexo deploy 至此，网站已基本恢复。 云备份至Github为了以后更方便的从云端备份下来，我又查了一些教程，下面便是详细步骤 基本原理网站的部署其实就是生成静态文件，hexo下所有生成的静态文件会放在public/文件夹中，所谓部署deploy其实就是 将public/文件夹中内容上传到git仓库myname.github.io中。也就是说，你的仓库myname.github.io中的文件只是blog（或者命名为hexo）文件夹下的public/下的文件。本背景下，方便放在myname.github.io的repository下创建一个分支来管理 建立分支hexo 在本地磁盘下（位置任意）右键 -&gt; Git bash here，执行以下指令将myname.github.io项目文件克隆到本地： 1git clone git@github.com:myname/myname.github.io.git 此目录下便有myname.github.io文件夹，把此文件夹中除了.git之外的所有文件删掉 把blog中所有文件复制到myname.github.io 文件夹中，其中会提示是否替换，选择跳过。 如果有.gitignore文件，把里面的内容修改成 1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/ 如果没有此文件，便在git bash中输入touch .gitignore 在myname.github.io 文件夹中右键 -&gt; Git bash here 创建一个叫hexo的分支并切换到这个分支上 git checkout -b hexo 提交复制过来的文件到暂存区git add --all 提交git commit -m &quot;&quot; 推送分支到githubgit push --set-upstream origin hexo在github上可以看到 branch中有master和hexo，至此，已经成功。并且hexo中的文件便在.gitirnore所忽略而剩下需要备份的文件， 更新文章，修改主题等步骤 在github中myname.github.io中，找到settings -&gt; Branches 将hexo设为默认 从此更新文章，修改主题等操作一直都在myname.github.io了 执行如下 123456hexo cleanhexo generatehexo deploygit add .git commit -m &quot;&quot;git push origin hexo 注意 -m “要写一点东西” 从github上还原此部分完全摘抄自网站，我并非试过，并不知道是否可行。 克隆项目 1git clone -b hexo git@github.com:myname/myname.github.io.git 进入博客目录 1cd myname.github.io.git 切换到博客文件分支 1git checkout -b hexo origin/hexo 安装hexo 1nmp install hexo --save 编辑，查看 12hexo ghexo s 提交git若提交过程中出现ERROR Deployer not found: git,可执行以下代码，然后重新提交 1npm install hexo-deployer-git --save 新的文章等更新 123git add .git commit -m &quot;新增博客&quot;git push origin hexo END]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[小小念]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%B0%8F%E5%B0%8F%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[​ 啊啊啊啊啊啊，当时心一热搭建了一个博客，时隔四个月一直却没有更新博客善哉善哉，但期间也经历了好多，从准备保研的焦头烂额，到现在天天看剧打游戏的糜烂生活，落差之大，以至于日日积累的罪恶感促使我又有好好学习之意，遂重新在网上找了小半天的教程，把静静躺在H盘的blog文件夹重新唤醒。​ 当时心心念的保研，经历了很多很多次的失败，多方权衡下，最后以去南方科技大学而告终，毕业设计的题目也基本确定，很经典的问题——多目标优化，这也可能是我研究生研究的方向了。 ​ 从今天开始，可能就要持续更新我的小博客，记录一下~~]]></content>
      <categories>
        <category>随笔</category>
      </categories>
      <tags>
        <tag>感慨</tag>
        <tag>随想</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2018%2F08%2F23%2Fmarkdown%2F</url>
    <content type="text"><![CDATA[Typora For Markdown 语法，才刚刚学习，用着可能不熟练，先自行收藏一下~ 数学表达式要启用这个功能，首先到Preference-&gt;Editor中启用。然后使用`符号包裹Tex命令，例如：`$lim_{x \to \infty} \ exp(-x)=0将产生如下的数学表达式： $\lim_{x \to \infty} \exp(-x)=0$ 下标下标使用~包裹，例如：H~2~O将产生H~2~O, 即水的分子式。 上标上标使用^包裹，例如：y^2^=4将产生表达式y^2^ = 4 插入表情:happy:使用:happy:输入表情:happy:,使用:sad:输入表情:sad:,使用:cry:输入表情:cry:等。以此类推！ 下划线用HTML的语法&lt;u&gt;Underline&lt;/u&gt;将产生下划线Underline. 删除线GFM添加了删除文本的语法，这是标准的Markdown语法木有的。使用~~包裹的文本将会具有删除的样式，例如~删除文本~将产生删除文本的样式。 代码 使用`包裹的内容将会以代码样式显示，例如 1使用`printf()` 则会产生printf()样式。 输入`12* ​1234public Class HelloWorld&#123; System.out.println("Hello World!");&#125;​ 1234567将会产生~~~javapublic Class HelloWorld&#123; System.out.println(&quot;Hello World!&quot;);&#125; 强调使用两个*号或者两个_包裹的内容将会被强调。例如 12**使用两个*号强调内容**__使用两个下划线强调内容__ 将会输出 使用两个*号强调内容使用两个下划线强调内容Typroa 推荐使用两个*号。 斜体在标准的Markdown语法中，*和_包裹的内容会是斜体显示，但是GFM下划线一般用来分隔人名和代码变量名，因此我们推荐是用星号来包裹斜体内容。如果要显示星号，则使用转义： 1\* 插入图片我们可以通过拖拉的方式，将本地文件夹中的图片或者网络上的图片插入。 ​ ​ 插入URL连接使用尖括号包裹的url将产生一个连接，例如：&lt;www.baidu.com&gt;将产生连接:. 如果是标准的url，则会自动产生连接，例如:www.google.com 目录列表Table of Contents（TOC）输入[toc]然后回车，将会产生一个目录，这个目录抽取了文章的所有标题，自动更新内容。 水平分割线使用***或者---，然后回车，来产生水平分割线。 标注我们可以对某一个词语进行标注。例如 12某些人用过了才知道[^注释][^注释]:Somebody that I used to know. 将产生： 某些人用过了才知道注释注释: Somebody that I used to know. 把鼠标放在注释上，将会有提示内容。 表格12345|姓名|性别|毕业学校|工资||:---|:---:|:---:|---:||杨洋|男|重庆交通大学|3200||峰哥|男|贵州大学|5000||坑货|女|北京大学|2000| 将产生: 姓名 性别 毕业学校 工资 杨洋 男 重庆交通大学 3200 峰哥 男 贵州大学 5000 坑货 女 北京大学 2000 其中代码的第二行指定对齐的方式，第一个是左对齐，第二个和第三个是居中，最后一个是右对齐。 数学表达式块输入两个美元符号，然后回车，就可以输入数学表达式块了。例如： 1$$\mathbf&#123;V&#125;_1 \times \mathbf&#123;V&#125;_2 = \begin&#123;vmatrix&#125; \mathbf&#123;i&#125; &amp; \mathbf&#123;j&#125; &amp; \mathbf&#123;k&#125; \\\frac&#123;\partial X&#125;&#123;\partial u&#125; &amp; \frac&#123;\partial Y&#125;&#123;\partial u&#125; &amp; 0 \\\frac&#123;\partial X&#125;&#123;\partial v&#125; &amp; \frac&#123;\partial Y&#125;&#123;\partial v&#125; &amp; 0 \\\end&#123;vmatrix&#125;$$ 将会产生: \mathbf{V}_1 \times \mathbf{V}_2 = \begin{vmatrix} \mathbf{i} & \mathbf{j} & \mathbf{k} \\\frac{\partial X}{\partial u} & \frac{\partial Y}{\partial u} & 0 \\\frac{\partial X}{\partial v} & \frac{\partial Y}{\partial v} & 0 \\\end{vmatrix}任务列表使用如下的代码创建任务列表，在[]中输入x表示完成，也可以通过点击选择完成或者没完成。 1234- [ ] 吃饭- [ ] 逛街- [ ] 看电影- [ ] 约泡 [x] 吃饭 ​ [x] 逛街 ​ [x] 看电影 ​ [x] 约泡 列表输入+, -, *,创建无序的列表，使用任意数字开头，创建有序列表，例如： 1234**无序的列表*** tfboys* 杨洋* 我爱你 无序的列表 tfboys 杨洋 我爱你 1234**有序的列表**1. 苹果6. 香蕉10. 我都不喜欢 有序的列表 苹果 香蕉 我都不喜欢 块引用使用&gt;来插入块引用。例如： 1&gt;这是一个块引用！ 将产生： 这是一个块引用！ 标题使用#表示一级标题，##表示二级标题，以此类推，有6个标题。]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello]]></title>
    <url>%2F2018%2F08%2F22%2FHello%2F</url>
    <content type="text"><![CDATA[大学已然过三年，也浑浑噩噩过了三年。一时兴起，想搞一个属于自己的博客，把未来生活与学习路上的点点滴滴记录下来，万事开头难，于是偷个懒，就把建这个网站的过程来作为我的第一篇博客吧，记录一下，哈哈哈哈哈 安装安装git、node.js新建一个储存博客的文件夹(blogblog)打开后右键-选择-Git Bash Here输入12npm install hexo -g hexo init -g表示全局安装, npm默认为当前项目安装 node_modules：是依赖包 public：存放的是生成的页面 source：用命令创建的各种文章 themes：主题 _config.yml：整个博客的配置 db.json：source解析所得到的 package.json：项目所需模块项目的配置信息 输入123hexo cleanhexo generatehexo server 游览器打开 http://localhost:4000 但是只能在本地登录，下一步便是可以从其他地点登录 搭桥到github 选择New repository/myname.github.iomyname 必须为github的账号名 输入 12git config --global user.name &quot;my name&quot;git config --global user.email &quot;my email&quot; 创建SSH输入 ssh-keygen -t rsa -C &quot;myemail@example.com&quot; 再按两次回车输入 cd ~/.ssh 再输入 cat id_rsa.pub会输出 12ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxmyemail@example.com 把ssh-rsa xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx输入到key位置输入ssh -T git@github.com 可验证时候正确 打开在blogblog目录下的_config.yml 注意冒号后有一个空格 1234deploy: type: git repo: https://github.com/mygithubName/mygithubName.github.io.git branch: master 注意：如果同一个电脑建第二个hexo需要如下： 1234deploy: type: git repo: git@github.com:mygithubName/mygithubName.github.io.git branch: master 在blogblog目录中打开 gitbash执行npm i hexo-server再执行npm install hexo-deployer-git --save执行 123hexo cleanhexo generatehexo deploy 打开 myname.github.io 就可以看到了~ 绑定域名 买一个域名，我是在阿里云买的 在项目的source文件夹中新建一个名为CNAME的文件(不需要文件后缀)，编辑文档时把所购 买的域名添加其中，注意，只可添加一个 在DNS中添加一条记录，也可以直接通过新手引导设置，其中所需的地址只需在cmd中执行 ping myname.github.io 再执行一次 123hexo cleanhexo generatehexo deploy 更换主题可以访问hexo的主题官网，我选择的是NexT主题，一来好看实用；二来很多功能都已经写好，添加功能时会更方便一些(渣渣没办法…)，因此以下为安装NexT主题为例。 执行$ git clone https://github.com/theme-next/hexo-theme-next-themes/next 打开blogblog目录的_config.yml ,其中，修改为 theme: next emmmmm…. 没错 主题就换完了，打开试试，突然就高大上了~ 修改blogblog下_config.yml的:1234567title: 清 泉subtitle:description:keywords:author: springlanguage: zh-CNtimezone: 修改blogblog/themes/next/_config.yml: 123456789menu: home: / || home #about: /about/ || user #tags: /tags/ || tags #categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 我习惯修改为 123456789menu: home: / || home #about: /about/ || user tags: /tags/ || tags categories: /categories/ || th archives: /archives/ || archive #schedule: /schedule/ || calendar #sitemap: /sitemap.xml || sitemap #commonweal: /404/ || heartbeat 想要选择哪个把前面的#去掉即可 对于tags项： 执行hexo new page &quot;tags&quot;打开\source\tags\index.md 123456---title:date: 2018-08-21 14:56:51type: &quot;tags&quot;comments: false--- 对于categories项： 执行hexo new page &quot;categories&quot; 打开\source\categories\index.md 123456--- title: date: 2018-08-21 14:57:23 type: &quot;categories&quot; comments: false --- Next主题 又分为四种形式，可自选： 12345# Schemesscheme: Muse#scheme: Mist#scheme: Pisces#scheme: Gemini 头像12345678avatar: url: #/images/avatar.gif 你的头像图片的路径 # If true, the avatar would be dispalyed in circle. rounded: false # The value of opacity should be choose from 0 to 1 to set the opacity of the avatar. opacity: 1 # If true, the avatar would be rotated with the cursor. rotated: false 删除底部隐藏由Hexo强力驱动、主题—NexT.Mist 打开blogblog/themes/next/layout/_partials/footer.swig，注释掉相应代码 1234567891011121314151617181920212223242526//用下面的符号注释，注释代码用下面括号括起来 &lt;!-- --&gt; &lt;!-- &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt; &#123;% if theme.footer.powered %&#125; &lt;div class=&quot;powered-by&quot;&gt;&#123;# #&#125;&#123;&#123; __(&apos;footer.powered&apos;, &apos;&lt;a class=&quot;theme-link&quot; target=&quot;_blank&quot; href=&quot;https://hexo.io&quot;&gt;Hexo&lt;/a&gt;&apos;) &#125;&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;&#123;% if theme.footer.powered and theme.footer.theme.enable %&#125; &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;&#123;% endif %&#125;&#123;% if theme.footer.theme.enable %&#125; &lt;div class=&quot;theme-info&quot;&gt;&#123;# #&#125;&#123;&#123; __(&apos;footer.theme&apos;) &#125;&#125; &amp;mdash; &#123;# #&#125;&lt;a class=&quot;theme-link&quot; target=&quot;_blank&quot; href=&quot;https://github.com/iissnan/hexo-theme-next&quot;&gt;&#123;# #&#125;NexT.&#123;&#123; theme.scheme &#125;&#125;&#123;# #&#125;&lt;/a&gt;&#123;% if theme.footer.theme.version %&#125; v&#123;&#123; theme.version &#125;&#125;&#123;% endif %&#125;&#123;##&#125;&lt;/div&gt; &#123;% endif %&#125; &#123;% if theme.footer.custom_text %&#125; &lt;div class=&quot;footer-custom&quot;&gt;&#123;# #&#125;&#123;&#123; theme.footer.custom_text &#125;&#125;&#123;##&#125;&lt;/div&gt;&#123;% endif %&#125;--&gt; 背景动态 canvas_nest git clone https://github.com/theme-next/theme-next-canvas-nest source/lib/canvas-nest 把&lt;script type=&quot;text/javascript&quot; src=&quot;//cdn.bootcss.com/canvas-nest.js/1.0.0/canvas-nest.min.js&quot;&gt;&lt;/script&gt; 插入至\blogblog\themes\next\layout\_layout.swig如下： 1234567891011&lt;html&gt;&lt;head&gt; ...&lt;/head&gt;&lt;body&gt; ... ... ... 插入到这里&lt;/body&gt;&lt;/html&gt; 再修改主题配置文件 打开/next/_config.yml,修改如下： 123# Canvas-nest# Dependencies: https://github.com/theme-next/theme-next-canvas-nestcanvas_nest: true 添加DaoVoice在线联系 首先到DaoVoice注册账号，邀请码是0f81ff2f ，登录成过后，进入到后台管理，点击应用设置——&gt;安装到网站查看安装代码和AppID。 找到app_id ，在主题配置文件中找到(没有的话添加) 123# Online contact daovoice: truedaovoice_app_id: 这里填你的刚才获得的 app_id 打开/themes/next/layout/_partials/head.swig ,代码放进去，哪行都可以 123456789&#123;% if theme.daovoice %&#125; &lt;script&gt; (function(i,s,o,g,r,a,m)&#123;i[&quot;DaoVoiceObject&quot;]=r;i[r]=i[r]||function()&#123;(i[r].q=i[r].q||[]).push(arguments)&#125;,i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;a.charset=&quot;utf-8&quot;;m.parentNode.insertBefore(a,m)&#125;)(window,document,&quot;script&quot;,(&apos;https:&apos; == document.location.protocol ? &apos;https:&apos; : &apos;http:&apos;) + &quot;//widget.daovoice.io/widget/0f81ff2f.js&quot;,&quot;daovoice&quot;) daovoice(&apos;init&apos;, &#123; app_id: &quot;&#123;&#123;theme.daovoice_app_id&#125;&#125;&quot; &#125;); daovoice(&apos;update&apos;); &lt;/script&gt;&#123;% endif %&#125; 在DaoVoice中找到聊天设置调节窗口的颜色以及位置我的参数：右侧像素20.0，下侧像素：80.0 在右上角或者左上角实现fork me on github 点击这里 或者 这里挑选自己喜欢的样式，并复制代码。 然后粘贴刚才复制的代码到themes/next/layout/_layout.swig文件中(放在&lt;div class=&quot;headband&quot;&gt;&lt;/div&gt;的下面)，并把href改为你的github地址 。 添加RSS 在blogblog中打开githash 执行 npm install --save hexo-generator-feed 在blogblog/_config.yml中添加 123# Extensions## Plugins: http://hexo.io/plugins/plugins: hexo-generate-feed 在主题配置文件中修改为： 1234# Set rss to false to disable feed link.# Leave rss as empty to use site&apos;s feed link.# Set rss to specific value if you have burned your feed already.rss: /atom.xml 添加音乐 在博客配置文件中执行npm install hexo-tag-aplayer@2.0.1 新建themes\next\source\dist\music.js ,添加内容： 12345678910111213141516171819202122232425const ap = new APlayer(&#123; container: document.getElementById(&apos;aplayer&apos;), fixed: true, autoplay: false, audio: [ &#123; name: &quot;Dream It Possible&quot;, artist: &apos;Delacey&apos;, url: &apos;http://www.ytmp3.cn/down/47868.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/84.jpg&apos;, &#125;, &#123; name: &apos;いとしすぎて&apos;, artist: &apos;KG&apos;, url: &apos;http://www.ytmp3.cn/down/35726.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/8.jpg&apos;, &#125;, &#123; name: &apos;茜さす&apos;, artist: &apos;Aimer&apos;, url: &apos;http://www.ytmp3.cn/down/44578.mp3&apos;, cover: &apos;http://oeff2vktt.bkt.clouddn.com/image/96.jpg&apos;, &#125; ]&#125;); 修改网站主题字体大小在主题配置文件中123456789101112131415161718192021222324252627282930313233font: enable: true # Uri of fonts host. E.g. //fonts.googleapis.com (Default) # 亲测这个可用，如果不可用，自己搜索 [Google 字体 国内镜像]，找个能用的就行 host: https://fonts.cat.net # Global font settings used on &lt;body&gt; element. # 全局字体，应用在 body 元素上 global: external: true family: Lato size: 16 #csdn上就是16看着舒服多了 # 标题字体 (h1, h2, h3, h4, h5, h6) headings: external: true family: Roboto Slab # 文章字体 posts: external: true family: # Logo 字体 logo: external: true family: Lobster Two size: 24 # 代码字体，应用于 code 以及代码块 codes: external: true family: Roboto Mono 站点收录百度收录在主题配置文件中修改成： 1baidu_site_verification: true 进入百度站点检验网站 ，选择http:// ,purespring.top 信息技术 由于前两个验证一直通过不了，所以我选择了CNAME验证 进入阿里云 我是在阿里云买的域名，所以进入那里。 进入解析设置 添加记录 类型： CNAME 主机记录： xxxxx.purespring.top(这个会告诉你) 记录值：zz.baidu(这个会告诉你) 就可以完成确认 更改细节主题在文件\themes\next\source\css\_custom\custom.styl中，放入如下代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600// Custom styles//首页头部样式.header &#123; background: url("/images/header-bk.jpg");&#125;.site-meta &#123; margin-left: 0px; text-align: center;&#125;.site-meta .site-title &#123; font-size: 20px; font-family: 'Comic Sans MS', sans-serif; color: #fff; letter-spacing: 1px; width: 81%;&#125;// 点文章进去的页面背景色.container &#123; background-color: rgba(255, 255, 255, 0.747);&#125;// 页面留白更改.header-inner &#123; padding-top: 0px; padding-bottom: 0px;&#125;.posts-expand &#123; padding-top: 80px;&#125;.posts-expand .post-meta &#123; margin: 5px 0px 0px 0px;&#125;.post-button &#123; margin-top: 0px;&#125;// 顶栏宽度.container .header-inner &#123; width: 100%;&#125;// 站点名背景.brand&#123; background-color: rgb(56, 53, 53); margin-top: 15px; padding: 0px;&#125;// 站点名字体.site-title &#123; line-height: 35px; letter-spacing: 3px;&#125;// 站点子标题.site-subtitle&#123; margin: 0px; font-size: 16px; letter-spacing: 1px; padding-bottom: 3px; font-weight: bold; color: rgb(219, 95, 95); border-bottom-width: 3px; border-bottom-style: solid; border-bottom-color: rgb(161, 102, 171);&#125;.logo-line-after &#123; display: none;&#125;.logo-line-before &#123; display: none;&#125;// 菜单.menu &#123; float: none;&#125;// 菜单超链接字体大小.menu .menu-item a &#123; font-size: 14px; color: rgb(15, 46, 65); border-radius: 4px;&#125;// 菜单各项边距.menu .menu-item &#123; margin: 5px 15px;&#125;// 菜单超链接样式.menu .menu-item a:hover &#123; border-bottom-color: rgba(161, 102, 171, 0);&#125;// 文章.post &#123; margin-bottom: 50px; padding: 45px 36px 36px 36px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255);&#125;// 文章标题字体.posts-expand .post-title &#123; font-size: 26px; font-weight: 700;&#125;// 文章标题动态效果.posts-expand .post-title-link::before &#123; background-image: linear-gradient(90deg, #a166ab 0%, #ef4e7b 25%, #f37055 50%, #ef4e7b 75%, #a166ab 100%);&#125;// 文章元数据（meta）留白更改.posts-expand .post-meta &#123; margin: 10px 0px 20px 0px;&#125;// 文章的描述description.posts-expand .post-meta .post-description &#123; font-style: italic; font-size: 14px; margin-top: 30px; margin-bottom: 0px; color: #666;&#125;// [Read More]按钮样式.post-button .btn &#123; color: rgba(219, 210, 210, 0.911)!important; background-color: rgba(56, 52, 52, 0.911); border-radius: 3px; font-size: 15px; box-shadow: inset 0px 0px 10px 0px rgba(0, 0, 0, 0.35); border: none !important; transition-property: unset; padding: 0px 15px;&#125;.post-button .btn:hover &#123; color: rgba(219, 210, 210, 0.911) !important; border-radius: 3px; font-size: 15px; box-shadow: inset 0px 0px 10px 0px rgba(0, 0, 0, 0.35); background-image: linear-gradient(100deg, #a166ab 0%, #ef4e7b 25%, #f37055 50%, #ef4e7b 75%, #a166ab 100%);&#125;// 去除在页面文章之间的分割线.posts-expand .post-eof &#123; margin: 0px; background-color: rgba(255, 255, 255, 0);&#125;// 去除页面底部页码上面的横线.pagination &#123; border: none; margin: 0px;&#125;// 页面底部页码.pagination .page-number.current &#123; border-radius: 100%; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgba(255, 255, 255, 0.35);&#125;.pagination .prev, .pagination .next, .pagination .page-number &#123; margin-bottom: 10px; border: none;&#125;.pagination .space &#123; color: rgb(255, 255, 255);&#125;// 页面底部页脚.footer &#123; line-height: 1.5; background-color: rgba(255, 255, 255, 0.75); color: #333; border-top-width: 3px; border-top-style: solid; border-top-color: rgb(161, 102, 171); box-shadow: 0px -10px 10px 0px rgba(0, 0, 0, 0.15);&#125;// 文章底部的tags.posts-expand .post-tags a &#123; border-bottom: none; margin-right: 0px; font-size: 13px; padding: 0px 5px; border-radius: 3px; transition-duration: 0.2s; transition-timing-function: ease-in-out; transition-delay: 0s;&#125;.posts-expand .post-tags a:hover &#123; background: #eee;&#125;// 文章底部留白更改.post-widgets &#123; padding-top: 0px;&#125;.post-nav &#123; margin-top: 30px;&#125;// 文章底部页面跳转.post-nav-item a &#123; color: rgb(80, 115, 184); font-weight: bold;&#125;.post-nav-item a:hover &#123; color: rgb(161, 102, 171); font-weight: bold;&#125;// 文章底部评论.comments &#123; background-color: rgb(255, 255, 255); box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.35); margin: 80px 0px 40px 0px;&#125;// 超链接样式a &#123; color: rgb(80, 115, 184); border-bottom-color: rgb(80, 115, 184);&#125;a:hover &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;// 分割线样式hr &#123; margin: 10px 0px 30px 0px;&#125;// 文章内标题样式（左边的竖线）.post-body h2, h3, h4, h5, h6 &#123; border-left: 4px solid rgb(161, 102, 171); margin-left: -36px; padding-left: 32px;&#125;// 去掉图片边框.posts-expand .post-body img &#123; border: none; padding: 0px;&#125;.post-gallery .post-gallery-img img &#123; padding: 3px;&#125;// 文章``代码块的自定义样式code &#123; margin: 0px 4px;&#125;// 文章```代码块顶部样式.highlight figcaption &#123; margin: 0em; padding: 0.5em; background: #eee; border-bottom: 1px solid #e9e9e9;&#125;.highlight figcaption a &#123; color: rgb(80, 115, 184);&#125;// 文章```代码块diff样式pre .addition &#123; background: #e6ffed;&#125;pre .deletion &#123; background: #ffeef0;&#125;// 右下角侧栏按钮样式.sidebar-toggle &#123; right: 10px; bottom: 43px; background-color: rgba(247, 149, 51, 0.75); border-radius: 5px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.35);&#125;.page-post-detail .sidebar-toggle-line &#123; background: rgb(17, 185, 163);&#125;// 右下角返回顶部按钮样式.back-to-top &#123; line-height: 1.5; right: 10px; padding-right: 5px; padding-left: 5px; padding-top: 2.5px; padding-bottom: 2.5px; background-color: rgba(247, 149, 51, 0.75); border-radius: 5px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.35);&#125;.back-to-top.back-to-top-on &#123; bottom: 10px;&#125;// 侧栏.sidebar &#123; box-shadow: inset 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgba(0, 0, 0, 0.75);&#125;.sidebar-inner &#123; margin-top: 30px;&#125;// 侧栏顶部文字.sidebar-nav li &#123; font-size: 15px; font-weight: bold; color: rgb(7, 179, 155);&#125;.sidebar-nav li:hover &#123; color: rgb(161, 102, 171);&#125;.sidebar-nav .sidebar-nav-active &#123; color: rgb(7, 179, 155); border-bottom-color: rgb(161, 102, 171); border-bottom-width: 1.5px;&#125;.sidebar-nav .sidebar-nav-active:hover &#123; color: rgb(7, 179, 155);&#125;// 侧栏站点概况行高.site-overview &#123; line-height: 1.3;&#125;// 侧栏头像（圆形以及旋转效果）.site-author-image &#123; border: 2px solid rgb(255, 255, 255); border-radius: 100%; transition: transform 1.0s ease-out;&#125;img:hover &#123; transform: rotateZ(360deg);&#125;.posts-expand .post-body img:hover &#123; transform: initial;&#125;// 侧栏站点作者名.site-author-name &#123; display: none;&#125;// 侧栏站点描述.site-description &#123; letter-spacing: 5px; font-size: 15px; font-weight: bold; margin-top: 15px; margin-left: 13px; color: rgb(243, 112, 85);&#125;// 侧栏站点文章、分类、标签.site-state &#123; line-height: 1.3; margin-left: 12px;&#125;.site-state-item &#123; padding: 0px 15px; border-left: 1.5px solid rgb(161, 102, 171);&#125;// 侧栏RSS按钮样式.feed-link &#123; margin-top: 15px; margin-left: 7px;&#125;.feed-link a &#123; color: rgb(255, 255, 255); border: 1px solid rgb(158, 158, 158) !important; border-radius: 15px;&#125;.feed-link a:hover &#123; background-color: rgb(161, 102, 171);&#125;.feed-link a i &#123; color: rgb(255, 255, 255);&#125;// 侧栏社交链接.links-of-author &#123; margin-top: 0px;&#125;// 侧栏友链标题.links-of-blogroll-title &#123; margin-bottom: 10px; margin-top: 15px; color: rgba(7, 179, 156, 0.74); margin-left: 6px; font-size: 15px; font-weight: bold;&#125;// 侧栏超链接样式（友链的样式）.sidebar a &#123; color: #ccc; border-bottom: none;&#125;.sidebar a:hover &#123; color: rgb(255, 255, 255);&#125;// 自定义的侧栏时间样式#days &#123; display: block; color: rgb(7, 179, 155); font-size: 13px; margin-top: 15px;&#125;// 侧栏目录链接样式.post-toc ol a &#123; color: rgb(75, 240, 215); border-bottom: 1px solid rgb(96, 125, 139);&#125;.post-toc ol a:hover &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;// 侧栏目录链接样式之当前目录.post-toc .nav .active &gt; a &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;.post-toc .nav .active &gt; a:hover &#123; color: rgb(161, 102, 171); border-bottom-color: rgb(161, 102, 171);&#125;/* 修侧栏目录bug，如果主题配置文件_config.yml的toc是wrap: true */.post-toc ol &#123; padding: 0px 10px 5px 10px;&#125;/* 侧栏目录默认全展开，已注释.post-toc .nav .nav-child &#123; display: block;&#125;*/// 时间轴样式.posts-collapse &#123; margin: 50px 0px;&#125;@media (max-width: 1023px) &#123; .posts-collapse &#123; margin: 50px 20px; &#125;&#125;// 时间轴左边线条.posts-collapse::after &#123; margin-left: -2px; background-image: linear-gradient(180deg,#f79533 0,#f37055 15%,#ef4e7b 30%,#a166ab 44%,#5073b8 58%,#1098ad 72%,#07b39b 86%,#6dba82 100%);&#125;// 时间轴左边线条圆点颜色.posts-collapse .collection-title::before &#123; background-color: rgb(255, 255, 255);&#125;// 时间轴文章标题左边圆点颜色.posts-collapse .post-header:hover::before &#123; background-color: rgb(161, 102, 171);&#125;// 时间轴年份.posts-collapse .collection-title h1, .posts-collapse .collection-title h2 &#123; color: rgb(255, 255, 255);&#125;// 时间轴文章标题.posts-collapse .post-title a &#123; color: rgb(80, 115, 184);&#125;.posts-collapse .post-title a:hover &#123; color: rgb(161, 102, 171);&#125;// 时间轴文章标题底部虚线.posts-collapse .post-header:hover &#123; border-bottom-color: rgb(161, 102, 171);&#125;// archives页面顶部文字.page-archive .archive-page-counter &#123; color: rgb(255, 255, 255);&#125;// archives页面时间轴左边线条第一个圆点颜色.page-archive .posts-collapse .archive-move-on &#123; top: 10px; opacity: 1; background-color: rgb(255, 255, 255); box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5);&#125;// 分类页面.post-block.page &#123; margin-top: 40px;&#125;.category-all-page &#123; margin: -80px 50px 40px 50px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 86px 36px 36px 36px;&#125;@media (max-width: 767px) &#123; .category-all-page &#123; margin: -73px 15px 50px 15px; &#125; .category-all-page .category-all-title &#123; margin-top: -5px; &#125;&#125;// 标签云页面.tag-cloud &#123; margin: -80px 50px 40px 50px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 86px 36px 36px 36px;&#125;.tag-cloud-title &#123; margin-bottom: 15px;&#125;@media (max-width: 767px) &#123; .tag-cloud &#123; margin: -73px 15px 50px 15px; padding: 86px 5px 36px 5px; &#125;&#125;// 自定义的TopX页面样式#top &#123; display: block; text-align: center; margin: -100px 50px 40px 50px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 106px 36px 10px 36px;&#125;@media (max-width: 767px) &#123; #top &#123; margin: -93px 15px 50px 15px; padding: 96px 10px 0px 10px; &#125;&#125;// 自定义ABOUT页面的样式.about-page &#123; margin: -80px 0px 60px 0px; box-shadow: 0px 0px 10px 0px rgba(0, 0, 0, 0.5); background-color: rgb(255, 255, 255); padding: 106px 36px 36px 36px;&#125;@media (max-width: 767px) &#123; .about-page &#123; margin: -73px 0px 50px 0px; padding: 96px 15px 20px 15px; &#125;&#125;h2.about-title &#123; border-left: none !important; margin-left: 0px !important; padding-left: 0px !important; text-align: center; background-image: linear-gradient(90deg, #a166ab 0%, #a166ab 40%, #ef4e7b 45%, #f37055 50%, #ef4e7b 55%, #a166ab 60%, #a166ab 100%); background-size: cover; -webkit-background-clip: text; -webkit-text-fill-color: transparent; user-select: none;&#125;// 本地搜索框.local-search-popup .search-icon, .local-search-popup .popup-btn-close &#123; color: rgb(247, 149, 51); margin-top: 7px;&#125;.local-search-popup .local-search-input-wrapper input &#123; padding: 9px 0px; height: 21px; background-color: rgb(255, 255, 255);&#125;.local-search-popup .popup-btn-close &#123; border-left: none;&#125;// 选中文字部分的样式::selection &#123; background-color: rgb(255, 241, 89); color: #555;&#125;/* 设置滚动条的样式 *//* 参考https://segmentfault.com/a/1190000003708894 */::-webkit-scrollbar &#123; height: 5px;&#125;/* 滚动槽 */::-webkit-scrollbar-track &#123; background: #eee;&#125;/* 滚动条滑块 */::-webkit-scrollbar-thumb &#123; border-radius: 5px; background-color: #ccc;&#125;::-webkit-scrollbar-thumb:hover &#123; background-color: rgb(247, 149, 51);&#125;// 音乐播放器aplayer.aplayer &#123; font-family: Lato, -apple-system, BlinkMacSystemFont, "PingFang SC", "Hiragino Sans GB", "Heiti SC", STHeiti, "Source Han Sans SC", "Noto Sans CJK SC", "WenQuanYi Micro Hei", "Droid Sans Fallback", "Microsoft YaHei", sans-serif !important;&#125;.aplayer-withlrc.aplayer .aplayer-info &#123; background-color: rgb(255, 255, 255);&#125;// 音乐播放器aplayer歌单.aplayer .aplayer-list ol &#123; background-color: rgb(255, 255, 255);&#125;// 修视频播放器dplayer页面全屏的bug.use-motion .post-body &#123; transform: inherit !important;&#125;// 自定义emoji样式img#github-emoji &#123; margin: 0px; padding: 0px; display: inline !important; vertical-align: text-bottom; border: none; cursor: text; box-shadow: none;&#125;.site-meta .brand &#123; width: 10%;&#125;// 页面最顶部的横线.headband &#123; height: 1.5px; background-image: linear-gradient(90deg, #F79533 0%, #F37055 15%, #EF4E7B 30%, #A166AB 44%, #5073B8 58%, #1098AD 72%, #07B39B 86%, #6DBA82 100%);&#125; 打开网站缓冲条式特效打开\themes\next\layout\_partials\head\head.swig文件 在下面增加如下代码 123456789101112131415161718&lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1, maximum-scale=1&quot;/&gt;&lt;!-- S 新增代码 --&gt;&lt;script src=&quot;//cdn.bootcss.com/pace/1.0.2/pace.min.js&quot;&gt;&lt;/script&gt;&lt;link href=&quot;//cdn.bootcss.com/pace/1.0.2/themes/pink/pace-theme-flash.css&quot; rel=&quot;stylesheet&quot;&gt;&lt;style&gt; .pace .pace-progress &#123; background: #24292e; /*进度条颜色*/ height: 3px; &#125; .pace .pace-progress-inner &#123; box-shadow: 0 0 10px #1E92FB, 0 0 5px #1E92FB; /*阴影颜色*/ &#125; .pace .pace-activity &#123; border-top-color: #1E92FB; /*上边框颜色*/ border-left-color: #1E92FB; /*左边框颜色*/ &#125;&lt;/style&gt;&lt;!-- E 新增代码 --&gt; 至此，网站已基本配置完成。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>教程</tag>
      </tags>
  </entry>
</search>
